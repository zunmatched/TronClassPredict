{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from MyPyTorch import *\n",
    "\n",
    "os.makedirs('model_pytorch', exist_ok=True)\n",
    "os.makedirs('figure', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1+cu113\n",
      "11.3\n",
      "True\n",
      "Quadro P2000\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData_Advice(course_name=None):\n",
    "    x_data = pd.read_csv('data/train/ScorePredict_x_{}.csv'.format(course_name) if course_name else 'data/train/ScorePredict_x.csv')\n",
    "    y_data = pd.read_csv('data/train/ScorePredict_y_{}.csv'.format(course_name) if course_name else 'data/train/ScorePredict_y.csv')\n",
    "    n_data = pd.read_csv('data/train/normalize.csv')\n",
    "    n_data.set_index('col', inplace=True)\n",
    "    feature = pd.read_excel('data/other/text_material.xlsx', sheet_name='advice')['feature'].tolist()\n",
    "    model_sp = torch.load('model_pytorch/ScoreRangePredict_MOE_{}.pt'.format(course_name) if course_name else 'model_pytorch/ScoreRangePredict.pt')\n",
    "    xn_data = n_data.apply(lambda n: normalize(n, x_data), axis = 1).T\n",
    "    xn_data = torch.tensor(xn_data.to_numpy(), dtype=torch.float)\n",
    "    yn_data = y_data.drop(['group'], axis = 1)\n",
    "    yn_data = torch.tensor(yn_data.to_numpy(), dtype=torch.float)\n",
    "    select_row = torch.argmax(model_sp(xn_data), dim=1) == torch.argmax(yn_data, dim=1)\n",
    "    select_row = select_row.detach().numpy().tolist()\n",
    "\n",
    "    x_data = x_data[select_row]\n",
    "    y_data = x_data[['score', 'group']]\n",
    "    x_data, y_data = y_data, x_data\n",
    "    x_train = x_data[x_data['group'] == 'training']\n",
    "    x_valid = x_data[x_data['group'] == 'validation']\n",
    "    x_test = x_data[x_data['group'] == 'testing']\n",
    "    y_train = y_data[y_data['group'] == 'training']\n",
    "    y_valid = y_data[y_data['group'] == 'validation']\n",
    "    y_test = y_data[y_data['group'] == 'testing']\n",
    "    y_train = n_data.apply(lambda n: normalize(n, y_train), axis = 1).T\n",
    "    y_valid = n_data.apply(lambda n: normalize(n, y_valid), axis = 1).T\n",
    "    y_test = n_data.apply(lambda n: normalize(n, y_test), axis = 1).T\n",
    "    x_train = pd.concat([x_train[['score']], y_train[['year', 'semester', 'week']]], axis=1)\n",
    "    x_valid = pd.concat([x_valid[['score']], y_valid[['year', 'semester', 'week']]], axis=1)\n",
    "    x_test = pd.concat([x_test[['score']], y_test[['year', 'semester', 'week']]], axis=1)\n",
    "    y_train = y_train.loc[:, y_train.columns.isin(feature)]\n",
    "    y_valid = y_valid.loc[:, y_valid.columns.isin(feature)]\n",
    "    y_test = y_test.loc[:, y_test.columns.isin(feature)]\n",
    "\n",
    "    x_train, x_valid, x_test, y_train, y_valid, y_test = map(lambda x: torch.tensor(x.to_numpy(), dtype=torch.float), (x_train, x_valid, x_test, y_train, y_valid, y_test))\n",
    "    return (x_train, x_valid, x_test, y_train, y_valid, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 train_loss 1.15430081 valid_loss 1.22706461 test_loss 1.17435372 < save\n",
      "2 train_loss 1.05799186 valid_loss 1.13713622 test_loss 1.08497012 < save\n",
      "3 train_loss 1.01961458 valid_loss 1.09613895 test_loss 1.04529059 < save\n",
      "4 train_loss 1.00611103 valid_loss 1.08283925 test_loss 1.03218138 < save\n",
      "5 train_loss 0.92934054 valid_loss 0.99266171 test_loss 0.94650364 < save\n",
      "6 train_loss 0.92325175 valid_loss 0.98545849 test_loss 0.93978435 < save\n",
      "7 train_loss 0.91953838 valid_loss 0.98135978 test_loss 0.93592691 < save\n",
      "8 train_loss 0.91644555 valid_loss 0.97860271 test_loss 0.93324000 < save\n",
      "9 train_loss 0.91334438 valid_loss 0.97503006 test_loss 0.92988390 < save\n",
      "10 train_loss 0.91003531 valid_loss 0.97138035 test_loss 0.92640698 < save\n",
      "11 train_loss 0.90152985 valid_loss 0.96462619 test_loss 0.91937792 < save\n",
      "12 train_loss 0.87349623 valid_loss 0.93940961 test_loss 0.89526057 < save\n",
      "13 train_loss 0.85176212 valid_loss 0.92257732 test_loss 0.87930918 < save\n",
      "14 train_loss 0.84373564 valid_loss 0.91514760 test_loss 0.87256312 < save\n",
      "15 train_loss 0.84045774 valid_loss 0.91160667 test_loss 0.86899024 < save\n",
      "16 train_loss 0.83767807 valid_loss 0.90908837 test_loss 0.86681485 < save\n",
      "17 train_loss 0.83647984 valid_loss 0.90763652 test_loss 0.86524898 < save\n",
      "18 train_loss 0.83508724 valid_loss 0.90666312 test_loss 0.86447722 < save\n",
      "19 train_loss 0.83457327 valid_loss 0.90621161 test_loss 0.86399704 < save\n",
      "20 train_loss 0.81984091 valid_loss 0.88738936 test_loss 0.84621280 < save\n",
      "21 train_loss 0.81818730 valid_loss 0.88632095 test_loss 0.84496188 < save\n",
      "22 train_loss 0.81741238 valid_loss 0.88562268 test_loss 0.84456038 < save\n",
      "23 train_loss 0.81688762 valid_loss 0.88491160 test_loss 0.84392023 < save\n",
      "24 train_loss 0.81695300 valid_loss 0.88427716 test_loss 0.84356463 < save\n",
      "25 train_loss 0.81682485 valid_loss 0.88402277 test_loss 0.84325439 < save\n",
      "26 train_loss 0.81665993 valid_loss 0.88384575 test_loss 0.84314328 < save\n",
      "27 train_loss 0.81692696 valid_loss 0.88495463 test_loss 0.84412831 \n",
      "28 train_loss 0.81591594 valid_loss 0.88393015 test_loss 0.84290022 \n",
      "29 train_loss 0.81532133 valid_loss 0.88288581 test_loss 0.84211087 < save\n",
      "30 train_loss 0.81574970 valid_loss 0.88278234 test_loss 0.84207189 < save\n",
      "31 train_loss 0.81682771 valid_loss 0.88350970 test_loss 0.84296483 \n",
      "32 train_loss 0.81498826 valid_loss 0.88218123 test_loss 0.84162050 < save\n",
      "33 train_loss 0.81535113 valid_loss 0.88244134 test_loss 0.84175569 \n",
      "34 train_loss 0.81554931 valid_loss 0.88229436 test_loss 0.84172565 \n",
      "35 train_loss 0.81514013 valid_loss 0.88273239 test_loss 0.84205145 \n",
      "36 train_loss 0.81496000 valid_loss 0.88200527 test_loss 0.84136635 < save\n",
      "37 train_loss 0.81460589 valid_loss 0.88186646 test_loss 0.84129983 < save\n",
      "38 train_loss 0.81544119 valid_loss 0.88319814 test_loss 0.84243226 \n",
      "39 train_loss 0.81451333 valid_loss 0.88198853 test_loss 0.84131861 \n",
      "40 train_loss 0.81439579 valid_loss 0.88166124 test_loss 0.84087253 < save\n",
      "41 train_loss 0.81477010 valid_loss 0.88170487 test_loss 0.84114778 \n",
      "42 train_loss 0.81470221 valid_loss 0.88141072 test_loss 0.84081435 < save\n",
      "43 train_loss 0.81419820 valid_loss 0.88101995 test_loss 0.84045923 < save\n",
      "44 train_loss 0.81391740 valid_loss 0.88106620 test_loss 0.84040105 \n",
      "45 train_loss 0.81507957 valid_loss 0.88150752 test_loss 0.84114099 \n",
      "46 train_loss 0.81593359 valid_loss 0.88207930 test_loss 0.84166056 \n",
      "47 train_loss 0.81510389 valid_loss 0.88156712 test_loss 0.84098482 \n",
      "48 train_loss 0.81352824 valid_loss 0.88062567 test_loss 0.83991659 < save\n",
      "49 train_loss 0.81352425 valid_loss 0.88038611 test_loss 0.83968502 < save\n",
      "50 train_loss 0.81327188 valid_loss 0.88032603 test_loss 0.83961755 < save\n",
      "51 train_loss 0.81307566 valid_loss 0.87997288 test_loss 0.83924818 < save\n",
      "52 train_loss 0.81291294 valid_loss 0.87994820 test_loss 0.83910751 < save\n",
      "53 train_loss 0.81300616 valid_loss 0.88000107 test_loss 0.83922654 \n",
      "54 train_loss 0.81289089 valid_loss 0.87933946 test_loss 0.83862066 < save\n",
      "55 train_loss 0.81430435 valid_loss 0.88201565 test_loss 0.84085780 \n",
      "56 train_loss 0.81201649 valid_loss 0.87889415 test_loss 0.83802968 < save\n",
      "57 train_loss 0.81176865 valid_loss 0.87818259 test_loss 0.83743876 < save\n",
      "58 train_loss 0.81157309 valid_loss 0.87800974 test_loss 0.83719969 < save\n",
      "59 train_loss 0.81103784 valid_loss 0.87766504 test_loss 0.83676869 < save\n",
      "60 train_loss 0.81100571 valid_loss 0.87713599 test_loss 0.83641970 < save\n",
      "61 train_loss 0.81092477 valid_loss 0.87685508 test_loss 0.83616686 < save\n",
      "62 train_loss 0.81014812 valid_loss 0.87689179 test_loss 0.83590871 \n",
      "63 train_loss 0.81073177 valid_loss 0.87640870 test_loss 0.83573496 < save\n",
      "64 train_loss 0.80909336 valid_loss 0.87486815 test_loss 0.83427256 < save\n",
      "65 train_loss 0.80864757 valid_loss 0.87433618 test_loss 0.83375889 < save\n",
      "66 train_loss 0.80838627 valid_loss 0.87362349 test_loss 0.83316427 < save\n",
      "67 train_loss 0.80796975 valid_loss 0.87398732 test_loss 0.83317989 \n",
      "68 train_loss 0.81057751 valid_loss 0.87471616 test_loss 0.83450681 \n",
      "69 train_loss 0.80638838 valid_loss 0.87141979 test_loss 0.83105552 < save\n",
      "70 train_loss 0.80573112 valid_loss 0.87066156 test_loss 0.83033878 < save\n",
      "71 train_loss 0.80577523 valid_loss 0.87136304 test_loss 0.83079576 \n",
      "72 train_loss 0.80602938 valid_loss 0.86992592 test_loss 0.82996368 < save\n",
      "73 train_loss 0.80401707 valid_loss 0.86824352 test_loss 0.82822603 < save\n",
      "74 train_loss 0.80287749 valid_loss 0.86746788 test_loss 0.82743037 < save\n",
      "75 train_loss 0.80633903 valid_loss 0.87178642 test_loss 0.83177328 \n",
      "76 train_loss 0.80162364 valid_loss 0.86579669 test_loss 0.82598317 < save\n",
      "77 train_loss 0.80102146 valid_loss 0.86453730 test_loss 0.82497835 < save\n",
      "78 train_loss 0.80006731 valid_loss 0.86372799 test_loss 0.82422298 < save\n",
      "79 train_loss 0.79941195 valid_loss 0.86318481 test_loss 0.82365119 < save\n",
      "80 train_loss 0.79880023 valid_loss 0.86252427 test_loss 0.82310420 < save\n",
      "81 train_loss 0.79993689 valid_loss 0.86402142 test_loss 0.82473612 \n",
      "82 train_loss 0.79802042 valid_loss 0.86173183 test_loss 0.82254142 < save\n",
      "83 train_loss 0.79712808 valid_loss 0.86061591 test_loss 0.82146597 < save\n",
      "84 train_loss 0.79674178 valid_loss 0.85970575 test_loss 0.82082415 < save\n",
      "85 train_loss 0.79683322 valid_loss 0.85922635 test_loss 0.82043707 < save\n",
      "86 train_loss 0.79820257 valid_loss 0.86241066 test_loss 0.82329285 \n",
      "87 train_loss 0.79520333 valid_loss 0.85808206 test_loss 0.81935388 < save\n",
      "88 train_loss 0.79471225 valid_loss 0.85788733 test_loss 0.81909907 < save\n",
      "89 train_loss 0.79514086 valid_loss 0.85857224 test_loss 0.81969953 \n",
      "90 train_loss 0.79474342 valid_loss 0.85685062 test_loss 0.81837207 < save\n",
      "91 train_loss 0.79342806 valid_loss 0.85608864 test_loss 0.81758541 < save\n",
      "92 train_loss 0.79306567 valid_loss 0.85515761 test_loss 0.81689268 < save\n",
      "93 train_loss 0.79260975 valid_loss 0.85523564 test_loss 0.81678420 \n",
      "94 train_loss 0.79217905 valid_loss 0.85447896 test_loss 0.81612420 < save\n",
      "95 train_loss 0.79193950 valid_loss 0.85477334 test_loss 0.81634331 \n",
      "96 train_loss 0.79201472 valid_loss 0.85387087 test_loss 0.81582165 < save\n",
      "97 train_loss 0.79215348 valid_loss 0.85390574 test_loss 0.81585550 \n",
      "98 train_loss 0.79170901 valid_loss 0.85335994 test_loss 0.81535065 < save\n",
      "99 train_loss 0.79064465 valid_loss 0.85296321 test_loss 0.81482226 < save\n",
      "100 train_loss 0.79045886 valid_loss 0.85294402 test_loss 0.81468314 < save\n",
      "101 train_loss 0.79012811 valid_loss 0.85244036 test_loss 0.81433457 < save\n",
      "102 train_loss 0.79176795 valid_loss 0.85522139 test_loss 0.81665385 \n",
      "103 train_loss 0.78991443 valid_loss 0.85254627 test_loss 0.81421226 \n",
      "104 train_loss 0.78969592 valid_loss 0.85196406 test_loss 0.81382507 < save\n",
      "105 train_loss 0.78953576 valid_loss 0.85207891 test_loss 0.81387639 \n",
      "106 train_loss 0.78926867 valid_loss 0.85178083 test_loss 0.81357747 < save\n",
      "107 train_loss 0.78935671 valid_loss 0.85108256 test_loss 0.81325638 < save\n",
      "108 train_loss 0.78914499 valid_loss 0.85130626 test_loss 0.81344378 \n",
      "109 train_loss 0.78906292 valid_loss 0.85093033 test_loss 0.81305653 < save\n",
      "110 train_loss 0.79005837 valid_loss 0.85336405 test_loss 0.81474632 \n",
      "111 train_loss 0.78922868 valid_loss 0.85201943 test_loss 0.81389755 \n",
      "112 train_loss 0.78989011 valid_loss 0.85300863 test_loss 0.81464869 \n",
      "113 train_loss 0.78854859 valid_loss 0.85037476 test_loss 0.81270164 < save\n",
      "114 train_loss 0.79018360 valid_loss 0.85350400 test_loss 0.81520289 \n",
      "115 train_loss 0.78919691 valid_loss 0.85225749 test_loss 0.81397128 \n",
      "116 train_loss 0.78902406 valid_loss 0.85056829 test_loss 0.81298709 \n",
      "117 train_loss 0.78825092 valid_loss 0.85035288 test_loss 0.81242561 < save\n",
      "118 train_loss 0.78801221 valid_loss 0.85042304 test_loss 0.81231850 \n",
      "119 train_loss 0.78859437 valid_loss 0.85155046 test_loss 0.81322640 \n",
      "120 train_loss 0.78801012 valid_loss 0.85004252 test_loss 0.81231588 < save\n",
      "121 train_loss 0.78890455 valid_loss 0.85196364 test_loss 0.81357938 \n",
      "122 train_loss 0.79422921 valid_loss 0.85482204 test_loss 0.81766468 \n",
      "123 train_loss 0.78821313 valid_loss 0.85119057 test_loss 0.81291574 \n",
      "124 train_loss 0.78820455 valid_loss 0.85009730 test_loss 0.81238300 \n",
      "125 train_loss 0.78779179 valid_loss 0.85035253 test_loss 0.81228906 \n",
      "126 train_loss 0.78894019 valid_loss 0.85230142 test_loss 0.81392819 \n",
      "127 train_loss 0.78794622 valid_loss 0.84989363 test_loss 0.81227642 < save\n",
      "128 train_loss 0.78777069 valid_loss 0.84969044 test_loss 0.81211466 < save\n",
      "129 train_loss 0.78813654 valid_loss 0.85105664 test_loss 0.81280649 \n",
      "130 train_loss 0.78768700 valid_loss 0.84982103 test_loss 0.81211555 \n",
      "131 train_loss 0.78747910 valid_loss 0.84953856 test_loss 0.81176847 < save\n",
      "132 train_loss 0.78773230 valid_loss 0.85049993 test_loss 0.81233245 \n",
      "133 train_loss 0.78833216 valid_loss 0.84992075 test_loss 0.81238490 \n",
      "134 train_loss 0.78752983 valid_loss 0.85003394 test_loss 0.81213766 \n",
      "135 train_loss 0.79243201 valid_loss 0.85683984 test_loss 0.81785494 \n",
      "136 train_loss 0.78750134 valid_loss 0.84996068 test_loss 0.81190932 \n",
      "137 train_loss 0.78797519 valid_loss 0.84984952 test_loss 0.81216705 \n",
      "138 train_loss 0.78739601 valid_loss 0.84981859 test_loss 0.81205857 \n",
      "139 train_loss 0.79032177 valid_loss 0.85440999 test_loss 0.81573123 \n",
      "140 train_loss 0.78758937 valid_loss 0.85054016 test_loss 0.81230158 \n",
      "141 train_loss 0.78786844 valid_loss 0.85103315 test_loss 0.81285596 \n",
      "142 train_loss 0.78734833 valid_loss 0.84968799 test_loss 0.81180799 \n",
      "143 train_loss 0.78743982 valid_loss 0.84994441 test_loss 0.81217813 \n",
      "144 train_loss 0.79066253 valid_loss 0.85158801 test_loss 0.81445593 \n",
      "145 train_loss 0.78733718 valid_loss 0.84945357 test_loss 0.81188411 < save\n",
      "146 train_loss 0.78752631 valid_loss 0.84958184 test_loss 0.81197768 \n",
      "147 train_loss 0.78829277 valid_loss 0.85001642 test_loss 0.81238216 \n",
      "148 train_loss 0.78774691 valid_loss 0.84978950 test_loss 0.81221348 \n",
      "149 train_loss 0.78733373 valid_loss 0.85017395 test_loss 0.81205755 \n",
      "150 train_loss 0.78772670 valid_loss 0.85083336 test_loss 0.81260478 \n",
      "151 train_loss 0.78726768 valid_loss 0.84986562 test_loss 0.81207025 \n",
      "152 train_loss 0.78915733 valid_loss 0.85296088 test_loss 0.81447440 \n",
      "153 train_loss 0.78774023 valid_loss 0.85098588 test_loss 0.81286359 \n",
      "154 train_loss 0.79144537 valid_loss 0.85593873 test_loss 0.81701738 \n",
      "155 train_loss 0.78720570 valid_loss 0.84955436 test_loss 0.81171775 \n",
      "156 train_loss 0.78737223 valid_loss 0.84961867 test_loss 0.81204033 \n",
      "157 train_loss 0.78795815 valid_loss 0.85144126 test_loss 0.81306493 \n",
      "158 train_loss 0.78916788 valid_loss 0.85071337 test_loss 0.81333214 \n",
      "159 train_loss 0.78716654 valid_loss 0.84994638 test_loss 0.81185728 \n",
      "160 train_loss 0.78744185 valid_loss 0.84958917 test_loss 0.81208146 \n",
      "161 train_loss 0.78742939 valid_loss 0.84966063 test_loss 0.81189662 \n",
      "162 train_loss 0.78728068 valid_loss 0.85025156 test_loss 0.81224793 \n",
      "163 train_loss 0.78974158 valid_loss 0.85383046 test_loss 0.81519115 \n",
      "164 train_loss 0.78847051 valid_loss 0.85010087 test_loss 0.81263167 \n",
      "165 train_loss 0.78795516 valid_loss 0.84985095 test_loss 0.81224400 \n",
      "166 train_loss 0.78725332 valid_loss 0.85040355 test_loss 0.81207681 \n",
      "167 train_loss 0.78819937 valid_loss 0.85016584 test_loss 0.81243044 \n",
      "168 train_loss 0.78836346 valid_loss 0.85008919 test_loss 0.81254953 \n",
      "169 train_loss 0.78859878 valid_loss 0.85240126 test_loss 0.81375158 \n",
      "170 train_loss 0.78960460 valid_loss 0.85123676 test_loss 0.81366354 \n",
      "171 train_loss 0.78741568 valid_loss 0.84945065 test_loss 0.81174237 < save\n",
      "172 train_loss 0.78726470 valid_loss 0.85019243 test_loss 0.81216598 \n",
      "173 train_loss 0.78720272 valid_loss 0.85024309 test_loss 0.81218767 \n",
      "174 train_loss 0.78726518 valid_loss 0.85009027 test_loss 0.81224740 \n",
      "175 train_loss 0.78853035 valid_loss 0.85023910 test_loss 0.81287760 \n",
      "176 train_loss 0.78715628 valid_loss 0.85004830 test_loss 0.81185681 \n",
      "177 train_loss 0.78863198 valid_loss 0.85037959 test_loss 0.81301677 \n",
      "178 train_loss 0.78721577 valid_loss 0.85025436 test_loss 0.81222516 \n",
      "179 train_loss 0.78747088 valid_loss 0.84977150 test_loss 0.81196070 \n",
      "180 train_loss 0.78703219 valid_loss 0.85004395 test_loss 0.81194544 \n",
      "181 train_loss 0.78707391 valid_loss 0.84953260 test_loss 0.81179869 \n",
      "182 train_loss 0.78763944 valid_loss 0.85098612 test_loss 0.81267464 \n",
      "183 train_loss 0.78720421 valid_loss 0.84966713 test_loss 0.81177044 \n",
      "184 train_loss 0.78778285 valid_loss 0.85138881 test_loss 0.81300658 \n",
      "185 train_loss 0.78707695 valid_loss 0.84941649 test_loss 0.81175411 < save\n",
      "186 train_loss 0.79169410 valid_loss 0.85312438 test_loss 0.81581676 \n",
      "187 train_loss 0.78694880 valid_loss 0.84974104 test_loss 0.81174415 \n",
      "188 train_loss 0.78725874 valid_loss 0.84980601 test_loss 0.81192034 \n",
      "189 train_loss 0.78752637 valid_loss 0.85092318 test_loss 0.81273162 \n",
      "190 train_loss 0.78697133 valid_loss 0.84951454 test_loss 0.81169707 \n",
      "191 train_loss 0.78708315 valid_loss 0.85019445 test_loss 0.81211513 \n",
      "192 train_loss 0.78727120 valid_loss 0.84969246 test_loss 0.81187755 \n",
      "193 train_loss 0.78699750 valid_loss 0.84958494 test_loss 0.81182772 \n",
      "194 train_loss 0.78797466 valid_loss 0.85152024 test_loss 0.81327409 \n",
      "195 train_loss 0.78697652 valid_loss 0.84982228 test_loss 0.81194603 \n",
      "196 train_loss 0.78793114 valid_loss 0.85147196 test_loss 0.81308270 \n",
      "197 train_loss 0.78709775 valid_loss 0.84945661 test_loss 0.81172997 \n",
      "198 train_loss 0.78848207 valid_loss 0.85031539 test_loss 0.81271929 \n",
      "199 train_loss 0.78757471 valid_loss 0.84993339 test_loss 0.81208974 \n",
      "200 train_loss 0.78698623 valid_loss 0.84994495 test_loss 0.81188518 \n",
      "201 train_loss 0.78721070 valid_loss 0.84954816 test_loss 0.81184107 \n",
      "202 train_loss 0.78702790 valid_loss 0.85010177 test_loss 0.81208450 \n",
      "203 train_loss 0.78708786 valid_loss 0.85014063 test_loss 0.81215978 \n",
      "204 train_loss 0.78768772 valid_loss 0.84997451 test_loss 0.81220561 \n",
      "205 train_loss 0.78835022 valid_loss 0.85224795 test_loss 0.81378525 \n",
      "206 train_loss 0.78748608 valid_loss 0.85074657 test_loss 0.81255513 \n",
      "207 train_loss 0.78829509 valid_loss 0.85019267 test_loss 0.81279385 \n",
      "208 train_loss 0.78725517 valid_loss 0.84965855 test_loss 0.81198084 \n",
      "209 train_loss 0.78752381 valid_loss 0.85007763 test_loss 0.81207824 \n",
      "210 train_loss 0.78703415 valid_loss 0.85033035 test_loss 0.81212962 \n",
      "211 train_loss 0.78743219 valid_loss 0.85001755 test_loss 0.81227463 \n",
      "212 train_loss 0.79184717 valid_loss 0.85311526 test_loss 0.81593698 \n",
      "213 train_loss 0.78704649 valid_loss 0.85021418 test_loss 0.81216091 \n",
      "214 train_loss 0.78757823 valid_loss 0.85117859 test_loss 0.81275415 \n",
      "215 train_loss 0.79836190 valid_loss 0.85911590 test_loss 0.82204044 \n",
      "216 train_loss 0.78816259 valid_loss 0.85198885 test_loss 0.81365395 \n",
      "217 train_loss 0.78768241 valid_loss 0.85136753 test_loss 0.81305271 \n",
      "218 train_loss 0.78715563 valid_loss 0.85044348 test_loss 0.81221455 \n",
      "219 train_loss 0.78693730 valid_loss 0.84975809 test_loss 0.81180513 \n",
      "220 train_loss 0.78757787 valid_loss 0.84980136 test_loss 0.81221634 \n",
      "221 train_loss 0.78715611 valid_loss 0.85051984 test_loss 0.81234145 \n",
      "222 train_loss 0.78703094 valid_loss 0.85018742 test_loss 0.81216586 \n",
      "223 train_loss 0.78708947 valid_loss 0.84966153 test_loss 0.81196171 \n",
      "224 train_loss 0.78791654 valid_loss 0.85170025 test_loss 0.81336945 \n",
      "225 train_loss 0.78700703 valid_loss 0.85021019 test_loss 0.81221396 \n",
      "226 train_loss 0.78713816 valid_loss 0.85035998 test_loss 0.81240863 \n",
      "227 train_loss 0.78725004 valid_loss 0.84967196 test_loss 0.81182027 \n",
      "228 train_loss 0.78723830 valid_loss 0.85067326 test_loss 0.81254572 \n",
      "229 train_loss 0.78751731 valid_loss 0.85084975 test_loss 0.81264603 \n",
      "230 train_loss 0.78691685 valid_loss 0.84995395 test_loss 0.81191742 \n",
      "231 train_loss 0.78699201 valid_loss 0.84983069 test_loss 0.81186849 \n",
      "232 train_loss 0.78686178 valid_loss 0.84982091 test_loss 0.81180722 \n",
      "233 train_loss 0.78869641 valid_loss 0.85066354 test_loss 0.81302893 \n",
      "234 train_loss 0.78711808 valid_loss 0.84968710 test_loss 0.81182331 \n",
      "235 train_loss 0.78747159 valid_loss 0.85102487 test_loss 0.81272399 \n",
      "236 train_loss 0.78718656 valid_loss 0.85046643 test_loss 0.81222957 \n",
      "237 train_loss 0.78853041 valid_loss 0.85272223 test_loss 0.81409574 \n",
      "238 train_loss 0.78800434 valid_loss 0.85200012 test_loss 0.81342751 \n",
      "239 train_loss 0.78688347 valid_loss 0.84988672 test_loss 0.81179100 \n",
      "240 train_loss 0.78705418 valid_loss 0.84951836 test_loss 0.81170523 \n",
      "241 train_loss 0.78704023 valid_loss 0.85014600 test_loss 0.81206793 \n",
      "242 train_loss 0.78869510 valid_loss 0.85284883 test_loss 0.81414467 \n",
      "243 train_loss 0.78720772 valid_loss 0.84990782 test_loss 0.81193918 \n",
      "244 train_loss 0.78755575 valid_loss 0.85122675 test_loss 0.81300396 \n",
      "245 train_loss 0.78691131 valid_loss 0.85010827 test_loss 0.81188047 \n",
      "246 train_loss 0.78789788 valid_loss 0.85163504 test_loss 0.81332189 \n",
      "247 train_loss 0.78722823 valid_loss 0.84998947 test_loss 0.81188673 \n",
      "248 train_loss 0.78752977 valid_loss 0.85121047 test_loss 0.81284350 \n",
      "249 train_loss 0.78811908 valid_loss 0.85035175 test_loss 0.81264865 \n",
      "250 train_loss 0.78707170 valid_loss 0.85037476 test_loss 0.81228954 \n",
      "251 train_loss 0.78706789 valid_loss 0.85036218 test_loss 0.81224632 \n",
      "252 train_loss 0.78752846 valid_loss 0.84975755 test_loss 0.81208426 \n",
      "253 train_loss 0.78748661 valid_loss 0.84995806 test_loss 0.81212431 \n",
      "254 train_loss 0.78717571 valid_loss 0.85072958 test_loss 0.81245345 \n",
      "255 train_loss 0.78699976 valid_loss 0.85013402 test_loss 0.81191701 \n",
      "256 train_loss 0.79186332 valid_loss 0.85339212 test_loss 0.81614590 \n",
      "257 train_loss 0.78687423 valid_loss 0.84978342 test_loss 0.81179589 \n",
      "258 train_loss 0.78790659 valid_loss 0.85010254 test_loss 0.81266868 \n",
      "259 train_loss 0.79014373 valid_loss 0.85466087 test_loss 0.81592798 \n",
      "260 train_loss 0.78718334 valid_loss 0.85056680 test_loss 0.81243151 \n",
      "261 train_loss 0.78706551 valid_loss 0.85054928 test_loss 0.81227595 \n",
      "262 train_loss 0.78728592 valid_loss 0.85088617 test_loss 0.81254506 \n",
      "263 train_loss 0.78695410 valid_loss 0.85009098 test_loss 0.81206322 \n",
      "264 train_loss 0.78694838 valid_loss 0.85000551 test_loss 0.81182182 \n",
      "265 train_loss 0.78694385 valid_loss 0.84987319 test_loss 0.81192076 \n",
      "266 train_loss 0.78695720 valid_loss 0.84967434 test_loss 0.81188530 \n",
      "267 train_loss 0.78685284 valid_loss 0.84975845 test_loss 0.81185383 \n",
      "268 train_loss 0.78686845 valid_loss 0.84973830 test_loss 0.81185389 \n",
      "269 train_loss 0.78685224 valid_loss 0.84989685 test_loss 0.81190324 \n",
      "270 train_loss 0.78708446 valid_loss 0.85036355 test_loss 0.81224555 \n",
      "271 train_loss 0.78818619 valid_loss 0.85207582 test_loss 0.81356925 \n",
      "272 train_loss 0.78694981 valid_loss 0.84999198 test_loss 0.81204242 \n",
      "273 train_loss 0.78751141 valid_loss 0.84978563 test_loss 0.81214118 \n",
      "274 train_loss 0.78896302 valid_loss 0.85090190 test_loss 0.81333393 \n",
      "275 train_loss 0.78725904 valid_loss 0.84966111 test_loss 0.81191373 \n",
      "276 train_loss 0.78941947 valid_loss 0.85384715 test_loss 0.81501985 \n",
      "277 train_loss 0.78858429 valid_loss 0.85262167 test_loss 0.81402969 \n",
      "278 train_loss 0.78783888 valid_loss 0.85013336 test_loss 0.81241345 \n",
      "279 train_loss 0.78702068 valid_loss 0.84970349 test_loss 0.81176811 \n",
      "280 train_loss 0.78839624 valid_loss 0.85058188 test_loss 0.81284350 \n",
      "281 train_loss 0.78690046 valid_loss 0.84974939 test_loss 0.81178218 \n",
      "282 train_loss 0.78699011 valid_loss 0.85008782 test_loss 0.81211185 \n",
      "283 train_loss 0.78686082 valid_loss 0.84990352 test_loss 0.81194460 \n",
      "284 train_loss 0.78695697 valid_loss 0.85010415 test_loss 0.81200916 \n",
      "285 train_loss 0.78692627 valid_loss 0.84986168 test_loss 0.81192762 \n",
      "286 train_loss 0.78765929 valid_loss 0.85106194 test_loss 0.81296515 \n",
      "287 train_loss 0.78690058 valid_loss 0.84970421 test_loss 0.81188828 \n",
      "288 train_loss 0.78929603 valid_loss 0.85370642 test_loss 0.81507540 \n",
      "289 train_loss 0.78810132 valid_loss 0.85007626 test_loss 0.81259936 \n",
      "290 train_loss 0.78697759 valid_loss 0.84967792 test_loss 0.81191546 \n",
      "291 train_loss 0.78810900 valid_loss 0.85042953 test_loss 0.81269717 \n",
      "292 train_loss 0.78685057 valid_loss 0.84989429 test_loss 0.81183493 \n",
      "293 train_loss 0.78691006 valid_loss 0.84963816 test_loss 0.81184971 \n",
      "294 train_loss 0.78731769 valid_loss 0.84977371 test_loss 0.81208479 \n",
      "295 train_loss 0.78720349 valid_loss 0.85074973 test_loss 0.81246573 \n",
      "296 train_loss 0.78705502 valid_loss 0.85001957 test_loss 0.81214321 \n",
      "297 train_loss 0.78851002 valid_loss 0.85254294 test_loss 0.81383187 \n",
      "298 train_loss 0.78743690 valid_loss 0.85108995 test_loss 0.81278884 \n",
      "299 train_loss 0.78843975 valid_loss 0.85266054 test_loss 0.81408775 \n",
      "300 train_loss 0.78692633 valid_loss 0.84998631 test_loss 0.81185740 \n",
      "301 train_loss 0.79010743 valid_loss 0.85173285 test_loss 0.81449574 \n",
      "302 train_loss 0.78769547 valid_loss 0.85139430 test_loss 0.81310117 \n",
      "303 train_loss 0.78706324 valid_loss 0.84953940 test_loss 0.81176978 \n",
      "304 train_loss 0.78716791 valid_loss 0.84981537 test_loss 0.81193602 \n",
      "305 train_loss 0.78687221 valid_loss 0.84969211 test_loss 0.81177264 \n",
      "306 train_loss 0.79013705 valid_loss 0.85199511 test_loss 0.81461740 \n",
      "307 train_loss 0.78694296 valid_loss 0.84978986 test_loss 0.81184655 \n",
      "308 train_loss 0.78922802 valid_loss 0.85355955 test_loss 0.81510335 \n",
      "309 train_loss 0.78736573 valid_loss 0.85085320 test_loss 0.81261152 \n",
      "310 train_loss 0.78737015 valid_loss 0.85089618 test_loss 0.81255394 \n",
      "311 train_loss 0.78734863 valid_loss 0.85090590 test_loss 0.81268883 \n",
      "312 train_loss 0.78700650 valid_loss 0.85018414 test_loss 0.81236452 \n",
      "313 train_loss 0.78714985 valid_loss 0.85055089 test_loss 0.81244600 \n",
      "314 train_loss 0.78688186 valid_loss 0.84996796 test_loss 0.81193995 \n",
      "315 train_loss 0.78802419 valid_loss 0.85205919 test_loss 0.81355506 \n",
      "316 train_loss 0.78812653 valid_loss 0.85025942 test_loss 0.81282306 \n",
      "317 train_loss 0.78720629 valid_loss 0.84985769 test_loss 0.81190079 \n",
      "318 train_loss 0.78708386 valid_loss 0.84970349 test_loss 0.81198174 \n",
      "319 train_loss 0.78692055 valid_loss 0.84970826 test_loss 0.81184840 \n",
      "320 train_loss 0.78728926 valid_loss 0.85091478 test_loss 0.81252086 \n",
      "321 train_loss 0.78714556 valid_loss 0.84970427 test_loss 0.81205249 \n",
      "322 train_loss 0.78692889 valid_loss 0.84970218 test_loss 0.81184816 \n",
      "323 train_loss 0.78912646 valid_loss 0.85103744 test_loss 0.81365538 \n",
      "324 train_loss 0.78739715 valid_loss 0.85001779 test_loss 0.81220156 \n",
      "325 train_loss 0.78693420 valid_loss 0.85009360 test_loss 0.81214243 \n",
      "326 train_loss 0.78961581 valid_loss 0.85135686 test_loss 0.81409514 \n",
      "327 train_loss 0.79216123 valid_loss 0.85729444 test_loss 0.81825334 \n",
      "328 train_loss 0.78711563 valid_loss 0.85052609 test_loss 0.81233597 \n",
      "329 train_loss 0.78804910 valid_loss 0.85200471 test_loss 0.81362647 \n",
      "330 train_loss 0.78689808 valid_loss 0.84999341 test_loss 0.81198770 \n",
      "331 train_loss 0.78782439 valid_loss 0.85135436 test_loss 0.81319910 \n",
      "332 train_loss 0.78765553 valid_loss 0.85005438 test_loss 0.81234044 \n",
      "333 train_loss 0.78702992 valid_loss 0.85036480 test_loss 0.81209993 \n",
      "334 train_loss 0.78759187 valid_loss 0.85131079 test_loss 0.81304234 \n",
      "335 train_loss 0.78716683 valid_loss 0.84976274 test_loss 0.81215763 \n",
      "336 train_loss 0.78883410 valid_loss 0.85088527 test_loss 0.81334460 \n",
      "337 train_loss 0.78979051 valid_loss 0.85437685 test_loss 0.81551272 \n",
      "338 train_loss 0.78869778 valid_loss 0.85287309 test_loss 0.81425142 \n",
      "339 train_loss 0.78906572 valid_loss 0.85115474 test_loss 0.81361729 \n",
      "340 train_loss 0.78687876 valid_loss 0.85006201 test_loss 0.81201106 \n",
      "341 train_loss 0.78762484 valid_loss 0.85121804 test_loss 0.81308717 \n",
      "342 train_loss 0.78693569 valid_loss 0.84989136 test_loss 0.81179607 \n",
      "343 train_loss 0.78712970 valid_loss 0.84984863 test_loss 0.81201690 \n",
      "344 train_loss 0.78761744 valid_loss 0.85138834 test_loss 0.81297636 \n",
      "345 train_loss 0.78719860 valid_loss 0.84980184 test_loss 0.81192207 \n",
      "346 train_loss 0.78793359 valid_loss 0.85015547 test_loss 0.81248546 \n",
      "347 train_loss 0.78760880 valid_loss 0.85128921 test_loss 0.81295049 \n",
      "348 train_loss 0.78769851 valid_loss 0.84992462 test_loss 0.81224555 \n",
      "349 train_loss 0.78705591 valid_loss 0.84970236 test_loss 0.81186628 \n",
      "350 train_loss 0.78706199 valid_loss 0.85007024 test_loss 0.81193221 \n",
      "351 train_loss 0.78703123 valid_loss 0.85022658 test_loss 0.81228673 \n",
      "352 train_loss 0.78688860 valid_loss 0.84998834 test_loss 0.81199497 \n",
      "353 train_loss 0.78686148 valid_loss 0.84984863 test_loss 0.81201518 \n",
      "354 train_loss 0.78700942 valid_loss 0.84984529 test_loss 0.81186485 \n",
      "355 train_loss 0.78696120 valid_loss 0.84989125 test_loss 0.81197345 \n",
      "356 train_loss 0.78708553 valid_loss 0.85035378 test_loss 0.81242543 \n",
      "357 train_loss 0.78706145 valid_loss 0.85034120 test_loss 0.81218117 \n",
      "358 train_loss 0.78725690 valid_loss 0.84996790 test_loss 0.81238437 \n",
      "359 train_loss 0.78743976 valid_loss 0.85042518 test_loss 0.81251347 \n",
      "360 train_loss 0.78763449 valid_loss 0.85130489 test_loss 0.81306571 \n",
      "361 train_loss 0.78734791 valid_loss 0.84974116 test_loss 0.81203252 \n",
      "362 train_loss 0.78735012 valid_loss 0.85005188 test_loss 0.81221926 \n",
      "363 train_loss 0.78764194 valid_loss 0.85135210 test_loss 0.81300592 \n",
      "364 train_loss 0.78813201 valid_loss 0.85220069 test_loss 0.81367916 \n",
      "365 train_loss 0.78878701 valid_loss 0.85084981 test_loss 0.81338853 \n",
      "366 train_loss 0.79065353 valid_loss 0.85551876 test_loss 0.81667310 \n",
      "367 train_loss 0.78737038 valid_loss 0.84978676 test_loss 0.81196350 \n",
      "368 train_loss 0.78699368 valid_loss 0.85018945 test_loss 0.81206769 \n",
      "369 train_loss 0.78694087 valid_loss 0.84972268 test_loss 0.81184095 \n",
      "370 train_loss 0.78695196 valid_loss 0.85019344 test_loss 0.81209159 \n",
      "371 train_loss 0.79088926 valid_loss 0.85576588 test_loss 0.81689322 \n",
      "Accuracy of validation is CRASH !!\n",
      "Training done, save model at VL: = 0.8494164943695068\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, x_test, y_train, y_valid, y_test = getData_Advice()\n",
    "train_dl = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), shuffle=True, batch_size=128, pin_memory=True) \n",
    "x_train = x_train.to(device)\n",
    "x_valid = x_valid.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_valid = y_valid.to(device)\n",
    "x_test = x_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "model = Advice(16)\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "minVL = 0\n",
    "minVL_ep = 0\n",
    "\n",
    "for e in range(1000):\n",
    "    ep = e + 1\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb.to(device))\n",
    "        loss = criterion(pred, yb.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = float(criterion(model(x_train), y_train))\n",
    "    valid_loss = float(criterion(model(x_valid), y_valid))\n",
    "    test_loss = float(criterion(model(x_test), y_test))\n",
    "    save = ''\n",
    "    if (minVL > valid_loss) | (minVL_ep < 1):\n",
    "        minVL = valid_loss\n",
    "        minVL_ep = ep\n",
    "        save = '< save'\n",
    "        torch.save(model, 'model_pytorch/Advice.pt')\n",
    "    print(ep, \n",
    "        'train_loss', '{:.8f}'.format(train_loss), \n",
    "        'valid_loss', '{:.8f}'.format(valid_loss), \n",
    "        'test_loss', '{:.8f}'.format(test_loss), \n",
    "        save)\n",
    "    ## early drop\n",
    "    if ep < 200: ## 至少執行200次\n",
    "        pass\n",
    "    elif minVL_ep < ep/2: ## 連續10次小於max_VA的一半\n",
    "        print('Accuracy of validation is CRASH !!')\n",
    "        break\n",
    "## 轉換成CPU版本後儲存\n",
    "model = torch.load('model_pytorch/Advice.pt')\n",
    "model = model.to(torch.device('cpu'))\n",
    "torch.save(model, 'model_pytorch/Advice.pt')\n",
    "print('Training done, save model at VL: =', minVL)\n",
    "os.makedirs('record', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing model: 企業倫理\n",
      "original RMSE: 0.9484657049179077\n",
      "企業倫理 | 1 train_loss 0.74814981 valid_loss 0.77662092 test_loss 0.67527670 < save\n",
      "企業倫理 | 2 train_loss 0.72194314 valid_loss 0.75070220 test_loss 0.65058929 < save\n",
      "企業倫理 | 3 train_loss 0.71191007 valid_loss 0.74093443 test_loss 0.64196730 < save\n",
      "企業倫理 | 4 train_loss 0.70377028 valid_loss 0.73318827 test_loss 0.63429654 < save\n",
      "企業倫理 | 5 train_loss 0.70220888 valid_loss 0.73141581 test_loss 0.63401538 < save\n",
      "企業倫理 | 6 train_loss 0.69531828 valid_loss 0.72504354 test_loss 0.62649530 < save\n",
      "企業倫理 | 7 train_loss 0.69236875 valid_loss 0.72196078 test_loss 0.62424946 < save\n",
      "企業倫理 | 8 train_loss 0.68971425 valid_loss 0.71934116 test_loss 0.62203223 < save\n",
      "企業倫理 | 9 train_loss 0.68746942 valid_loss 0.71720827 test_loss 0.61981022 < save\n",
      "企業倫理 | 10 train_loss 0.68627441 valid_loss 0.71611524 test_loss 0.61853027 < save\n",
      "企業倫理 | 11 train_loss 0.68414634 valid_loss 0.71355134 test_loss 0.61704588 < save\n",
      "企業倫理 | 12 train_loss 0.69804686 valid_loss 0.72712225 test_loss 0.63347965 \n",
      "企業倫理 | 13 train_loss 0.68159735 valid_loss 0.71096808 test_loss 0.61509722 < save\n",
      "企業倫理 | 14 train_loss 0.68306470 valid_loss 0.71197146 test_loss 0.61765891 \n",
      "企業倫理 | 15 train_loss 0.68616468 valid_loss 0.71452475 test_loss 0.62126309 \n",
      "企業倫理 | 16 train_loss 0.67863005 valid_loss 0.70767605 test_loss 0.61263430 < save\n",
      "企業倫理 | 17 train_loss 0.67791128 valid_loss 0.70689845 test_loss 0.61184955 < save\n",
      "企業倫理 | 18 train_loss 0.67703575 valid_loss 0.70613897 test_loss 0.61162877 < save\n",
      "企業倫理 | 19 train_loss 0.67873377 valid_loss 0.70731682 test_loss 0.61415291 \n",
      "企業倫理 | 20 train_loss 0.67592174 valid_loss 0.70476139 test_loss 0.61029303 < save\n",
      "企業倫理 | 21 train_loss 0.67539489 valid_loss 0.70454663 test_loss 0.61008549 < save\n",
      "企業倫理 | 22 train_loss 0.67427725 valid_loss 0.70306367 test_loss 0.60957813 < save\n",
      "企業倫理 | 23 train_loss 0.67482883 valid_loss 0.70332915 test_loss 0.61075842 \n",
      "企業倫理 | 24 train_loss 0.67314088 valid_loss 0.70157564 test_loss 0.60877120 < save\n",
      "企業倫理 | 25 train_loss 0.67479718 valid_loss 0.70276725 test_loss 0.61108476 \n",
      "企業倫理 | 26 train_loss 0.67478567 valid_loss 0.70282382 test_loss 0.61135995 \n",
      "企業倫理 | 27 train_loss 0.67252320 valid_loss 0.70076025 test_loss 0.60890853 < save\n",
      "企業倫理 | 28 train_loss 0.67286170 valid_loss 0.70168340 test_loss 0.60814369 \n",
      "企業倫理 | 29 train_loss 0.67009163 valid_loss 0.69812518 test_loss 0.60593086 < save\n",
      "企業倫理 | 30 train_loss 0.66997319 valid_loss 0.69861227 test_loss 0.60594553 \n",
      "企業倫理 | 31 train_loss 0.66909683 valid_loss 0.69741291 test_loss 0.60526854 < save\n",
      "企業倫理 | 32 train_loss 0.67479116 valid_loss 0.70252275 test_loss 0.61219394 \n",
      "企業倫理 | 33 train_loss 0.66844243 valid_loss 0.69659925 test_loss 0.60519320 < save\n",
      "企業倫理 | 34 train_loss 0.66780591 valid_loss 0.69588917 test_loss 0.60412973 < save\n",
      "企業倫理 | 35 train_loss 0.66873544 valid_loss 0.69678569 test_loss 0.60450935 \n",
      "企業倫理 | 36 train_loss 0.66669971 valid_loss 0.69427586 test_loss 0.60318673 < save\n",
      "企業倫理 | 37 train_loss 0.66754407 valid_loss 0.69568354 test_loss 0.60368127 \n",
      "企業倫理 | 38 train_loss 0.67309183 valid_loss 0.70176315 test_loss 0.60855877 \n",
      "企業倫理 | 39 train_loss 0.66536003 valid_loss 0.69290674 test_loss 0.60244161 < save\n",
      "企業倫理 | 40 train_loss 0.66676372 valid_loss 0.69464248 test_loss 0.60302377 \n",
      "企業倫理 | 41 train_loss 0.66461647 valid_loss 0.69224501 test_loss 0.60190737 < save\n",
      "企業倫理 | 42 train_loss 0.66553223 valid_loss 0.69287699 test_loss 0.60315698 \n",
      "企業倫理 | 43 train_loss 0.66414481 valid_loss 0.69093084 test_loss 0.60147727 < save\n",
      "企業倫理 | 44 train_loss 0.66328758 valid_loss 0.69069278 test_loss 0.60047966 < save\n",
      "企業倫理 | 45 train_loss 0.66557914 valid_loss 0.69251871 test_loss 0.60381436 \n",
      "企業倫理 | 46 train_loss 0.66285795 valid_loss 0.68969321 test_loss 0.59993643 < save\n",
      "企業倫理 | 47 train_loss 0.66403091 valid_loss 0.69166154 test_loss 0.60097545 \n",
      "企業倫理 | 48 train_loss 0.66247737 valid_loss 0.68982887 test_loss 0.59972656 \n",
      "企業倫理 | 49 train_loss 0.66189921 valid_loss 0.68915725 test_loss 0.59925020 < save\n",
      "企業倫理 | 50 train_loss 0.66254425 valid_loss 0.68964571 test_loss 0.60117108 \n",
      "企業倫理 | 51 train_loss 0.66261286 valid_loss 0.68959647 test_loss 0.60129058 \n",
      "企業倫理 | 52 train_loss 0.66114837 valid_loss 0.68826026 test_loss 0.59969848 < save\n",
      "企業倫理 | 53 train_loss 0.65975636 valid_loss 0.68646866 test_loss 0.59809899 < save\n",
      "企業倫理 | 54 train_loss 0.66078430 valid_loss 0.68792135 test_loss 0.59848297 \n",
      "企業倫理 | 55 train_loss 0.65909928 valid_loss 0.68585300 test_loss 0.59763247 < save\n",
      "企業倫理 | 56 train_loss 0.65928078 valid_loss 0.68540955 test_loss 0.59788251 < save\n",
      "企業倫理 | 57 train_loss 0.65852320 valid_loss 0.68514699 test_loss 0.59732759 < save\n",
      "企業倫理 | 58 train_loss 0.65877521 valid_loss 0.68564862 test_loss 0.59697431 \n",
      "企業倫理 | 59 train_loss 0.66013354 valid_loss 0.68626624 test_loss 0.59977818 \n",
      "企業倫理 | 60 train_loss 0.66396099 valid_loss 0.69119513 test_loss 0.60146123 \n",
      "企業倫理 | 61 train_loss 0.65749329 valid_loss 0.68353897 test_loss 0.59666812 < save\n",
      "企業倫理 | 62 train_loss 0.66459507 valid_loss 0.69116098 test_loss 0.60536510 \n",
      "企業倫理 | 63 train_loss 0.66141784 valid_loss 0.68802643 test_loss 0.60193187 \n",
      "企業倫理 | 64 train_loss 0.65681100 valid_loss 0.68318009 test_loss 0.59566891 < save\n",
      "企業倫理 | 65 train_loss 0.65640473 valid_loss 0.68286437 test_loss 0.59566569 < save\n",
      "企業倫理 | 66 train_loss 0.65644538 valid_loss 0.68273479 test_loss 0.59542501 < save\n",
      "企業倫理 | 67 train_loss 0.65686375 valid_loss 0.68330562 test_loss 0.59577107 \n",
      "企業倫理 | 68 train_loss 0.65644920 valid_loss 0.68285924 test_loss 0.59545046 \n",
      "企業倫理 | 69 train_loss 0.65789896 valid_loss 0.68426758 test_loss 0.59663564 \n",
      "企業倫理 | 70 train_loss 0.65550649 valid_loss 0.68201953 test_loss 0.59523201 < save\n",
      "企業倫理 | 71 train_loss 0.65558028 valid_loss 0.68120927 test_loss 0.59507328 < save\n",
      "企業倫理 | 72 train_loss 0.65652436 valid_loss 0.68317008 test_loss 0.59586602 \n",
      "企業倫理 | 73 train_loss 0.66597474 valid_loss 0.69218767 test_loss 0.60783017 \n",
      "企業倫理 | 74 train_loss 0.65534431 valid_loss 0.68129075 test_loss 0.59562224 \n",
      "企業倫理 | 75 train_loss 0.65677595 valid_loss 0.68297732 test_loss 0.59773153 \n",
      "企業倫理 | 76 train_loss 0.65947944 valid_loss 0.68544883 test_loss 0.60082889 \n",
      "企業倫理 | 77 train_loss 0.65451008 valid_loss 0.68047875 test_loss 0.59421033 < save\n",
      "企業倫理 | 78 train_loss 0.66823250 valid_loss 0.69420213 test_loss 0.61102200 \n",
      "企業倫理 | 79 train_loss 0.65188074 valid_loss 0.67746717 test_loss 0.59264672 < save\n",
      "企業倫理 | 80 train_loss 0.65483212 valid_loss 0.68073177 test_loss 0.59446001 \n",
      "企業倫理 | 81 train_loss 0.66054189 valid_loss 0.68590969 test_loss 0.60316926 \n",
      "企業倫理 | 82 train_loss 0.65116239 valid_loss 0.67690331 test_loss 0.59176236 < save\n",
      "企業倫理 | 83 train_loss 0.65179873 valid_loss 0.67757034 test_loss 0.59324592 \n",
      "企業倫理 | 84 train_loss 0.65557504 valid_loss 0.68120873 test_loss 0.59798622 \n",
      "企業倫理 | 85 train_loss 0.65436238 valid_loss 0.67971623 test_loss 0.59639233 \n",
      "企業倫理 | 86 train_loss 0.65195507 valid_loss 0.67814106 test_loss 0.59379709 \n",
      "企業倫理 | 87 train_loss 0.65049994 valid_loss 0.67597497 test_loss 0.59148663 < save\n",
      "企業倫理 | 88 train_loss 0.65472412 valid_loss 0.68059558 test_loss 0.59731841 \n",
      "企業倫理 | 89 train_loss 0.65051633 valid_loss 0.67581874 test_loss 0.59134126 < save\n",
      "企業倫理 | 90 train_loss 0.65218991 valid_loss 0.67737353 test_loss 0.59430110 \n",
      "企業倫理 | 91 train_loss 0.65018737 valid_loss 0.67561227 test_loss 0.59182376 < save\n",
      "企業倫理 | 92 train_loss 0.65654165 valid_loss 0.68273628 test_loss 0.59666812 \n",
      "企業倫理 | 93 train_loss 0.65037447 valid_loss 0.67573899 test_loss 0.59143937 \n",
      "企業倫理 | 94 train_loss 0.64977849 valid_loss 0.67528546 test_loss 0.59131002 < save\n",
      "企業倫理 | 95 train_loss 0.64989191 valid_loss 0.67539227 test_loss 0.59126163 \n",
      "企業倫理 | 96 train_loss 0.65215135 valid_loss 0.67768633 test_loss 0.59298861 \n",
      "企業倫理 | 97 train_loss 0.65110272 valid_loss 0.67682517 test_loss 0.59219205 \n",
      "企業倫理 | 98 train_loss 0.65721339 valid_loss 0.68282634 test_loss 0.60074520 \n",
      "企業倫理 | 99 train_loss 0.65173864 valid_loss 0.67712677 test_loss 0.59253794 \n",
      "企業倫理 | 100 train_loss 0.65100420 valid_loss 0.67669672 test_loss 0.59379417 \n",
      "企業倫理 | 101 train_loss 0.64933318 valid_loss 0.67478877 test_loss 0.59123379 < save\n",
      "企業倫理 | 102 train_loss 0.65799838 valid_loss 0.68349659 test_loss 0.60183269 \n",
      "企業倫理 | 103 train_loss 0.65083718 valid_loss 0.67639810 test_loss 0.59368664 \n",
      "企業倫理 | 104 train_loss 0.65797853 valid_loss 0.68373442 test_loss 0.59852946 \n",
      "企業倫理 | 105 train_loss 0.64938438 valid_loss 0.67452508 test_loss 0.59175503 < save\n",
      "企業倫理 | 106 train_loss 0.65017873 valid_loss 0.67513382 test_loss 0.59284967 \n",
      "企業倫理 | 107 train_loss 0.64918238 valid_loss 0.67430234 test_loss 0.59127122 < save\n",
      "企業倫理 | 108 train_loss 0.64933395 valid_loss 0.67441726 test_loss 0.59188956 \n",
      "企業倫理 | 109 train_loss 0.64947945 valid_loss 0.67420810 test_loss 0.59201777 < save\n",
      "企業倫理 | 110 train_loss 0.64904797 valid_loss 0.67412204 test_loss 0.59094495 < save\n",
      "企業倫理 | 111 train_loss 0.65646780 valid_loss 0.68144178 test_loss 0.60051674 \n",
      "企業倫理 | 112 train_loss 0.65182614 valid_loss 0.67667276 test_loss 0.59517586 \n",
      "企業倫理 | 113 train_loss 0.64893788 valid_loss 0.67389816 test_loss 0.59154218 < save\n",
      "企業倫理 | 114 train_loss 0.66413414 valid_loss 0.69045109 test_loss 0.60481942 \n",
      "企業倫理 | 115 train_loss 0.64901078 valid_loss 0.67400944 test_loss 0.59096754 \n",
      "企業倫理 | 116 train_loss 0.65539849 valid_loss 0.68135965 test_loss 0.59668624 \n",
      "企業倫理 | 117 train_loss 0.64873588 valid_loss 0.67393249 test_loss 0.59132868 \n",
      "企業倫理 | 118 train_loss 0.65198189 valid_loss 0.67711276 test_loss 0.59349090 \n",
      "企業倫理 | 119 train_loss 0.65414703 valid_loss 0.67944473 test_loss 0.59562927 \n",
      "企業倫理 | 120 train_loss 0.64868039 valid_loss 0.67363632 test_loss 0.59098023 < save\n",
      "企業倫理 | 121 train_loss 0.64845437 valid_loss 0.67298758 test_loss 0.59119004 < save\n",
      "企業倫理 | 122 train_loss 0.65047103 valid_loss 0.67544627 test_loss 0.59422624 \n",
      "企業倫理 | 123 train_loss 0.65014738 valid_loss 0.67474943 test_loss 0.59211868 \n",
      "企業倫理 | 124 train_loss 0.64826542 valid_loss 0.67297363 test_loss 0.59114707 < save\n",
      "企業倫理 | 125 train_loss 0.66350639 valid_loss 0.68866813 test_loss 0.60896748 \n",
      "企業倫理 | 126 train_loss 0.65036368 valid_loss 0.67456645 test_loss 0.59388739 \n",
      "企業倫理 | 127 train_loss 0.64823484 valid_loss 0.67320514 test_loss 0.59114134 \n",
      "企業倫理 | 128 train_loss 0.65351737 valid_loss 0.67822260 test_loss 0.59798926 \n",
      "企業倫理 | 129 train_loss 0.65225828 valid_loss 0.67778826 test_loss 0.59428787 \n",
      "企業倫理 | 130 train_loss 0.64884657 valid_loss 0.67375135 test_loss 0.59239578 \n",
      "企業倫理 | 131 train_loss 0.64959812 valid_loss 0.67441577 test_loss 0.59187204 \n",
      "企業倫理 | 132 train_loss 0.64875638 valid_loss 0.67323226 test_loss 0.59124285 \n",
      "企業倫理 | 133 train_loss 0.64870518 valid_loss 0.67340571 test_loss 0.59132582 \n",
      "企業倫理 | 134 train_loss 0.64793444 valid_loss 0.67263412 test_loss 0.59103161 < save\n",
      "企業倫理 | 135 train_loss 0.64838725 valid_loss 0.67302221 test_loss 0.59204233 \n",
      "企業倫理 | 136 train_loss 0.64863634 valid_loss 0.67376190 test_loss 0.59231919 \n",
      "企業倫理 | 137 train_loss 0.66540265 valid_loss 0.69001293 test_loss 0.61136198 \n",
      "企業倫理 | 138 train_loss 0.65007651 valid_loss 0.67488873 test_loss 0.59246284 \n",
      "企業倫理 | 139 train_loss 0.65122175 valid_loss 0.67631936 test_loss 0.59345365 \n",
      "企業倫理 | 140 train_loss 0.64803678 valid_loss 0.67256904 test_loss 0.59154588 < save\n",
      "企業倫理 | 141 train_loss 0.64822048 valid_loss 0.67268640 test_loss 0.59182924 \n",
      "企業倫理 | 142 train_loss 0.64868355 valid_loss 0.67337745 test_loss 0.59252071 \n",
      "企業倫理 | 143 train_loss 0.64801759 valid_loss 0.67252886 test_loss 0.59104639 < save\n",
      "企業倫理 | 144 train_loss 0.65083063 valid_loss 0.67600340 test_loss 0.59339023 \n",
      "企業倫理 | 145 train_loss 0.65469074 valid_loss 0.67951620 test_loss 0.59673113 \n",
      "企業倫理 | 146 train_loss 0.64841849 valid_loss 0.67293465 test_loss 0.59141201 \n",
      "企業倫理 | 147 train_loss 0.64776838 valid_loss 0.67239964 test_loss 0.59124750 < save\n",
      "企業倫理 | 148 train_loss 0.64787191 valid_loss 0.67227829 test_loss 0.59159082 < save\n",
      "企業倫理 | 149 train_loss 0.65054208 valid_loss 0.67509788 test_loss 0.59517241 \n",
      "企業倫理 | 150 train_loss 0.64946967 valid_loss 0.67411309 test_loss 0.59389472 \n",
      "企業倫理 | 151 train_loss 0.65175843 valid_loss 0.67673808 test_loss 0.59422171 \n",
      "企業倫理 | 152 train_loss 0.64771670 valid_loss 0.67231023 test_loss 0.59099287 \n",
      "企業倫理 | 153 train_loss 0.64845765 valid_loss 0.67267668 test_loss 0.59151345 \n",
      "企業倫理 | 154 train_loss 0.64771086 valid_loss 0.67194086 test_loss 0.59130150 < save\n",
      "企業倫理 | 155 train_loss 0.64871991 valid_loss 0.67343938 test_loss 0.59288806 \n",
      "企業倫理 | 156 train_loss 0.64800739 valid_loss 0.67294806 test_loss 0.59157449 \n",
      "企業倫理 | 157 train_loss 0.64801663 valid_loss 0.67261851 test_loss 0.59135902 \n",
      "企業倫理 | 158 train_loss 0.64989299 valid_loss 0.67425758 test_loss 0.59459603 \n",
      "企業倫理 | 159 train_loss 0.64800608 valid_loss 0.67269307 test_loss 0.59202850 \n",
      "企業倫理 | 160 train_loss 0.64792234 valid_loss 0.67263919 test_loss 0.59152347 \n",
      "企業倫理 | 161 train_loss 0.64808148 valid_loss 0.67197865 test_loss 0.59200889 \n",
      "企業倫理 | 162 train_loss 0.64939857 valid_loss 0.67395437 test_loss 0.59236795 \n",
      "企業倫理 | 163 train_loss 0.65350485 valid_loss 0.67801422 test_loss 0.59876251 \n",
      "企業倫理 | 164 train_loss 0.65185589 valid_loss 0.67657077 test_loss 0.59464496 \n",
      "企業倫理 | 165 train_loss 0.64889997 valid_loss 0.67341274 test_loss 0.59199315 \n",
      "企業倫理 | 166 train_loss 0.65414691 valid_loss 0.67853636 test_loss 0.59973413 \n",
      "企業倫理 | 167 train_loss 0.64834225 valid_loss 0.67264920 test_loss 0.59169084 \n",
      "企業倫理 | 168 train_loss 0.64776063 valid_loss 0.67244154 test_loss 0.59171414 \n",
      "企業倫理 | 169 train_loss 0.64756161 valid_loss 0.67164230 test_loss 0.59167904 < save\n",
      "企業倫理 | 170 train_loss 0.64929944 valid_loss 0.67385751 test_loss 0.59245670 \n",
      "企業倫理 | 171 train_loss 0.65410537 valid_loss 0.67883891 test_loss 0.59663880 \n",
      "企業倫理 | 172 train_loss 0.65507179 valid_loss 0.67971379 test_loss 0.59756500 \n",
      "企業倫理 | 173 train_loss 0.64920127 valid_loss 0.67372090 test_loss 0.59395427 \n",
      "企業倫理 | 174 train_loss 0.65420890 valid_loss 0.67921036 test_loss 0.59677756 \n",
      "企業倫理 | 175 train_loss 0.64770854 valid_loss 0.67218357 test_loss 0.59189862 \n",
      "企業倫理 | 176 train_loss 0.65191644 valid_loss 0.67703098 test_loss 0.59488058 \n",
      "企業倫理 | 177 train_loss 0.64736682 valid_loss 0.67148739 test_loss 0.59142655 < save\n",
      "企業倫理 | 178 train_loss 0.65111965 valid_loss 0.67581302 test_loss 0.59401453 \n",
      "企業倫理 | 179 train_loss 0.64781153 valid_loss 0.67184347 test_loss 0.59132308 \n",
      "企業倫理 | 180 train_loss 0.65041786 valid_loss 0.67452931 test_loss 0.59550732 \n",
      "企業倫理 | 181 train_loss 0.64808279 valid_loss 0.67259043 test_loss 0.59153974 \n",
      "企業倫理 | 182 train_loss 0.64840853 valid_loss 0.67300367 test_loss 0.59314317 \n",
      "企業倫理 | 183 train_loss 0.65209609 valid_loss 0.67675447 test_loss 0.59757566 \n",
      "企業倫理 | 184 train_loss 0.66021609 valid_loss 0.68489540 test_loss 0.60687846 \n",
      "企業倫理 | 185 train_loss 0.64767027 valid_loss 0.67203712 test_loss 0.59136993 \n",
      "企業倫理 | 186 train_loss 0.65353227 valid_loss 0.67765212 test_loss 0.59933305 \n",
      "企業倫理 | 187 train_loss 0.64996922 valid_loss 0.67436314 test_loss 0.59528571 \n",
      "企業倫理 | 188 train_loss 0.64778018 valid_loss 0.67180479 test_loss 0.59247899 \n",
      "企業倫理 | 189 train_loss 0.64846832 valid_loss 0.67315447 test_loss 0.59223002 \n",
      "企業倫理 | 190 train_loss 0.64734316 valid_loss 0.67181629 test_loss 0.59147894 \n",
      "企業倫理 | 191 train_loss 0.65061802 valid_loss 0.67532158 test_loss 0.59385806 \n",
      "企業倫理 | 192 train_loss 0.65912461 valid_loss 0.68370396 test_loss 0.60567778 \n",
      "企業倫理 | 193 train_loss 0.64742070 valid_loss 0.67174762 test_loss 0.59140640 \n",
      "企業倫理 | 194 train_loss 0.64774203 valid_loss 0.67221874 test_loss 0.59233171 \n",
      "企業倫理 | 195 train_loss 0.65562278 valid_loss 0.68027955 test_loss 0.59836400 \n",
      "企業倫理 | 196 train_loss 0.65610904 valid_loss 0.68047255 test_loss 0.60235417 \n",
      "企業倫理 | 197 train_loss 0.64752501 valid_loss 0.67177486 test_loss 0.59145242 \n",
      "企業倫理 | 198 train_loss 0.64849252 valid_loss 0.67283130 test_loss 0.59353364 \n",
      "企業倫理 | 199 train_loss 0.64755797 valid_loss 0.67147851 test_loss 0.59217918 < save\n",
      "企業倫理 | 200 train_loss 0.64769328 valid_loss 0.67185605 test_loss 0.59244335 \n",
      "企業倫理 | 201 train_loss 0.64956331 valid_loss 0.67423135 test_loss 0.59311777 \n",
      "企業倫理 | 202 train_loss 0.64908379 valid_loss 0.67336059 test_loss 0.59258050 \n",
      "企業倫理 | 203 train_loss 0.64724934 valid_loss 0.67111892 test_loss 0.59147125 < save\n",
      "企業倫理 | 204 train_loss 0.64715016 valid_loss 0.67115396 test_loss 0.59130597 \n",
      "企業倫理 | 205 train_loss 0.64855987 valid_loss 0.67274946 test_loss 0.59218949 \n",
      "企業倫理 | 206 train_loss 0.65167791 valid_loss 0.67645705 test_loss 0.59489220 \n",
      "企業倫理 | 207 train_loss 0.64851248 valid_loss 0.67212981 test_loss 0.59321797 \n",
      "企業倫理 | 208 train_loss 0.64800292 valid_loss 0.67186058 test_loss 0.59201050 \n",
      "企業倫理 | 209 train_loss 0.64818692 valid_loss 0.67257339 test_loss 0.59312767 \n",
      "企業倫理 | 210 train_loss 0.64723891 valid_loss 0.67144680 test_loss 0.59167266 \n",
      "企業倫理 | 211 train_loss 0.64743263 valid_loss 0.67157656 test_loss 0.59206116 \n",
      "企業倫理 | 212 train_loss 0.64715648 valid_loss 0.67112046 test_loss 0.59142876 \n",
      "企業倫理 | 213 train_loss 0.64719468 valid_loss 0.67118657 test_loss 0.59160471 \n",
      "企業倫理 | 214 train_loss 0.65512472 valid_loss 0.68009627 test_loss 0.59798867 \n",
      "企業倫理 | 215 train_loss 0.64722520 valid_loss 0.67154837 test_loss 0.59162050 \n",
      "企業倫理 | 216 train_loss 0.64721692 valid_loss 0.67117429 test_loss 0.59179527 \n",
      "企業倫理 | 217 train_loss 0.64743030 valid_loss 0.67127585 test_loss 0.59186894 \n",
      "企業倫理 | 218 train_loss 0.65071881 valid_loss 0.67541766 test_loss 0.59429872 \n",
      "企業倫理 | 219 train_loss 0.64782834 valid_loss 0.67216480 test_loss 0.59177071 \n",
      "企業倫理 | 220 train_loss 0.65252620 valid_loss 0.67689037 test_loss 0.59858328 \n",
      "企業倫理 | 221 train_loss 0.64762962 valid_loss 0.67140871 test_loss 0.59158576 \n",
      "企業倫理 | 222 train_loss 0.64802378 valid_loss 0.67231119 test_loss 0.59194952 \n",
      "企業倫理 | 223 train_loss 0.64815158 valid_loss 0.67274588 test_loss 0.59237033 \n",
      "企業倫理 | 224 train_loss 0.65965462 valid_loss 0.68333554 test_loss 0.60626763 \n",
      "企業倫理 | 225 train_loss 0.64847499 valid_loss 0.67254883 test_loss 0.59222400 \n",
      "企業倫理 | 226 train_loss 0.64728868 valid_loss 0.67105597 test_loss 0.59160644 < save\n",
      "企業倫理 | 227 train_loss 0.65051460 valid_loss 0.67487401 test_loss 0.59404981 \n",
      "企業倫理 | 228 train_loss 0.64842319 valid_loss 0.67243379 test_loss 0.59365189 \n",
      "企業倫理 | 229 train_loss 0.64953679 valid_loss 0.67381734 test_loss 0.59322286 \n",
      "企業倫理 | 230 train_loss 0.64778477 valid_loss 0.67150879 test_loss 0.59274054 \n",
      "企業倫理 | 231 train_loss 0.64811385 valid_loss 0.67224652 test_loss 0.59196365 \n",
      "企業倫理 | 232 train_loss 0.64710116 valid_loss 0.67112094 test_loss 0.59163517 \n",
      "企業倫理 | 233 train_loss 0.65250117 valid_loss 0.67684782 test_loss 0.59562546 \n",
      "企業倫理 | 234 train_loss 0.65113699 valid_loss 0.67513567 test_loss 0.59679526 \n",
      "企業倫理 | 235 train_loss 0.64748192 valid_loss 0.67145282 test_loss 0.59149796 \n",
      "企業倫理 | 236 train_loss 0.64900428 valid_loss 0.67257404 test_loss 0.59431070 \n",
      "企業倫理 | 237 train_loss 0.65840912 valid_loss 0.68188494 test_loss 0.60486412 \n",
      "企業倫理 | 238 train_loss 0.65014088 valid_loss 0.67433101 test_loss 0.59362561 \n",
      "企業倫理 | 239 train_loss 0.64728111 valid_loss 0.67096865 test_loss 0.59194189 < save\n",
      "企業倫理 | 240 train_loss 0.64800447 valid_loss 0.67209339 test_loss 0.59325397 \n",
      "企業倫理 | 241 train_loss 0.65139925 valid_loss 0.67549145 test_loss 0.59737873 \n",
      "企業倫理 | 242 train_loss 0.64903569 valid_loss 0.67292029 test_loss 0.59448838 \n",
      "企業倫理 | 243 train_loss 0.64746940 valid_loss 0.67138058 test_loss 0.59245473 \n",
      "企業倫理 | 244 train_loss 0.64869374 valid_loss 0.67240077 test_loss 0.59249413 \n",
      "企業倫理 | 245 train_loss 0.64724034 valid_loss 0.67129844 test_loss 0.59151500 \n",
      "企業倫理 | 246 train_loss 0.64741659 valid_loss 0.67126209 test_loss 0.59153455 \n",
      "企業倫理 | 247 train_loss 0.64771730 valid_loss 0.67147464 test_loss 0.59267294 \n",
      "企業倫理 | 248 train_loss 0.64741951 valid_loss 0.67157823 test_loss 0.59151226 \n",
      "企業倫理 | 249 train_loss 0.64781624 valid_loss 0.67143565 test_loss 0.59272176 \n",
      "企業倫理 | 250 train_loss 0.65771145 valid_loss 0.68253887 test_loss 0.60056728 \n",
      "企業倫理 | 251 train_loss 0.64727986 valid_loss 0.67123264 test_loss 0.59203887 \n",
      "企業倫理 | 252 train_loss 0.64933789 valid_loss 0.67301685 test_loss 0.59474039 \n",
      "企業倫理 | 253 train_loss 0.65163296 valid_loss 0.67607629 test_loss 0.59503663 \n",
      "企業倫理 | 254 train_loss 0.64860767 valid_loss 0.67267597 test_loss 0.59247106 \n",
      "企業倫理 | 255 train_loss 0.64725828 valid_loss 0.67091703 test_loss 0.59177953 < save\n",
      "企業倫理 | 256 train_loss 0.64725637 valid_loss 0.67147022 test_loss 0.59167862 \n",
      "企業倫理 | 257 train_loss 0.64746815 valid_loss 0.67100334 test_loss 0.59177440 \n",
      "企業倫理 | 258 train_loss 0.65231234 valid_loss 0.67692101 test_loss 0.59571290 \n",
      "企業倫理 | 259 train_loss 0.64775944 valid_loss 0.67184919 test_loss 0.59189874 \n",
      "企業倫理 | 260 train_loss 0.64934176 valid_loss 0.67309642 test_loss 0.59484297 \n",
      "企業倫理 | 261 train_loss 0.64869773 valid_loss 0.67265117 test_loss 0.59422368 \n",
      "企業倫理 | 262 train_loss 0.65160191 valid_loss 0.67580134 test_loss 0.59513241 \n",
      "企業倫理 | 263 train_loss 0.65319580 valid_loss 0.67749470 test_loss 0.59653413 \n",
      "企業倫理 | 264 train_loss 0.65045393 valid_loss 0.67456394 test_loss 0.59410691 \n",
      "企業倫理 | 265 train_loss 0.64705992 valid_loss 0.67102557 test_loss 0.59159464 \n",
      "企業倫理 | 266 train_loss 0.64838570 valid_loss 0.67280585 test_loss 0.59372675 \n",
      "企業倫理 | 267 train_loss 0.64712757 valid_loss 0.67123485 test_loss 0.59197843 \n",
      "企業倫理 | 268 train_loss 0.65166324 valid_loss 0.67600256 test_loss 0.59793037 \n",
      "企業倫理 | 269 train_loss 0.64897114 valid_loss 0.67270929 test_loss 0.59448278 \n",
      "企業倫理 | 270 train_loss 0.64881462 valid_loss 0.67278427 test_loss 0.59435523 \n",
      "企業倫理 | 271 train_loss 0.64762551 valid_loss 0.67181468 test_loss 0.59189302 \n",
      "企業倫理 | 272 train_loss 0.64938873 valid_loss 0.67347914 test_loss 0.59324068 \n",
      "企業倫理 | 273 train_loss 0.64886767 valid_loss 0.67269480 test_loss 0.59440237 \n",
      "企業倫理 | 274 train_loss 0.65061563 valid_loss 0.67509454 test_loss 0.59445322 \n",
      "企業倫理 | 275 train_loss 0.64749259 valid_loss 0.67147958 test_loss 0.59266680 \n",
      "企業倫理 | 276 train_loss 0.64770693 valid_loss 0.67161012 test_loss 0.59195155 \n",
      "企業倫理 | 277 train_loss 0.64702195 valid_loss 0.67093301 test_loss 0.59183586 \n",
      "企業倫理 | 278 train_loss 0.64733589 valid_loss 0.67112827 test_loss 0.59180057 \n",
      "企業倫理 | 279 train_loss 0.64743340 valid_loss 0.67156804 test_loss 0.59188032 \n",
      "企業倫理 | 280 train_loss 0.66638619 valid_loss 0.69107461 test_loss 0.61442012 \n",
      "企業倫理 | 281 train_loss 0.64887196 valid_loss 0.67326713 test_loss 0.59291583 \n",
      "企業倫理 | 282 train_loss 0.64706349 valid_loss 0.67089713 test_loss 0.59174496 < save\n",
      "企業倫理 | 283 train_loss 0.65272021 valid_loss 0.67685848 test_loss 0.59905267 \n",
      "企業倫理 | 284 train_loss 0.65019929 valid_loss 0.67449814 test_loss 0.59395427 \n",
      "企業倫理 | 285 train_loss 0.65060902 valid_loss 0.67400020 test_loss 0.59644276 \n",
      "企業倫理 | 286 train_loss 0.64699757 valid_loss 0.67075711 test_loss 0.59175956 < save\n",
      "企業倫理 | 287 train_loss 0.64813089 valid_loss 0.67200071 test_loss 0.59372622 \n",
      "企業倫理 | 288 train_loss 0.65485185 valid_loss 0.67931491 test_loss 0.59817767 \n",
      "企業倫理 | 289 train_loss 0.65010351 valid_loss 0.67376292 test_loss 0.59593767 \n",
      "企業倫理 | 290 train_loss 0.64724857 valid_loss 0.67139947 test_loss 0.59192652 \n",
      "企業倫理 | 291 train_loss 0.65858775 valid_loss 0.68260121 test_loss 0.60570359 \n",
      "企業倫理 | 292 train_loss 0.64822376 valid_loss 0.67213583 test_loss 0.59252989 \n",
      "企業倫理 | 293 train_loss 0.64726728 valid_loss 0.67128170 test_loss 0.59178466 \n",
      "企業倫理 | 294 train_loss 0.64740521 valid_loss 0.67129725 test_loss 0.59255010 \n",
      "企業倫理 | 295 train_loss 0.64736938 valid_loss 0.67116737 test_loss 0.59252566 \n",
      "企業倫理 | 296 train_loss 0.66408968 valid_loss 0.68942899 test_loss 0.60666716 \n",
      "企業倫理 | 297 train_loss 0.64745736 valid_loss 0.67110145 test_loss 0.59264344 \n",
      "企業倫理 | 298 train_loss 0.65121639 valid_loss 0.67531753 test_loss 0.59489864 \n",
      "企業倫理 | 299 train_loss 0.65591949 valid_loss 0.68005019 test_loss 0.60270274 \n",
      "企業倫理 | 300 train_loss 0.64802742 valid_loss 0.67189831 test_loss 0.59340990 \n",
      "企業倫理 | 301 train_loss 0.64713073 valid_loss 0.67103451 test_loss 0.59156948 \n",
      "企業倫理 | 302 train_loss 0.64730626 valid_loss 0.67138547 test_loss 0.59169751 \n",
      "企業倫理 | 303 train_loss 0.64909536 valid_loss 0.67314005 test_loss 0.59302098 \n",
      "企業倫理 | 304 train_loss 0.64831710 valid_loss 0.67255616 test_loss 0.59236693 \n",
      "企業倫理 | 305 train_loss 0.64715368 valid_loss 0.67096967 test_loss 0.59164530 \n",
      "企業倫理 | 306 train_loss 0.64725417 valid_loss 0.67131031 test_loss 0.59171188 \n",
      "企業倫理 | 307 train_loss 0.64715999 valid_loss 0.67075515 test_loss 0.59212905 < save\n",
      "企業倫理 | 308 train_loss 0.65187442 valid_loss 0.67633969 test_loss 0.59541500 \n",
      "企業倫理 | 309 train_loss 0.65397424 valid_loss 0.67844772 test_loss 0.59730375 \n",
      "企業倫理 | 310 train_loss 0.65040547 valid_loss 0.67449301 test_loss 0.59640223 \n",
      "企業倫理 | 311 train_loss 0.64725530 valid_loss 0.67085910 test_loss 0.59241796 \n",
      "企業倫理 | 312 train_loss 0.64808202 valid_loss 0.67153257 test_loss 0.59335518 \n",
      "企業倫理 | 313 train_loss 0.65407300 valid_loss 0.67832863 test_loss 0.59731650 \n",
      "企業倫理 | 314 train_loss 0.66050702 valid_loss 0.68503851 test_loss 0.60795784 \n",
      "企業倫理 | 315 train_loss 0.65617073 valid_loss 0.68079948 test_loss 0.59939611 \n",
      "企業倫理 | 316 train_loss 0.64703357 valid_loss 0.67103934 test_loss 0.59179258 \n",
      "企業倫理 | 317 train_loss 0.64756310 valid_loss 0.67134702 test_loss 0.59290987 \n",
      "企業倫理 | 318 train_loss 0.64913946 valid_loss 0.67278689 test_loss 0.59487319 \n",
      "企業倫理 | 319 train_loss 0.64946359 valid_loss 0.67332995 test_loss 0.59530008 \n",
      "企業倫理 | 320 train_loss 0.65415627 valid_loss 0.67873627 test_loss 0.59743416 \n",
      "企業倫理 | 321 train_loss 0.64701939 valid_loss 0.67076385 test_loss 0.59190685 \n",
      "企業倫理 | 322 train_loss 0.64719546 valid_loss 0.67127955 test_loss 0.59203225 \n",
      "企業倫理 | 323 train_loss 0.66691530 valid_loss 0.69232124 test_loss 0.60931081 \n",
      "企業倫理 | 324 train_loss 0.64920676 valid_loss 0.67311752 test_loss 0.59331042 \n",
      "企業倫理 | 325 train_loss 0.64939237 valid_loss 0.67309254 test_loss 0.59332144 \n",
      "企業倫理 | 326 train_loss 0.64782792 valid_loss 0.67185670 test_loss 0.59314960 \n",
      "企業倫理 | 327 train_loss 0.65204132 valid_loss 0.67573094 test_loss 0.59561217 \n",
      "企業倫理 | 328 train_loss 0.65025151 valid_loss 0.67370814 test_loss 0.59618294 \n",
      "企業倫理 | 329 train_loss 0.64803481 valid_loss 0.67233872 test_loss 0.59223652 \n",
      "企業倫理 | 330 train_loss 0.64761269 valid_loss 0.67159730 test_loss 0.59189093 \n",
      "企業倫理 | 331 train_loss 0.64725757 valid_loss 0.67131913 test_loss 0.59188920 \n",
      "企業倫理 | 332 train_loss 0.65399408 valid_loss 0.67755628 test_loss 0.60054833 \n",
      "企業倫理 | 333 train_loss 0.65694916 valid_loss 0.68181008 test_loss 0.59998137 \n",
      "企業倫理 | 334 train_loss 0.64754546 valid_loss 0.67122155 test_loss 0.59268743 \n",
      "企業倫理 | 335 train_loss 0.64743853 valid_loss 0.67115116 test_loss 0.59266102 \n",
      "企業倫理 | 336 train_loss 0.65050566 valid_loss 0.67460519 test_loss 0.59652722 \n",
      "企業倫理 | 337 train_loss 0.64770186 valid_loss 0.67173189 test_loss 0.59194952 \n",
      "企業倫理 | 338 train_loss 0.65111029 valid_loss 0.67512578 test_loss 0.59468669 \n",
      "企業倫理 | 339 train_loss 0.64753377 valid_loss 0.67121845 test_loss 0.59192073 \n",
      "企業倫理 | 340 train_loss 0.65273064 valid_loss 0.67658591 test_loss 0.59917229 \n",
      "企業倫理 | 341 train_loss 0.64752465 valid_loss 0.67161584 test_loss 0.59193850 \n",
      "企業倫理 | 342 train_loss 0.64806986 valid_loss 0.67156655 test_loss 0.59336114 \n",
      "企業倫理 | 343 train_loss 0.65032375 valid_loss 0.67461693 test_loss 0.59419751 \n",
      "企業倫理 | 344 train_loss 0.64761269 valid_loss 0.67128444 test_loss 0.59279430 \n",
      "企業倫理 | 345 train_loss 0.64821303 valid_loss 0.67232186 test_loss 0.59368390 \n",
      "企業倫理 | 346 train_loss 0.65386653 valid_loss 0.67852587 test_loss 0.59724021 \n",
      "企業倫理 | 347 train_loss 0.64759165 valid_loss 0.67143095 test_loss 0.59298664 \n",
      "企業倫理 | 348 train_loss 0.65928400 valid_loss 0.68392164 test_loss 0.60215855 \n",
      "企業倫理 | 349 train_loss 0.64835423 valid_loss 0.67262125 test_loss 0.59247005 \n",
      "企業倫理 | 350 train_loss 0.64721304 valid_loss 0.67104089 test_loss 0.59237278 \n",
      "企業倫理 | 351 train_loss 0.64793032 valid_loss 0.67155689 test_loss 0.59342235 \n",
      "企業倫理 | 352 train_loss 0.65192336 valid_loss 0.67568225 test_loss 0.59825921 \n",
      "企業倫理 | 353 train_loss 0.64763290 valid_loss 0.67128479 test_loss 0.59296411 \n",
      "企業倫理 | 354 train_loss 0.64947283 valid_loss 0.67278713 test_loss 0.59525269 \n",
      "企業倫理 | 355 train_loss 0.65971029 valid_loss 0.68426037 test_loss 0.60258090 \n",
      "企業倫理 | 356 train_loss 0.65422612 valid_loss 0.67877191 test_loss 0.59766614 \n",
      "企業倫理 | 357 train_loss 0.64886153 valid_loss 0.67275226 test_loss 0.59296829 \n",
      "企業倫理 | 358 train_loss 0.64989507 valid_loss 0.67394286 test_loss 0.59380192 \n",
      "企業倫理 | 359 train_loss 0.64983726 valid_loss 0.67425174 test_loss 0.59380770 \n",
      "企業倫理 | 360 train_loss 0.64705056 valid_loss 0.67053705 test_loss 0.59185869 < save\n",
      "企業倫理 | 361 train_loss 0.64877957 valid_loss 0.67230725 test_loss 0.59435344 \n",
      "企業倫理 | 362 train_loss 0.65035450 valid_loss 0.67425847 test_loss 0.59420115 \n",
      "企業倫理 | 363 train_loss 0.64748967 valid_loss 0.67122871 test_loss 0.59279072 \n",
      "企業倫理 | 364 train_loss 0.64753759 valid_loss 0.67145723 test_loss 0.59197313 \n",
      "企業倫理 | 365 train_loss 0.64752525 valid_loss 0.67113519 test_loss 0.59271801 \n",
      "企業倫理 | 366 train_loss 0.64753616 valid_loss 0.67137635 test_loss 0.59278488 \n",
      "企業倫理 | 367 train_loss 0.64968842 valid_loss 0.67390513 test_loss 0.59354335 \n",
      "企業倫理 | 368 train_loss 0.64858121 valid_loss 0.67265338 test_loss 0.59270734 \n",
      "企業倫理 | 369 train_loss 0.65449095 valid_loss 0.67859668 test_loss 0.60119003 \n",
      "企業倫理 | 370 train_loss 0.64846301 valid_loss 0.67196608 test_loss 0.59403753 \n",
      "企業倫理 | 371 train_loss 0.64732575 valid_loss 0.67127520 test_loss 0.59247273 \n",
      "企業倫理 | 372 train_loss 0.64730608 valid_loss 0.67085052 test_loss 0.59242755 \n",
      "企業倫理 | 373 train_loss 0.64969802 valid_loss 0.67338842 test_loss 0.59543961 \n",
      "企業倫理 | 374 train_loss 0.64911252 valid_loss 0.67309839 test_loss 0.59485394 \n",
      "企業倫理 | 375 train_loss 0.64719522 valid_loss 0.67088288 test_loss 0.59233272 \n",
      "企業倫理 | 376 train_loss 0.65851247 valid_loss 0.68253124 test_loss 0.60573107 \n",
      "企業倫理 | 377 train_loss 0.64907825 valid_loss 0.67270267 test_loss 0.59491700 \n",
      "企業倫理 | 378 train_loss 0.64770937 valid_loss 0.67149353 test_loss 0.59311557 \n",
      "企業倫理 | 379 train_loss 0.64923751 valid_loss 0.67320478 test_loss 0.59318542 \n",
      "企業倫理 | 380 train_loss 0.64797914 valid_loss 0.67211956 test_loss 0.59236300 \n",
      "企業倫理 | 381 train_loss 0.64786804 valid_loss 0.67169869 test_loss 0.59212035 \n",
      "企業倫理 | 382 train_loss 0.65863633 valid_loss 0.68265551 test_loss 0.60582995 \n",
      "企業倫理 | 383 train_loss 0.64715099 valid_loss 0.67075443 test_loss 0.59208488 \n",
      "企業倫理 | 384 train_loss 0.64777470 valid_loss 0.67166275 test_loss 0.59198016 \n",
      "企業倫理 | 385 train_loss 0.64733392 valid_loss 0.67132914 test_loss 0.59169966 \n",
      "企業倫理 | 386 train_loss 0.66602111 valid_loss 0.68983692 test_loss 0.61361611 \n",
      "企業倫理 | 387 train_loss 0.65401638 valid_loss 0.67845744 test_loss 0.59735149 \n",
      "企業倫理 | 388 train_loss 0.65535909 valid_loss 0.67933327 test_loss 0.60231233 \n",
      "企業倫理 | 389 train_loss 0.64770955 valid_loss 0.67098969 test_loss 0.59265375 \n",
      "企業倫理 | 390 train_loss 0.64832425 valid_loss 0.67212534 test_loss 0.59396046 \n",
      "企業倫理 | 391 train_loss 0.64794916 valid_loss 0.67203957 test_loss 0.59225351 \n",
      "企業倫理 | 392 train_loss 0.65225011 valid_loss 0.67586422 test_loss 0.59856200 \n",
      "企業倫理 | 393 train_loss 0.64994085 valid_loss 0.67374504 test_loss 0.59381300 \n",
      "企業倫理 | 394 train_loss 0.64704770 valid_loss 0.67084676 test_loss 0.59202451 \n",
      "企業倫理 | 395 train_loss 0.64717537 valid_loss 0.67121583 test_loss 0.59207553 \n",
      "企業倫理 | 396 train_loss 0.65069646 valid_loss 0.67494106 test_loss 0.59439528 \n",
      "企業倫理 | 397 train_loss 0.64707464 valid_loss 0.67071152 test_loss 0.59204406 \n",
      "企業倫理 | 398 train_loss 0.64769739 valid_loss 0.67186832 test_loss 0.59231937 \n",
      "企業倫理 | 399 train_loss 0.65306854 valid_loss 0.67737377 test_loss 0.59656113 \n",
      "企業倫理 | 400 train_loss 0.64883018 valid_loss 0.67295927 test_loss 0.59282488 \n",
      "企業倫理 | 401 train_loss 0.65214574 valid_loss 0.67648584 test_loss 0.59563106 \n",
      "企業倫理 | 402 train_loss 0.64754844 valid_loss 0.67149562 test_loss 0.59190542 \n",
      "企業倫理 | 403 train_loss 0.64719999 valid_loss 0.67066920 test_loss 0.59173900 \n",
      "企業倫理 | 404 train_loss 0.64879668 valid_loss 0.67218864 test_loss 0.59422082 \n",
      "企業倫理 | 405 train_loss 0.64725786 valid_loss 0.67120230 test_loss 0.59166414 \n",
      "企業倫理 | 406 train_loss 0.65110254 valid_loss 0.67565018 test_loss 0.59487092 \n",
      "企業倫理 | 407 train_loss 0.64709067 valid_loss 0.67103887 test_loss 0.59192485 \n",
      "企業倫理 | 408 train_loss 0.64791477 valid_loss 0.67179811 test_loss 0.59334755 \n",
      "企業倫理 | 409 train_loss 0.64889032 valid_loss 0.67247552 test_loss 0.59457380 \n",
      "企業倫理 | 410 train_loss 0.65073848 valid_loss 0.67492145 test_loss 0.59684587 \n",
      "企業倫理 | 411 train_loss 0.64949238 valid_loss 0.67355102 test_loss 0.59349382 \n",
      "企業倫理 | 412 train_loss 0.64723706 valid_loss 0.67108202 test_loss 0.59235078 \n",
      "企業倫理 | 413 train_loss 0.65300971 valid_loss 0.67663813 test_loss 0.59939861 \n",
      "企業倫理 | 414 train_loss 0.65223485 valid_loss 0.67659843 test_loss 0.59575868 \n",
      "企業倫理 | 415 train_loss 0.64723164 valid_loss 0.67086166 test_loss 0.59232944 \n",
      "企業倫理 | 416 train_loss 0.65007663 valid_loss 0.67353731 test_loss 0.59590214 \n",
      "企業倫理 | 417 train_loss 0.64775735 valid_loss 0.67142355 test_loss 0.59316391 \n",
      "企業倫理 | 418 train_loss 0.64782113 valid_loss 0.67106438 test_loss 0.59267700 \n",
      "企業倫理 | 419 train_loss 0.64720345 valid_loss 0.67059952 test_loss 0.59184176 \n",
      "企業倫理 | 420 train_loss 0.64739257 valid_loss 0.67149115 test_loss 0.59192652 \n",
      "企業倫理 | 421 train_loss 0.64918697 valid_loss 0.67331219 test_loss 0.59313208 \n",
      "企業倫理 | 422 train_loss 0.64844614 valid_loss 0.67248607 test_loss 0.59392178 \n",
      "企業倫理 | 423 train_loss 0.65239680 valid_loss 0.67672575 test_loss 0.59591264 \n",
      "企業倫理 | 424 train_loss 0.64906007 valid_loss 0.67309904 test_loss 0.59299976 \n",
      "企業倫理 | 425 train_loss 0.64713204 valid_loss 0.67103124 test_loss 0.59161764 \n",
      "企業倫理 | 426 train_loss 0.64719421 valid_loss 0.67093140 test_loss 0.59238756 \n",
      "企業倫理 | 427 train_loss 0.64727831 valid_loss 0.67114347 test_loss 0.59184051 \n",
      "企業倫理 | 428 train_loss 0.65207958 valid_loss 0.67611194 test_loss 0.59557199 \n",
      "企業倫理 | 429 train_loss 0.64773339 valid_loss 0.67175061 test_loss 0.59201175 \n",
      "企業倫理 | 430 train_loss 0.64758146 valid_loss 0.67124569 test_loss 0.59296256 \n",
      "企業倫理 | 431 train_loss 0.65317953 valid_loss 0.67688423 test_loss 0.59983182 \n",
      "企業倫理 | 432 train_loss 0.64788300 valid_loss 0.67152578 test_loss 0.59331602 \n",
      "企業倫理 | 433 train_loss 0.64739388 valid_loss 0.67097038 test_loss 0.59264857 \n",
      "企業倫理 | 434 train_loss 0.66822755 valid_loss 0.69283611 test_loss 0.61636752 \n",
      "企業倫理 | 435 train_loss 0.64953709 valid_loss 0.67295194 test_loss 0.59537631 \n",
      "企業倫理 | 436 train_loss 0.65229923 valid_loss 0.67644858 test_loss 0.59881032 \n",
      "企業倫理 | 437 train_loss 0.64992005 valid_loss 0.67378217 test_loss 0.59599400 \n",
      "企業倫理 | 438 train_loss 0.65031677 valid_loss 0.67376608 test_loss 0.59633237 \n",
      "企業倫理 | 439 train_loss 0.64852089 valid_loss 0.67267370 test_loss 0.59250027 \n",
      "企業倫理 | 440 train_loss 0.65000427 valid_loss 0.67334586 test_loss 0.59582192 \n",
      "企業倫理 | 441 train_loss 0.64905155 valid_loss 0.67307389 test_loss 0.59299678 \n",
      "企業倫理 | 442 train_loss 0.64815849 valid_loss 0.67148840 test_loss 0.59353161 \n",
      "企業倫理 | 443 train_loss 0.64746296 valid_loss 0.67142767 test_loss 0.59185076 \n",
      "企業倫理 | 444 train_loss 0.65186805 valid_loss 0.67611873 test_loss 0.59538686 \n",
      "企業倫理 | 445 train_loss 0.64696997 valid_loss 0.67063904 test_loss 0.59160817 \n",
      "企業倫理 | 446 train_loss 0.64888120 valid_loss 0.67304289 test_loss 0.59279728 \n",
      "企業倫理 | 447 train_loss 0.64840436 valid_loss 0.67222482 test_loss 0.59398860 \n",
      "企業倫理 | 448 train_loss 0.64710665 valid_loss 0.67102158 test_loss 0.59154248 \n",
      "企業倫理 | 449 train_loss 0.64754552 valid_loss 0.67106527 test_loss 0.59189695 \n",
      "企業倫理 | 450 train_loss 0.65199757 valid_loss 0.67574155 test_loss 0.59812123 \n",
      "企業倫理 | 451 train_loss 0.64844388 valid_loss 0.67245179 test_loss 0.59254688 \n",
      "企業倫理 | 452 train_loss 0.64703977 valid_loss 0.67063338 test_loss 0.59193736 \n",
      "企業倫理 | 453 train_loss 0.64699948 valid_loss 0.67059666 test_loss 0.59192932 \n",
      "企業倫理 | 454 train_loss 0.64884067 valid_loss 0.67299557 test_loss 0.59443909 \n",
      "企業倫理 | 455 train_loss 0.64751953 valid_loss 0.67138559 test_loss 0.59194255 \n",
      "企業倫理 | 456 train_loss 0.64713442 valid_loss 0.67089671 test_loss 0.59165925 \n",
      "企業倫理 | 457 train_loss 0.64700371 valid_loss 0.67066830 test_loss 0.59156334 \n",
      "企業倫理 | 458 train_loss 0.64776063 valid_loss 0.67172796 test_loss 0.59299284 \n",
      "企業倫理 | 459 train_loss 0.65657085 valid_loss 0.68142790 test_loss 0.59965366 \n",
      "企業倫理 | 460 train_loss 0.65177393 valid_loss 0.67560250 test_loss 0.59801954 \n",
      "企業倫理 | 461 train_loss 0.65096682 valid_loss 0.67455155 test_loss 0.59701627 \n",
      "企業倫理 | 462 train_loss 0.64932346 valid_loss 0.67336667 test_loss 0.59321505 \n",
      "企業倫理 | 463 train_loss 0.64723271 valid_loss 0.67064375 test_loss 0.59190053 \n",
      "企業倫理 | 464 train_loss 0.64711004 valid_loss 0.67070454 test_loss 0.59163421 \n",
      "企業倫理 | 465 train_loss 0.64944857 valid_loss 0.67307013 test_loss 0.59540302 \n",
      "企業倫理 | 466 train_loss 0.64697576 valid_loss 0.67064679 test_loss 0.59184587 \n",
      "企業倫理 | 467 train_loss 0.64699799 valid_loss 0.67071211 test_loss 0.59209788 \n",
      "企業倫理 | 468 train_loss 0.65704334 valid_loss 0.68093228 test_loss 0.60403818 \n",
      "企業倫理 | 469 train_loss 0.64723438 valid_loss 0.67103219 test_loss 0.59232134 \n",
      "企業倫理 | 470 train_loss 0.64727622 valid_loss 0.67064542 test_loss 0.59186035 \n",
      "企業倫理 | 471 train_loss 0.64755189 valid_loss 0.67143971 test_loss 0.59271383 \n",
      "企業倫理 | 472 train_loss 0.64721423 valid_loss 0.67078733 test_loss 0.59162796 \n",
      "企業倫理 | 473 train_loss 0.64750057 valid_loss 0.67164248 test_loss 0.59197992 \n",
      "企業倫理 | 474 train_loss 0.64840776 valid_loss 0.67251569 test_loss 0.59253585 \n",
      "企業倫理 | 475 train_loss 0.65621758 valid_loss 0.68065155 test_loss 0.59933597 \n",
      "企業倫理 | 476 train_loss 0.64956754 valid_loss 0.67386264 test_loss 0.59341204 \n",
      "企業倫理 | 477 train_loss 0.64776528 valid_loss 0.67153800 test_loss 0.59300721 \n",
      "企業倫理 | 478 train_loss 0.64744937 valid_loss 0.67152375 test_loss 0.59243160 \n",
      "企業倫理 | 479 train_loss 0.65152657 valid_loss 0.67537934 test_loss 0.59509480 \n",
      "企業倫理 | 480 train_loss 0.65010774 valid_loss 0.67427820 test_loss 0.59382898 \n",
      "企業倫理 | 481 train_loss 0.64725745 valid_loss 0.67098677 test_loss 0.59153795 \n",
      "企業倫理 | 482 train_loss 0.64694393 valid_loss 0.67073643 test_loss 0.59164107 \n",
      "企業倫理 | 483 train_loss 0.64750642 valid_loss 0.67173904 test_loss 0.59211808 \n",
      "企業倫理 | 484 train_loss 0.65017015 valid_loss 0.67340994 test_loss 0.59590471 \n",
      "企業倫理 | 485 train_loss 0.65335286 valid_loss 0.67767036 test_loss 0.59664810 \n",
      "企業倫理 | 486 train_loss 0.64692211 valid_loss 0.67063212 test_loss 0.59154469 \n",
      "企業倫理 | 487 train_loss 0.64756185 valid_loss 0.67117751 test_loss 0.59284401 \n",
      "企業倫理 | 488 train_loss 0.65295047 valid_loss 0.67669851 test_loss 0.59918666 \n",
      "企業倫理 | 489 train_loss 0.64708871 valid_loss 0.67067862 test_loss 0.59154737 \n",
      "企業倫理 | 490 train_loss 0.65529126 valid_loss 0.67965138 test_loss 0.59847718 \n",
      "企業倫理 | 491 train_loss 0.64758104 valid_loss 0.67165107 test_loss 0.59183580 \n",
      "企業倫理 | 492 train_loss 0.64772069 valid_loss 0.67088872 test_loss 0.59287846 \n",
      "企業倫理 | 493 train_loss 0.65093005 valid_loss 0.67474705 test_loss 0.59706426 \n",
      "企業倫理 | 494 train_loss 0.64961767 valid_loss 0.67363095 test_loss 0.59542704 \n",
      "企業倫理 | 495 train_loss 0.64702511 valid_loss 0.67081547 test_loss 0.59167224 \n",
      "企業倫理 | 496 train_loss 0.64697653 valid_loss 0.67071575 test_loss 0.59141946 \n",
      "企業倫理 | 497 train_loss 0.65176111 valid_loss 0.67612940 test_loss 0.59522110 \n",
      "企業倫理 | 498 train_loss 0.65450150 valid_loss 0.67918408 test_loss 0.59770048 \n",
      "企業倫理 | 499 train_loss 0.64709216 valid_loss 0.67064035 test_loss 0.59160978 \n",
      "企業倫理 | 500 train_loss 0.64744943 valid_loss 0.67140853 test_loss 0.59259564 \n",
      "企業倫理 | 501 train_loss 0.64723754 valid_loss 0.67113632 test_loss 0.59238279 \n",
      "企業倫理 | 502 train_loss 0.64733404 valid_loss 0.67084348 test_loss 0.59262967 \n",
      "企業倫理 | 503 train_loss 0.64704233 valid_loss 0.67066199 test_loss 0.59218889 \n",
      "企業倫理 | 504 train_loss 0.64823055 valid_loss 0.67187637 test_loss 0.59404784 \n",
      "企業倫理 | 505 train_loss 0.64767540 valid_loss 0.67146671 test_loss 0.59302634 \n",
      "企業倫理 | 506 train_loss 0.64905888 valid_loss 0.67273015 test_loss 0.59502381 \n",
      "企業倫理 | 507 train_loss 0.64864451 valid_loss 0.67231143 test_loss 0.59434891 \n",
      "企業倫理 | 508 train_loss 0.64860928 valid_loss 0.67274302 test_loss 0.59266049 \n",
      "企業倫理 | 509 train_loss 0.64803725 valid_loss 0.67168623 test_loss 0.59346884 \n",
      "企業倫理 | 510 train_loss 0.64716411 valid_loss 0.67119253 test_loss 0.59168774 \n",
      "企業倫理 | 511 train_loss 0.64713705 valid_loss 0.67053139 test_loss 0.59203964 < save\n",
      "企業倫理 | 512 train_loss 0.65072620 valid_loss 0.67382908 test_loss 0.59639806 \n",
      "企業倫理 | 513 train_loss 0.65123653 valid_loss 0.67548990 test_loss 0.59493065 \n",
      "企業倫理 | 514 train_loss 0.64844841 valid_loss 0.67217141 test_loss 0.59398550 \n",
      "企業倫理 | 515 train_loss 0.64745462 valid_loss 0.67146170 test_loss 0.59177500 \n",
      "企業倫理 | 516 train_loss 0.64934874 valid_loss 0.67334950 test_loss 0.59318680 \n",
      "企業倫理 | 517 train_loss 0.64840341 valid_loss 0.67267895 test_loss 0.59236985 \n",
      "企業倫理 | 518 train_loss 0.64714330 valid_loss 0.67057973 test_loss 0.59196758 \n",
      "企業倫理 | 519 train_loss 0.64866972 valid_loss 0.67283332 test_loss 0.59420449 \n",
      "企業倫理 | 520 train_loss 0.64728087 valid_loss 0.67082095 test_loss 0.59166783 \n",
      "企業倫理 | 521 train_loss 0.64733762 valid_loss 0.67097646 test_loss 0.59172440 \n",
      "企業倫理 | 522 train_loss 0.64781910 valid_loss 0.67145842 test_loss 0.59204537 \n",
      "企業倫理 | 523 train_loss 0.64709759 valid_loss 0.67083013 test_loss 0.59169298 \n",
      "企業倫理 | 524 train_loss 0.64715636 valid_loss 0.67094457 test_loss 0.59160364 \n",
      "企業倫理 | 525 train_loss 0.64742851 valid_loss 0.67107975 test_loss 0.59171373 \n",
      "企業倫理 | 526 train_loss 0.64707440 valid_loss 0.67089337 test_loss 0.59199750 \n",
      "企業倫理 | 527 train_loss 0.65470630 valid_loss 0.67855132 test_loss 0.60127932 \n",
      "企業倫理 | 528 train_loss 0.64694935 valid_loss 0.67075962 test_loss 0.59180099 \n",
      "企業倫理 | 529 train_loss 0.64705837 valid_loss 0.67051923 test_loss 0.59174347 < save\n",
      "企業倫理 | 530 train_loss 0.65165704 valid_loss 0.67526454 test_loss 0.59777790 \n",
      "企業倫理 | 531 train_loss 0.64698893 valid_loss 0.67065018 test_loss 0.59156239 \n",
      "企業倫理 | 532 train_loss 0.64835143 valid_loss 0.67210603 test_loss 0.59389931 \n",
      "企業倫理 | 533 train_loss 0.65171152 valid_loss 0.67599308 test_loss 0.59522021 \n",
      "企業倫理 | 534 train_loss 0.64720345 valid_loss 0.67093658 test_loss 0.59156686 \n",
      "企業倫理 | 535 train_loss 0.64962685 valid_loss 0.67345053 test_loss 0.59330481 \n",
      "企業倫理 | 536 train_loss 0.64869159 valid_loss 0.67208624 test_loss 0.59429824 \n",
      "企業倫理 | 537 train_loss 0.64710349 valid_loss 0.67099750 test_loss 0.59200114 \n",
      "企業倫理 | 538 train_loss 0.65253794 valid_loss 0.67692649 test_loss 0.59595686 \n",
      "企業倫理 | 539 train_loss 0.64792657 valid_loss 0.67185318 test_loss 0.59225434 \n",
      "企業倫理 | 540 train_loss 0.64740121 valid_loss 0.67139357 test_loss 0.59178174 \n",
      "企業倫理 | 541 train_loss 0.65920126 valid_loss 0.68416685 test_loss 0.60202128 \n",
      "企業倫理 | 542 train_loss 0.64860553 valid_loss 0.67207313 test_loss 0.59414124 \n",
      "企業倫理 | 543 train_loss 0.65176672 valid_loss 0.67616302 test_loss 0.59542620 \n",
      "企業倫理 | 544 train_loss 0.65104276 valid_loss 0.67504233 test_loss 0.59465134 \n",
      "企業倫理 | 545 train_loss 0.64694083 valid_loss 0.67073095 test_loss 0.59179324 \n",
      "企業倫理 | 546 train_loss 0.64871573 valid_loss 0.67283523 test_loss 0.59261835 \n",
      "企業倫理 | 547 train_loss 0.64725333 valid_loss 0.67100984 test_loss 0.59156603 \n",
      "企業倫理 | 548 train_loss 0.64959967 valid_loss 0.67299312 test_loss 0.59526289 \n",
      "企業倫理 | 549 train_loss 0.64893484 valid_loss 0.67255992 test_loss 0.59455979 \n",
      "企業倫理 | 550 train_loss 0.65679932 valid_loss 0.68121332 test_loss 0.59978294 \n",
      "企業倫理 | 551 train_loss 0.64765233 valid_loss 0.67134321 test_loss 0.59281540 \n",
      "企業倫理 | 552 train_loss 0.64757293 valid_loss 0.67136407 test_loss 0.59184474 \n",
      "企業倫理 | 553 train_loss 0.65047938 valid_loss 0.67442220 test_loss 0.59408474 \n",
      "企業倫理 | 554 train_loss 0.65103525 valid_loss 0.67459154 test_loss 0.59706956 \n",
      "企業倫理 | 555 train_loss 0.64973134 valid_loss 0.67378098 test_loss 0.59547651 \n",
      "企業倫理 | 556 train_loss 0.64697742 valid_loss 0.67087865 test_loss 0.59159023 \n",
      "企業倫理 | 557 train_loss 0.64787060 valid_loss 0.67190045 test_loss 0.59202379 \n",
      "企業倫理 | 558 train_loss 0.64941776 valid_loss 0.67349094 test_loss 0.59325486 \n",
      "企業倫理 | 559 train_loss 0.64762026 valid_loss 0.67112702 test_loss 0.59282559 \n",
      "企業倫理 | 560 train_loss 0.64747900 valid_loss 0.67135382 test_loss 0.59274805 \n",
      "企業倫理 | 561 train_loss 0.64698690 valid_loss 0.67059714 test_loss 0.59193397 \n",
      "企業倫理 | 562 train_loss 0.64802235 valid_loss 0.67170978 test_loss 0.59213567 \n",
      "企業倫理 | 563 train_loss 0.64746922 valid_loss 0.67124897 test_loss 0.59181964 \n",
      "企業倫理 | 564 train_loss 0.64936727 valid_loss 0.67281866 test_loss 0.59514427 \n",
      "企業倫理 | 565 train_loss 0.64763504 valid_loss 0.67074293 test_loss 0.59237486 \n",
      "企業倫理 | 566 train_loss 0.64766437 valid_loss 0.67119884 test_loss 0.59298754 \n",
      "企業倫理 | 567 train_loss 0.65081859 valid_loss 0.67436928 test_loss 0.59691209 \n",
      "企業倫理 | 568 train_loss 0.64765149 valid_loss 0.67124856 test_loss 0.59291983 \n",
      "企業倫理 | 569 train_loss 0.64819872 valid_loss 0.67181969 test_loss 0.59367210 \n",
      "企業倫理 | 570 train_loss 0.65072364 valid_loss 0.67488694 test_loss 0.59436595 \n",
      "企業倫理 | 571 train_loss 0.65514070 valid_loss 0.67876482 test_loss 0.60170382 \n",
      "企業倫理 | 572 train_loss 0.64760000 valid_loss 0.67141026 test_loss 0.59180963 \n",
      "企業倫理 | 573 train_loss 0.64723551 valid_loss 0.67086077 test_loss 0.59223866 \n",
      "企業倫理 | 574 train_loss 0.67139137 valid_loss 0.69674242 test_loss 0.61346728 \n",
      "企業倫理 | 575 train_loss 0.64761353 valid_loss 0.67143101 test_loss 0.59189087 \n",
      "企業倫理 | 576 train_loss 0.64773560 valid_loss 0.67180496 test_loss 0.59289944 \n",
      "企業倫理 | 577 train_loss 0.64815450 valid_loss 0.67186111 test_loss 0.59221065 \n",
      "企業倫理 | 578 train_loss 0.64692920 valid_loss 0.67055404 test_loss 0.59166497 \n",
      "企業倫理 | 579 train_loss 0.65149474 valid_loss 0.67518765 test_loss 0.59756619 \n",
      "企業倫理 | 580 train_loss 0.64877170 valid_loss 0.67238468 test_loss 0.59440249 \n",
      "企業倫理 | 581 train_loss 0.64707065 valid_loss 0.67052603 test_loss 0.59180117 \n",
      "企業倫理 | 582 train_loss 0.64717573 valid_loss 0.67090601 test_loss 0.59152913 \n",
      "企業倫理 | 583 train_loss 0.64795101 valid_loss 0.67180657 test_loss 0.59333754 \n",
      "企業倫理 | 584 train_loss 0.65145987 valid_loss 0.67589509 test_loss 0.59503776 \n",
      "企業倫理 | 585 train_loss 0.65297657 valid_loss 0.67739910 test_loss 0.59637862 \n",
      "企業倫理 | 586 train_loss 0.65011233 valid_loss 0.67403233 test_loss 0.59617227 \n",
      "企業倫理 | 587 train_loss 0.64739674 valid_loss 0.67101896 test_loss 0.59181625 \n",
      "企業倫理 | 588 train_loss 0.64838552 valid_loss 0.67220402 test_loss 0.59263825 \n",
      "企業倫理 | 589 train_loss 0.65400851 valid_loss 0.67836469 test_loss 0.60065836 \n",
      "企業倫理 | 590 train_loss 0.64705580 valid_loss 0.67075831 test_loss 0.59212863 \n",
      "企業倫理 | 591 train_loss 0.65120035 valid_loss 0.67582113 test_loss 0.59509104 \n",
      "企業倫理 | 592 train_loss 0.64718711 valid_loss 0.67123234 test_loss 0.59182322 \n",
      "企業倫理 | 593 train_loss 0.64824289 valid_loss 0.67250502 test_loss 0.59263361 \n",
      "企業倫理 | 594 train_loss 0.64872652 valid_loss 0.67269617 test_loss 0.59284598 \n",
      "企業倫理 | 595 train_loss 0.64749736 valid_loss 0.67099106 test_loss 0.59279972 \n",
      "企業倫理 | 596 train_loss 0.64908379 valid_loss 0.67317319 test_loss 0.59299129 \n",
      "企業倫理 | 597 train_loss 0.64947182 valid_loss 0.67322952 test_loss 0.59519148 \n",
      "企業倫理 | 598 train_loss 0.64792037 valid_loss 0.67204452 test_loss 0.59210253 \n",
      "企業倫理 | 599 train_loss 0.64694315 valid_loss 0.67059368 test_loss 0.59161812 \n",
      "企業倫理 | 600 train_loss 0.64694190 valid_loss 0.67066443 test_loss 0.59167850 \n",
      "企業倫理 | 601 train_loss 0.64742410 valid_loss 0.67126650 test_loss 0.59265941 \n",
      "企業倫理 | 602 train_loss 0.64693093 valid_loss 0.67059064 test_loss 0.59189701 \n",
      "企業倫理 | 603 train_loss 0.64722967 valid_loss 0.67067468 test_loss 0.59233594 \n",
      "企業倫理 | 604 train_loss 0.64891499 valid_loss 0.67254019 test_loss 0.59462315 \n",
      "企業倫理 | 605 train_loss 0.64707392 valid_loss 0.67082924 test_loss 0.59169197 \n",
      "企業倫理 | 606 train_loss 0.66548276 valid_loss 0.69081980 test_loss 0.60796738 \n",
      "企業倫理 | 607 train_loss 0.65544575 valid_loss 0.67944443 test_loss 0.60237116 \n",
      "企業倫理 | 608 train_loss 0.64708340 valid_loss 0.67080033 test_loss 0.59170079 \n",
      "企業倫理 | 609 train_loss 0.64940876 valid_loss 0.67288488 test_loss 0.59535855 \n",
      "企業倫理 | 610 train_loss 0.64695388 valid_loss 0.67047173 test_loss 0.59177047 < save\n",
      "企業倫理 | 611 train_loss 0.65445185 valid_loss 0.67839539 test_loss 0.60113209 \n",
      "企業倫理 | 612 train_loss 0.64722335 valid_loss 0.67100745 test_loss 0.59159762 \n",
      "企業倫理 | 613 train_loss 0.64805335 valid_loss 0.67189360 test_loss 0.59212953 \n",
      "企業倫理 | 614 train_loss 0.65436345 valid_loss 0.67884266 test_loss 0.59761560 \n",
      "企業倫理 | 615 train_loss 0.64899927 valid_loss 0.67289400 test_loss 0.59466201 \n",
      "企業倫理 | 616 train_loss 0.65349913 valid_loss 0.67746681 test_loss 0.59675598 \n",
      "企業倫理 | 617 train_loss 0.65042567 valid_loss 0.67456412 test_loss 0.59404087 \n",
      "企業倫理 | 618 train_loss 0.64711481 valid_loss 0.67099696 test_loss 0.59144622 \n",
      "企業倫理 | 619 train_loss 0.64935911 valid_loss 0.67358261 test_loss 0.59322393 \n",
      "企業倫理 | 620 train_loss 0.65570372 valid_loss 0.68022549 test_loss 0.59894544 \n",
      "企業倫理 | 621 train_loss 0.64915371 valid_loss 0.67240989 test_loss 0.59473056 \n",
      "企業倫理 | 622 train_loss 0.65096718 valid_loss 0.67518818 test_loss 0.59471542 \n",
      "企業倫理 | 623 train_loss 0.64718896 valid_loss 0.67094147 test_loss 0.59225124 \n",
      "企業倫理 | 624 train_loss 0.64714491 valid_loss 0.67075479 test_loss 0.59208047 \n",
      "企業倫理 | 625 train_loss 0.64836526 valid_loss 0.67181063 test_loss 0.59375000 \n",
      "企業倫理 | 626 train_loss 0.64742988 valid_loss 0.67072505 test_loss 0.59204656 \n",
      "企業倫理 | 627 train_loss 0.65223581 valid_loss 0.67657721 test_loss 0.59580898 \n",
      "企業倫理 | 628 train_loss 0.64697444 valid_loss 0.67048657 test_loss 0.59168583 \n",
      "企業倫理 | 629 train_loss 0.64741075 valid_loss 0.67125642 test_loss 0.59168237 \n",
      "企業倫理 | 630 train_loss 0.64732003 valid_loss 0.67061257 test_loss 0.59189075 \n",
      "企業倫理 | 631 train_loss 0.64918005 valid_loss 0.67287481 test_loss 0.59479868 \n",
      "企業倫理 | 632 train_loss 0.64734280 valid_loss 0.67119795 test_loss 0.59184009 \n",
      "企業倫理 | 633 train_loss 0.65285403 valid_loss 0.67644238 test_loss 0.59927380 \n",
      "企業倫理 | 634 train_loss 0.64763921 valid_loss 0.67145485 test_loss 0.59180963 \n",
      "企業倫理 | 635 train_loss 0.64767116 valid_loss 0.67115343 test_loss 0.59294367 \n",
      "企業倫理 | 636 train_loss 0.65912789 valid_loss 0.68325639 test_loss 0.60633355 \n",
      "企業倫理 | 637 train_loss 0.64714640 valid_loss 0.67092574 test_loss 0.59217346 \n",
      "企業倫理 | 638 train_loss 0.64938509 valid_loss 0.67293674 test_loss 0.59505981 \n",
      "企業倫理 | 639 train_loss 0.64761811 valid_loss 0.67094761 test_loss 0.59268713 \n",
      "企業倫理 | 640 train_loss 0.64850271 valid_loss 0.67212665 test_loss 0.59250844 \n",
      "企業倫理 | 641 train_loss 0.64697486 valid_loss 0.67075855 test_loss 0.59165210 \n",
      "企業倫理 | 642 train_loss 0.64810598 valid_loss 0.67178607 test_loss 0.59349972 \n",
      "企業倫理 | 643 train_loss 0.64700061 valid_loss 0.67046362 test_loss 0.59156948 < save\n",
      "企業倫理 | 644 train_loss 0.64692652 valid_loss 0.67059153 test_loss 0.59156656 \n",
      "企業倫理 | 645 train_loss 0.64866054 valid_loss 0.67267621 test_loss 0.59267759 \n",
      "企業倫理 | 646 train_loss 0.64737070 valid_loss 0.67104608 test_loss 0.59249640 \n",
      "企業倫理 | 647 train_loss 0.64778358 valid_loss 0.67153794 test_loss 0.59192431 \n",
      "企業倫理 | 648 train_loss 0.65437329 valid_loss 0.67883164 test_loss 0.59752852 \n",
      "企業倫理 | 649 train_loss 0.64726025 valid_loss 0.67115343 test_loss 0.59173661 \n",
      "企業倫理 | 650 train_loss 0.64905566 valid_loss 0.67313129 test_loss 0.59295970 \n",
      "企業倫理 | 651 train_loss 0.64714772 valid_loss 0.67112267 test_loss 0.59191632 \n",
      "企業倫理 | 652 train_loss 0.64739466 valid_loss 0.67077327 test_loss 0.59241015 \n",
      "企業倫理 | 653 train_loss 0.64777946 valid_loss 0.67169297 test_loss 0.59198081 \n",
      "企業倫理 | 654 train_loss 0.64875746 valid_loss 0.67254341 test_loss 0.59268177 \n",
      "企業倫理 | 655 train_loss 0.65137684 valid_loss 0.67557567 test_loss 0.59499568 \n",
      "企業倫理 | 656 train_loss 0.65847731 valid_loss 0.68284899 test_loss 0.60546798 \n",
      "企業倫理 | 657 train_loss 0.65250093 valid_loss 0.67670709 test_loss 0.59589601 \n",
      "企業倫理 | 658 train_loss 0.64776963 valid_loss 0.67139107 test_loss 0.59305841 \n",
      "企業倫理 | 659 train_loss 0.64804494 valid_loss 0.67164695 test_loss 0.59355378 \n",
      "企業倫理 | 660 train_loss 0.64741349 valid_loss 0.67109680 test_loss 0.59256989 \n",
      "企業倫理 | 661 train_loss 0.65094405 valid_loss 0.67526901 test_loss 0.59446681 \n",
      "企業倫理 | 662 train_loss 0.64760083 valid_loss 0.67143369 test_loss 0.59290957 \n",
      "企業倫理 | 663 train_loss 0.64986229 valid_loss 0.67407590 test_loss 0.59370357 \n",
      "企業倫理 | 664 train_loss 0.64795285 valid_loss 0.67161292 test_loss 0.59341019 \n",
      "企業倫理 | 665 train_loss 0.64760554 valid_loss 0.67070204 test_loss 0.59246874 \n",
      "企業倫理 | 666 train_loss 0.64706433 valid_loss 0.67089254 test_loss 0.59172076 \n",
      "企業倫理 | 667 train_loss 0.64874703 valid_loss 0.67231089 test_loss 0.59441054 \n",
      "企業倫理 | 668 train_loss 0.64864218 valid_loss 0.67237872 test_loss 0.59279013 \n",
      "企業倫理 | 669 train_loss 0.64777666 valid_loss 0.67190057 test_loss 0.59205872 \n",
      "企業倫理 | 670 train_loss 0.64715183 valid_loss 0.67120486 test_loss 0.59192163 \n",
      "企業倫理 | 671 train_loss 0.65149814 valid_loss 0.67587942 test_loss 0.59504652 \n",
      "企業倫理 | 672 train_loss 0.65037310 valid_loss 0.67355925 test_loss 0.59610003 \n",
      "企業倫理 | 673 train_loss 0.64726692 valid_loss 0.67077684 test_loss 0.59169292 \n",
      "企業倫理 | 674 train_loss 0.64802027 valid_loss 0.67150921 test_loss 0.59337997 \n",
      "企業倫理 | 675 train_loss 0.65154922 valid_loss 0.67577702 test_loss 0.59495878 \n",
      "企業倫理 | 676 train_loss 0.64922243 valid_loss 0.67272466 test_loss 0.59480864 \n",
      "企業倫理 | 677 train_loss 0.65489537 valid_loss 0.67862785 test_loss 0.60157019 \n",
      "企業倫理 | 678 train_loss 0.65076941 valid_loss 0.67430097 test_loss 0.59681344 \n",
      "企業倫理 | 679 train_loss 0.67651469 valid_loss 0.70087940 test_loss 0.62509412 \n",
      "企業倫理 | 680 train_loss 0.64785188 valid_loss 0.67135495 test_loss 0.59312040 \n",
      "企業倫理 | 681 train_loss 0.64751071 valid_loss 0.67116725 test_loss 0.59166509 \n",
      "企業倫理 | 682 train_loss 0.64919555 valid_loss 0.67348009 test_loss 0.59297621 \n",
      "企業倫理 | 683 train_loss 0.64723599 valid_loss 0.67100102 test_loss 0.59214860 \n",
      "企業倫理 | 684 train_loss 0.64713031 valid_loss 0.67083699 test_loss 0.59147358 \n",
      "企業倫理 | 685 train_loss 0.64968258 valid_loss 0.67380446 test_loss 0.59542239 \n",
      "企業倫理 | 686 train_loss 0.65871412 valid_loss 0.68229234 test_loss 0.60576034 \n",
      "企業倫理 | 687 train_loss 0.64754224 valid_loss 0.67152596 test_loss 0.59269804 \n",
      "企業倫理 | 688 train_loss 0.65198028 valid_loss 0.67637402 test_loss 0.59542894 \n",
      "企業倫理 | 689 train_loss 0.64938712 valid_loss 0.67320436 test_loss 0.59313589 \n",
      "企業倫理 | 690 train_loss 0.64706057 valid_loss 0.67049330 test_loss 0.59172946 \n",
      "企業倫理 | 691 train_loss 0.65357459 valid_loss 0.67809844 test_loss 0.59688383 \n",
      "企業倫理 | 692 train_loss 0.64799708 valid_loss 0.67185843 test_loss 0.59196919 \n",
      "企業倫理 | 693 train_loss 0.65918052 valid_loss 0.68381488 test_loss 0.60204059 \n",
      "企業倫理 | 694 train_loss 0.64719456 valid_loss 0.67063576 test_loss 0.59178060 \n",
      "企業倫理 | 695 train_loss 0.64796555 valid_loss 0.67171967 test_loss 0.59328246 \n",
      "企業倫理 | 696 train_loss 0.64708477 valid_loss 0.67086744 test_loss 0.59196550 \n",
      "企業倫理 | 697 train_loss 0.64694554 valid_loss 0.67072964 test_loss 0.59145063 \n",
      "企業倫理 | 698 train_loss 0.64824396 valid_loss 0.67198551 test_loss 0.59225279 \n",
      "企業倫理 | 699 train_loss 0.64761895 valid_loss 0.67088449 test_loss 0.59260035 \n",
      "企業倫理 | 700 train_loss 0.64703202 valid_loss 0.67065519 test_loss 0.59144175 \n",
      "企業倫理 | 701 train_loss 0.65248817 valid_loss 0.67669272 test_loss 0.59575605 \n",
      "企業倫理 | 702 train_loss 0.64813656 valid_loss 0.67209536 test_loss 0.59218198 \n",
      "企業倫理 | 703 train_loss 0.64859426 valid_loss 0.67227000 test_loss 0.59400558 \n",
      "企業倫理 | 704 train_loss 0.64952654 valid_loss 0.67309391 test_loss 0.59519053 \n",
      "企業倫理 | 705 train_loss 0.64890730 valid_loss 0.67284316 test_loss 0.59279931 \n",
      "企業倫理 | 706 train_loss 0.64709479 valid_loss 0.67060786 test_loss 0.59186703 \n",
      "企業倫理 | 707 train_loss 0.64962691 valid_loss 0.67365867 test_loss 0.59349692 \n",
      "企業倫理 | 708 train_loss 0.64719224 valid_loss 0.67072797 test_loss 0.59210223 \n",
      "企業倫理 | 709 train_loss 0.64772880 valid_loss 0.67137086 test_loss 0.59190828 \n",
      "企業倫理 | 710 train_loss 0.64718622 valid_loss 0.67063946 test_loss 0.59155911 \n",
      "企業倫理 | 711 train_loss 0.65204936 valid_loss 0.67638910 test_loss 0.59552109 \n",
      "企業倫理 | 712 train_loss 0.64735663 valid_loss 0.67076182 test_loss 0.59230548 \n",
      "企業倫理 | 713 train_loss 0.65802246 valid_loss 0.68266082 test_loss 0.60083598 \n",
      "企業倫理 | 714 train_loss 0.64709169 valid_loss 0.67075288 test_loss 0.59156549 \n",
      "企業倫理 | 715 train_loss 0.64727151 valid_loss 0.67055863 test_loss 0.59207648 \n",
      "企業倫理 | 716 train_loss 0.64853692 valid_loss 0.67247176 test_loss 0.59237570 \n",
      "企業倫理 | 717 train_loss 0.64805466 valid_loss 0.67168766 test_loss 0.59207553 \n",
      "企業倫理 | 718 train_loss 0.64710599 valid_loss 0.67098832 test_loss 0.59193671 \n",
      "企業倫理 | 719 train_loss 0.65188938 valid_loss 0.67571640 test_loss 0.59800780 \n",
      "企業倫理 | 720 train_loss 0.65491521 valid_loss 0.67839062 test_loss 0.60148418 \n",
      "企業倫理 | 721 train_loss 0.65049481 valid_loss 0.67456168 test_loss 0.59409761 \n",
      "企業倫理 | 722 train_loss 0.64694619 valid_loss 0.67068863 test_loss 0.59181780 \n",
      "企業倫理 | 723 train_loss 0.64844036 valid_loss 0.67252833 test_loss 0.59243840 \n",
      "企業倫理 | 724 train_loss 0.64743274 valid_loss 0.67122817 test_loss 0.59253943 \n",
      "企業倫理 | 725 train_loss 0.64731842 valid_loss 0.67093760 test_loss 0.59240288 \n",
      "企業倫理 | 726 train_loss 0.64766115 valid_loss 0.67161316 test_loss 0.59198117 \n",
      "企業倫理 | 727 train_loss 0.65005106 valid_loss 0.67426884 test_loss 0.59376854 \n",
      "企業倫理 | 728 train_loss 0.65183377 valid_loss 0.67595279 test_loss 0.59524256 \n",
      "企業倫理 | 729 train_loss 0.65491152 valid_loss 0.67934310 test_loss 0.59801638 \n",
      "企業倫理 | 730 train_loss 0.65361410 valid_loss 0.67792046 test_loss 0.59684372 \n",
      "企業倫理 | 731 train_loss 0.65199375 valid_loss 0.67619085 test_loss 0.59542471 \n",
      "企業倫理 | 732 train_loss 0.64693004 valid_loss 0.67051655 test_loss 0.59169364 \n",
      "企業倫理 | 733 train_loss 0.64781260 valid_loss 0.67194849 test_loss 0.59204352 \n",
      "企業倫理 | 734 train_loss 0.64826268 valid_loss 0.67183346 test_loss 0.59243083 \n",
      "企業倫理 | 735 train_loss 0.64719623 valid_loss 0.67090601 test_loss 0.59226781 \n",
      "企業倫理 | 736 train_loss 0.64766145 valid_loss 0.67171752 test_loss 0.59267247 \n",
      "企業倫理 | 737 train_loss 0.64880890 valid_loss 0.67250103 test_loss 0.59435058 \n",
      "企業倫理 | 738 train_loss 0.64798313 valid_loss 0.67201042 test_loss 0.59209198 \n",
      "企業倫理 | 739 train_loss 0.64724010 valid_loss 0.67062765 test_loss 0.59163195 \n",
      "企業倫理 | 740 train_loss 0.64712161 valid_loss 0.67076635 test_loss 0.59154934 \n",
      "企業倫理 | 741 train_loss 0.64810073 valid_loss 0.67165321 test_loss 0.59216160 \n",
      "企業倫理 | 742 train_loss 0.64782083 valid_loss 0.67147452 test_loss 0.59195691 \n",
      "企業倫理 | 743 train_loss 0.64791691 valid_loss 0.67166412 test_loss 0.59207958 \n",
      "企業倫理 | 744 train_loss 0.64714289 valid_loss 0.67084128 test_loss 0.59155619 \n",
      "企業倫理 | 745 train_loss 0.64732414 valid_loss 0.67086172 test_loss 0.59238702 \n",
      "企業倫理 | 746 train_loss 0.64714396 valid_loss 0.67067367 test_loss 0.59208190 \n",
      "企業倫理 | 747 train_loss 0.64809471 valid_loss 0.67182374 test_loss 0.59349191 \n",
      "企業倫理 | 748 train_loss 0.64722484 valid_loss 0.67072743 test_loss 0.59223145 \n",
      "企業倫理 | 749 train_loss 0.64743155 valid_loss 0.67064559 test_loss 0.59222496 \n",
      "企業倫理 | 750 train_loss 0.64974171 valid_loss 0.67324680 test_loss 0.59557706 \n",
      "企業倫理 | 751 train_loss 0.64693671 valid_loss 0.67056656 test_loss 0.59168380 \n",
      "企業倫理 | 752 train_loss 0.64710170 valid_loss 0.67052412 test_loss 0.59201854 \n",
      "企業倫理 | 753 train_loss 0.64721328 valid_loss 0.67106712 test_loss 0.59170079 \n",
      "企業倫理 | 754 train_loss 0.64810675 valid_loss 0.67203093 test_loss 0.59222877 \n",
      "企業倫理 | 755 train_loss 0.64705545 valid_loss 0.67082459 test_loss 0.59186333 \n",
      "企業倫理 | 756 train_loss 0.64934951 valid_loss 0.67356712 test_loss 0.59310454 \n",
      "企業倫理 | 757 train_loss 0.64753491 valid_loss 0.67138690 test_loss 0.59174585 \n",
      "企業倫理 | 758 train_loss 0.65020639 valid_loss 0.67381394 test_loss 0.59606296 \n",
      "企業倫理 | 759 train_loss 0.64711946 valid_loss 0.67078036 test_loss 0.59207439 \n",
      "企業倫理 | 760 train_loss 0.65013647 valid_loss 0.67366141 test_loss 0.59602541 \n",
      "企業倫理 | 761 train_loss 0.64844674 valid_loss 0.67193758 test_loss 0.59387630 \n",
      "企業倫理 | 762 train_loss 0.64752477 valid_loss 0.67067581 test_loss 0.59231234 \n",
      "企業倫理 | 763 train_loss 0.64792770 valid_loss 0.67210507 test_loss 0.59217656 \n",
      "企業倫理 | 764 train_loss 0.64702082 valid_loss 0.67079514 test_loss 0.59151977 \n",
      "企業倫理 | 765 train_loss 0.64751565 valid_loss 0.67130983 test_loss 0.59184623 \n",
      "企業倫理 | 766 train_loss 0.65041620 valid_loss 0.67366421 test_loss 0.59615529 \n",
      "企業倫理 | 767 train_loss 0.64792961 valid_loss 0.67175531 test_loss 0.59331262 \n",
      "企業倫理 | 768 train_loss 0.64707661 valid_loss 0.67085671 test_loss 0.59157604 \n",
      "企業倫理 | 769 train_loss 0.64820594 valid_loss 0.67203116 test_loss 0.59377044 \n",
      "企業倫理 | 770 train_loss 0.64802623 valid_loss 0.67179465 test_loss 0.59225565 \n",
      "企業倫理 | 771 train_loss 0.64713132 valid_loss 0.67095715 test_loss 0.59151298 \n",
      "企業倫理 | 772 train_loss 0.64752549 valid_loss 0.67128575 test_loss 0.59186798 \n",
      "企業倫理 | 773 train_loss 0.64723241 valid_loss 0.67085439 test_loss 0.59158254 \n",
      "企業倫理 | 774 train_loss 0.64895213 valid_loss 0.67254579 test_loss 0.59456390 \n",
      "企業倫理 | 775 train_loss 0.64794588 valid_loss 0.67201859 test_loss 0.59215933 \n",
      "企業倫理 | 776 train_loss 0.64836800 valid_loss 0.67203104 test_loss 0.59245706 \n",
      "企業倫理 | 777 train_loss 0.64735442 valid_loss 0.67068690 test_loss 0.59222823 \n",
      "企業倫理 | 778 train_loss 0.64724708 valid_loss 0.67092103 test_loss 0.59216499 \n",
      "企業倫理 | 779 train_loss 0.64733583 valid_loss 0.67098612 test_loss 0.59228140 \n",
      "企業倫理 | 780 train_loss 0.64935279 valid_loss 0.67294371 test_loss 0.59504741 \n",
      "企業倫理 | 781 train_loss 0.65605128 valid_loss 0.68070471 test_loss 0.59899682 \n",
      "企業倫理 | 782 train_loss 0.64695996 valid_loss 0.67041171 test_loss 0.59156066 < save\n",
      "企業倫理 | 783 train_loss 0.64987689 valid_loss 0.67365700 test_loss 0.59557116 \n",
      "企業倫理 | 784 train_loss 0.64781940 valid_loss 0.67110449 test_loss 0.59291375 \n",
      "企業倫理 | 785 train_loss 0.64712566 valid_loss 0.67094439 test_loss 0.59145403 \n",
      "企業倫理 | 786 train_loss 0.65548074 valid_loss 0.68019450 test_loss 0.59851128 \n",
      "企業倫理 | 787 train_loss 0.64786571 valid_loss 0.67152810 test_loss 0.59192997 \n",
      "企業倫理 | 788 train_loss 0.65293270 valid_loss 0.67655778 test_loss 0.59924746 \n",
      "企業倫理 | 789 train_loss 0.65674442 valid_loss 0.68008661 test_loss 0.60362250 \n",
      "企業倫理 | 790 train_loss 0.64822024 valid_loss 0.67159748 test_loss 0.59359717 \n",
      "企業倫理 | 791 train_loss 0.64714444 valid_loss 0.67053115 test_loss 0.59191412 \n",
      "企業倫理 | 792 train_loss 0.64829141 valid_loss 0.67215103 test_loss 0.59229863 \n",
      "企業倫理 | 793 train_loss 0.64858586 valid_loss 0.67177421 test_loss 0.59395730 \n",
      "企業倫理 | 794 train_loss 0.64689571 valid_loss 0.67045426 test_loss 0.59158224 \n",
      "企業倫理 | 795 train_loss 0.65043354 valid_loss 0.67478019 test_loss 0.59421724 \n",
      "企業倫理 | 796 train_loss 0.65186936 valid_loss 0.67585832 test_loss 0.59526634 \n",
      "企業倫理 | 797 train_loss 0.65300590 valid_loss 0.67673081 test_loss 0.59625930 \n",
      "企業倫理 | 798 train_loss 0.64834130 valid_loss 0.67168975 test_loss 0.59372497 \n",
      "企業倫理 | 799 train_loss 0.65619493 valid_loss 0.68057680 test_loss 0.59924114 \n",
      "企業倫理 | 800 train_loss 0.64696246 valid_loss 0.67058992 test_loss 0.59169340 \n",
      "企業倫理 | 801 train_loss 0.64707422 valid_loss 0.67057341 test_loss 0.59192604 \n",
      "企業倫理 | 802 train_loss 0.65365404 valid_loss 0.67747974 test_loss 0.60017812 \n",
      "企業倫理 | 803 train_loss 0.66346848 valid_loss 0.68763834 test_loss 0.61134732 \n",
      "企業倫理 | 804 train_loss 0.64728147 valid_loss 0.67121786 test_loss 0.59216022 \n",
      "企業倫理 | 805 train_loss 0.65061378 valid_loss 0.67489207 test_loss 0.59433317 \n",
      "企業倫理 | 806 train_loss 0.64757413 valid_loss 0.67136711 test_loss 0.59266198 \n",
      "企業倫理 | 807 train_loss 0.64738858 valid_loss 0.67070150 test_loss 0.59214652 \n",
      "企業倫理 | 808 train_loss 0.64731109 valid_loss 0.67087966 test_loss 0.59155852 \n",
      "企業倫理 | 809 train_loss 0.64778262 valid_loss 0.67153889 test_loss 0.59181786 \n",
      "企業倫理 | 810 train_loss 0.64996642 valid_loss 0.67366600 test_loss 0.59363317 \n",
      "企業倫理 | 811 train_loss 0.64697117 valid_loss 0.67057222 test_loss 0.59146941 \n",
      "企業倫理 | 812 train_loss 0.64891189 valid_loss 0.67314041 test_loss 0.59286010 \n",
      "企業倫理 | 813 train_loss 0.64709008 valid_loss 0.67060798 test_loss 0.59145665 \n",
      "企業倫理 | 814 train_loss 0.64912289 valid_loss 0.67266649 test_loss 0.59460622 \n",
      "企業倫理 | 815 train_loss 0.64942485 valid_loss 0.67353475 test_loss 0.59305090 \n",
      "企業倫理 | 816 train_loss 0.64698690 valid_loss 0.67052960 test_loss 0.59152973 \n",
      "企業倫理 | 817 train_loss 0.64788789 valid_loss 0.67150879 test_loss 0.59313375 \n",
      "企業倫理 | 818 train_loss 0.65282255 valid_loss 0.67701715 test_loss 0.59609824 \n",
      "企業倫理 | 819 train_loss 0.64894396 valid_loss 0.67270458 test_loss 0.59462196 \n",
      "企業倫理 | 820 train_loss 0.64980596 valid_loss 0.67317492 test_loss 0.59558457 \n",
      "企業倫理 | 821 train_loss 0.64698023 valid_loss 0.67075849 test_loss 0.59154135 \n",
      "企業倫理 | 822 train_loss 0.64762819 valid_loss 0.67100465 test_loss 0.59184623 \n",
      "企業倫理 | 823 train_loss 0.65031093 valid_loss 0.67460543 test_loss 0.59407163 \n",
      "企業倫理 | 824 train_loss 0.64701170 valid_loss 0.67086512 test_loss 0.59190404 \n",
      "企業倫理 | 825 train_loss 0.64840972 valid_loss 0.67226940 test_loss 0.59245032 \n",
      "企業倫理 | 826 train_loss 0.65055484 valid_loss 0.67450643 test_loss 0.59412658 \n",
      "企業倫理 | 827 train_loss 0.64804882 valid_loss 0.67193431 test_loss 0.59202045 \n",
      "企業倫理 | 828 train_loss 0.64746541 valid_loss 0.67062497 test_loss 0.59209275 \n",
      "企業倫理 | 829 train_loss 0.65053940 valid_loss 0.67418343 test_loss 0.59664601 \n",
      "企業倫理 | 830 train_loss 0.64702505 valid_loss 0.67054170 test_loss 0.59196949 \n",
      "企業倫理 | 831 train_loss 0.66448617 valid_loss 0.68939906 test_loss 0.60690343 \n",
      "企業倫理 | 832 train_loss 0.65427762 valid_loss 0.67808497 test_loss 0.60093844 \n",
      "企業倫理 | 833 train_loss 0.64786839 valid_loss 0.67146373 test_loss 0.59196472 \n",
      "企業倫理 | 834 train_loss 0.64704365 valid_loss 0.67044860 test_loss 0.59183377 \n",
      "企業倫理 | 835 train_loss 0.64806992 valid_loss 0.67180216 test_loss 0.59221578 \n",
      "企業倫理 | 836 train_loss 0.64708775 valid_loss 0.67097694 test_loss 0.59197497 \n",
      "企業倫理 | 837 train_loss 0.67398697 valid_loss 0.69854575 test_loss 0.62269324 \n",
      "企業倫理 | 838 train_loss 0.64971346 valid_loss 0.67319822 test_loss 0.59546697 \n",
      "企業倫理 | 839 train_loss 0.64752811 valid_loss 0.67159694 test_loss 0.59224993 \n",
      "企業倫理 | 840 train_loss 0.65285236 valid_loss 0.67722565 test_loss 0.59630251 \n",
      "企業倫理 | 841 train_loss 0.64738017 valid_loss 0.67120296 test_loss 0.59249210 \n",
      "企業倫理 | 842 train_loss 0.64742035 valid_loss 0.67150968 test_loss 0.59202498 \n",
      "企業倫理 | 843 train_loss 0.64834207 valid_loss 0.67184430 test_loss 0.59379041 \n",
      "企業倫理 | 844 train_loss 0.64766002 valid_loss 0.67140025 test_loss 0.59303355 \n",
      "企業倫理 | 845 train_loss 0.65202183 valid_loss 0.67527455 test_loss 0.59806663 \n",
      "企業倫理 | 846 train_loss 0.65139329 valid_loss 0.67484057 test_loss 0.59760338 \n",
      "企業倫理 | 847 train_loss 0.64868844 valid_loss 0.67237800 test_loss 0.59432071 \n",
      "企業倫理 | 848 train_loss 0.64744085 valid_loss 0.67080265 test_loss 0.59251666 \n",
      "企業倫理 | 849 train_loss 0.64945012 valid_loss 0.67348844 test_loss 0.59315306 \n",
      "企業倫理 | 850 train_loss 0.65195781 valid_loss 0.67629969 test_loss 0.59807694 \n",
      "企業倫理 | 851 train_loss 0.64730173 valid_loss 0.67101228 test_loss 0.59154135 \n",
      "企業倫理 | 852 train_loss 0.64770967 valid_loss 0.67087299 test_loss 0.59216446 \n",
      "企業倫理 | 853 train_loss 0.64844137 valid_loss 0.67218542 test_loss 0.59389901 \n",
      "企業倫理 | 854 train_loss 0.64847261 valid_loss 0.67186862 test_loss 0.59397697 \n",
      "企業倫理 | 855 train_loss 0.64909172 valid_loss 0.67275709 test_loss 0.59296250 \n",
      "企業倫理 | 856 train_loss 0.64795536 valid_loss 0.67152417 test_loss 0.59329540 \n",
      "企業倫理 | 857 train_loss 0.64702427 valid_loss 0.67047238 test_loss 0.59161937 \n",
      "企業倫理 | 858 train_loss 0.64846289 valid_loss 0.67208105 test_loss 0.59394008 \n",
      "企業倫理 | 859 train_loss 0.64812559 valid_loss 0.67162997 test_loss 0.59347326 \n",
      "企業倫理 | 860 train_loss 0.64705461 valid_loss 0.67064261 test_loss 0.59185082 \n",
      "企業倫理 | 861 train_loss 0.64743161 valid_loss 0.67118514 test_loss 0.59165758 \n",
      "企業倫理 | 862 train_loss 0.64724684 valid_loss 0.67072952 test_loss 0.59163797 \n",
      "企業倫理 | 863 train_loss 0.64747149 valid_loss 0.67152756 test_loss 0.59226888 \n",
      "企業倫理 | 864 train_loss 0.64925623 valid_loss 0.67311031 test_loss 0.59311461 \n",
      "企業倫理 | 865 train_loss 0.64881986 valid_loss 0.67259073 test_loss 0.59277004 \n",
      "企業倫理 | 866 train_loss 0.64702481 valid_loss 0.67051035 test_loss 0.59174120 \n",
      "企業倫理 | 867 train_loss 0.64770406 valid_loss 0.67129904 test_loss 0.59182525 \n",
      "企業倫理 | 868 train_loss 0.64814836 valid_loss 0.67201608 test_loss 0.59224546 \n",
      "企業倫理 | 869 train_loss 0.64812297 valid_loss 0.67179000 test_loss 0.59222788 \n",
      "企業倫理 | 870 train_loss 0.65340739 valid_loss 0.67681283 test_loss 0.59969711 \n",
      "企業倫理 | 871 train_loss 0.65206546 valid_loss 0.67616200 test_loss 0.59543735 \n",
      "企業倫理 | 872 train_loss 0.64784390 valid_loss 0.67145550 test_loss 0.59208542 \n",
      "企業倫理 | 873 train_loss 0.64692348 valid_loss 0.67051983 test_loss 0.59154910 \n",
      "企業倫理 | 874 train_loss 0.65016174 valid_loss 0.67411673 test_loss 0.59392828 \n",
      "企業倫理 | 875 train_loss 0.64787960 valid_loss 0.67164516 test_loss 0.59326988 \n",
      "企業倫理 | 876 train_loss 0.64860147 valid_loss 0.67248887 test_loss 0.59257203 \n",
      "企業倫理 | 877 train_loss 0.65401143 valid_loss 0.67770427 test_loss 0.60053211 \n",
      "企業倫理 | 878 train_loss 0.64762485 valid_loss 0.67144376 test_loss 0.59191799 \n",
      "企業倫理 | 879 train_loss 0.64838827 valid_loss 0.67212683 test_loss 0.59237528 \n",
      "企業倫理 | 880 train_loss 0.64970118 valid_loss 0.67364663 test_loss 0.59545100 \n",
      "企業倫理 | 881 train_loss 0.64870018 valid_loss 0.67258912 test_loss 0.59424180 \n",
      "企業倫理 | 882 train_loss 0.64786029 valid_loss 0.67132485 test_loss 0.59316564 \n",
      "企業倫理 | 883 train_loss 0.64923078 valid_loss 0.67229116 test_loss 0.59458220 \n",
      "企業倫理 | 884 train_loss 0.65675408 valid_loss 0.68041533 test_loss 0.60353529 \n",
      "企業倫理 | 885 train_loss 0.64862520 valid_loss 0.67237246 test_loss 0.59254605 \n",
      "企業倫理 | 886 train_loss 0.64781111 valid_loss 0.67164171 test_loss 0.59173685 \n",
      "企業倫理 | 887 train_loss 0.65150613 valid_loss 0.67523396 test_loss 0.59754610 \n",
      "企業倫理 | 888 train_loss 0.64698493 valid_loss 0.67057884 test_loss 0.59168434 \n",
      "企業倫理 | 889 train_loss 0.65237558 valid_loss 0.67599857 test_loss 0.59847558 \n",
      "企業倫理 | 890 train_loss 0.64800435 valid_loss 0.67175424 test_loss 0.59201431 \n",
      "企業倫理 | 891 train_loss 0.64698225 valid_loss 0.67049390 test_loss 0.59151226 \n",
      "企業倫理 | 892 train_loss 0.65495515 valid_loss 0.67923117 test_loss 0.59806317 \n",
      "企業倫理 | 893 train_loss 0.64717972 valid_loss 0.67060757 test_loss 0.59201455 \n",
      "企業倫理 | 894 train_loss 0.64869612 valid_loss 0.67258722 test_loss 0.59254962 \n",
      "企業倫理 | 895 train_loss 0.64694136 valid_loss 0.67051184 test_loss 0.59144843 \n",
      "企業倫理 | 896 train_loss 0.64992201 valid_loss 0.67418092 test_loss 0.59357750 \n",
      "企業倫理 | 897 train_loss 0.64969814 valid_loss 0.67375106 test_loss 0.59349972 \n",
      "企業倫理 | 898 train_loss 0.65327424 valid_loss 0.67754024 test_loss 0.59652084 \n",
      "企業倫理 | 899 train_loss 0.65050006 valid_loss 0.67417014 test_loss 0.59399194 \n",
      "企業倫理 | 900 train_loss 0.64968574 valid_loss 0.67388487 test_loss 0.59331036 \n",
      "企業倫理 | 901 train_loss 0.64883554 valid_loss 0.67273760 test_loss 0.59436631 \n",
      "企業倫理 | 902 train_loss 0.65360147 valid_loss 0.67798531 test_loss 0.59676212 \n",
      "企業倫理 | 903 train_loss 0.64834785 valid_loss 0.67203844 test_loss 0.59371465 \n",
      "企業倫理 | 904 train_loss 0.64967734 valid_loss 0.67386413 test_loss 0.59335744 \n",
      "企業倫理 | 905 train_loss 0.65722317 valid_loss 0.68182129 test_loss 0.60001469 \n",
      "企業倫理 | 906 train_loss 0.65063167 valid_loss 0.67403746 test_loss 0.59648031 \n",
      "企業倫理 | 907 train_loss 0.64853245 valid_loss 0.67244947 test_loss 0.59244454 \n",
      "企業倫理 | 908 train_loss 0.64816499 valid_loss 0.67153031 test_loss 0.59338510 \n",
      "企業倫理 | 909 train_loss 0.65502465 valid_loss 0.67868626 test_loss 0.60155141 \n",
      "企業倫理 | 910 train_loss 0.64762384 valid_loss 0.67143840 test_loss 0.59183586 \n",
      "企業倫理 | 911 train_loss 0.64723390 valid_loss 0.67064732 test_loss 0.59205806 \n",
      "企業倫理 | 912 train_loss 0.65297097 valid_loss 0.67737865 test_loss 0.59629929 \n",
      "企業倫理 | 913 train_loss 0.64983302 valid_loss 0.67393976 test_loss 0.59350073 \n",
      "企業倫理 | 914 train_loss 0.64871913 valid_loss 0.67263031 test_loss 0.59265465 \n",
      "企業倫理 | 915 train_loss 0.64698964 valid_loss 0.67064226 test_loss 0.59146893 \n",
      "企業倫理 | 916 train_loss 0.64730889 valid_loss 0.67136055 test_loss 0.59184545 \n",
      "企業倫理 | 917 train_loss 0.64949632 valid_loss 0.67346185 test_loss 0.59317285 \n",
      "企業倫理 | 918 train_loss 0.64820570 valid_loss 0.67168587 test_loss 0.59351408 \n",
      "企業倫理 | 919 train_loss 0.65915501 valid_loss 0.68336129 test_loss 0.60620588 \n",
      "企業倫理 | 920 train_loss 0.64695299 valid_loss 0.67069215 test_loss 0.59144813 \n",
      "企業倫理 | 921 train_loss 0.64711422 valid_loss 0.67088813 test_loss 0.59129661 \n",
      "企業倫理 | 922 train_loss 0.64708471 valid_loss 0.67053390 test_loss 0.59175700 \n",
      "企業倫理 | 923 train_loss 0.64702326 valid_loss 0.67074311 test_loss 0.59166080 \n",
      "企業倫理 | 924 train_loss 0.64707112 valid_loss 0.67076701 test_loss 0.59150696 \n",
      "企業倫理 | 925 train_loss 0.64694756 valid_loss 0.67065090 test_loss 0.59153217 \n",
      "企業倫理 | 926 train_loss 0.65788054 valid_loss 0.68180519 test_loss 0.60483229 \n",
      "企業倫理 | 927 train_loss 0.64693695 valid_loss 0.67045933 test_loss 0.59141612 \n",
      "企業倫理 | 928 train_loss 0.64717299 valid_loss 0.67049950 test_loss 0.59183168 \n",
      "企業倫理 | 929 train_loss 0.65189546 valid_loss 0.67577481 test_loss 0.59796774 \n",
      "企業倫理 | 930 train_loss 0.64724302 valid_loss 0.67112917 test_loss 0.59145808 \n",
      "企業倫理 | 931 train_loss 0.64713353 valid_loss 0.67089647 test_loss 0.59201431 \n",
      "企業倫理 | 932 train_loss 0.64833206 valid_loss 0.67171508 test_loss 0.59357512 \n",
      "企業倫理 | 933 train_loss 0.64922440 valid_loss 0.67315578 test_loss 0.59304070 \n",
      "企業倫理 | 934 train_loss 0.64824051 valid_loss 0.67212278 test_loss 0.59219456 \n",
      "企業倫理 | 935 train_loss 0.64890718 valid_loss 0.67215645 test_loss 0.59420562 \n",
      "企業倫理 | 936 train_loss 0.64694691 valid_loss 0.67056119 test_loss 0.59159678 \n",
      "企業倫理 | 937 train_loss 0.64848953 valid_loss 0.67230803 test_loss 0.59395123 \n",
      "企業倫理 | 938 train_loss 0.64711803 valid_loss 0.67089170 test_loss 0.59161437 \n",
      "企業倫理 | 939 train_loss 0.64711893 valid_loss 0.67077368 test_loss 0.59205770 \n",
      "企業倫理 | 940 train_loss 0.64694750 valid_loss 0.67053354 test_loss 0.59150475 \n",
      "企業倫理 | 941 train_loss 0.65014780 valid_loss 0.67401218 test_loss 0.59387225 \n",
      "企業倫理 | 942 train_loss 0.65505415 valid_loss 0.67921770 test_loss 0.60190785 \n",
      "企業倫理 | 943 train_loss 0.65312624 valid_loss 0.67710137 test_loss 0.59650505 \n",
      "企業倫理 | 944 train_loss 0.65186536 valid_loss 0.67625934 test_loss 0.59538740 \n",
      "企業倫理 | 945 train_loss 0.64712012 valid_loss 0.67108154 test_loss 0.59185624 \n",
      "企業倫理 | 946 train_loss 0.64717096 valid_loss 0.67082387 test_loss 0.59213823 \n",
      "企業倫理 | 947 train_loss 0.65126181 valid_loss 0.67521071 test_loss 0.59486645 \n",
      "企業倫理 | 948 train_loss 0.64692950 valid_loss 0.67058611 test_loss 0.59152180 \n",
      "企業倫理 | 949 train_loss 0.64723825 valid_loss 0.67052174 test_loss 0.59211540 \n",
      "企業倫理 | 950 train_loss 0.64697242 valid_loss 0.67049199 test_loss 0.59155649 \n",
      "企業倫理 | 951 train_loss 0.64896494 valid_loss 0.67294300 test_loss 0.59458160 \n",
      "企業倫理 | 952 train_loss 0.64714819 valid_loss 0.67060924 test_loss 0.59203041 \n",
      "企業倫理 | 953 train_loss 0.64791101 valid_loss 0.67175919 test_loss 0.59190041 \n",
      "企業倫理 | 954 train_loss 0.64938676 valid_loss 0.67346483 test_loss 0.59327358 \n",
      "企業倫理 | 955 train_loss 0.64989746 valid_loss 0.67301214 test_loss 0.59553885 \n",
      "企業倫理 | 956 train_loss 0.64703846 valid_loss 0.67072070 test_loss 0.59192067 \n",
      "企業倫理 | 957 train_loss 0.64955771 valid_loss 0.67311972 test_loss 0.59529823 \n",
      "企業倫理 | 958 train_loss 0.64773256 valid_loss 0.67138368 test_loss 0.59290493 \n",
      "企業倫理 | 959 train_loss 0.64711958 valid_loss 0.67067879 test_loss 0.59145570 \n",
      "企業倫理 | 960 train_loss 0.64696419 valid_loss 0.67056626 test_loss 0.59163308 \n",
      "企業倫理 | 961 train_loss 0.64757043 valid_loss 0.67132485 test_loss 0.59266376 \n",
      "企業倫理 | 962 train_loss 0.64968073 valid_loss 0.67394507 test_loss 0.59337938 \n",
      "企業倫理 | 963 train_loss 0.64775497 valid_loss 0.67095089 test_loss 0.59267604 \n",
      "企業倫理 | 964 train_loss 0.64745766 valid_loss 0.67094731 test_loss 0.59160691 \n",
      "企業倫理 | 965 train_loss 0.65867639 valid_loss 0.68358725 test_loss 0.60136199 \n",
      "企業倫理 | 966 train_loss 0.65421873 valid_loss 0.67862368 test_loss 0.59728622 \n",
      "企業倫理 | 967 train_loss 0.64951861 valid_loss 0.67311770 test_loss 0.59503943 \n",
      "企業倫理 | 968 train_loss 0.65070105 valid_loss 0.67491776 test_loss 0.59413570 \n",
      "企業倫理 | 969 train_loss 0.64821875 valid_loss 0.67205077 test_loss 0.59357154 \n",
      "企業倫理 | 970 train_loss 0.64727825 valid_loss 0.67048776 test_loss 0.59181982 \n",
      "企業倫理 | 971 train_loss 0.64733601 valid_loss 0.67126787 test_loss 0.59172142 \n",
      "企業倫理 | 972 train_loss 0.65696400 valid_loss 0.68158543 test_loss 0.59990788 \n",
      "企業倫理 | 973 train_loss 0.65035295 valid_loss 0.67448145 test_loss 0.59391886 \n",
      "企業倫理 | 974 train_loss 0.64842832 valid_loss 0.67245549 test_loss 0.59233832 \n",
      "企業倫理 | 975 train_loss 0.65212888 valid_loss 0.67634642 test_loss 0.59546483 \n",
      "企業倫理 | 976 train_loss 0.64819592 valid_loss 0.67190528 test_loss 0.59360176 \n",
      "企業倫理 | 977 train_loss 0.65378869 valid_loss 0.67778099 test_loss 0.60028738 \n",
      "企業倫理 | 978 train_loss 0.64791358 valid_loss 0.67181444 test_loss 0.59208065 \n",
      "企業倫理 | 979 train_loss 0.64732140 valid_loss 0.67108697 test_loss 0.59219146 \n",
      "企業倫理 | 980 train_loss 0.64815164 valid_loss 0.67154312 test_loss 0.59343868 \n",
      "企業倫理 | 981 train_loss 0.65042466 valid_loss 0.67383677 test_loss 0.59625196 \n",
      "企業倫理 | 982 train_loss 0.64872867 valid_loss 0.67263925 test_loss 0.59265745 \n",
      "企業倫理 | 983 train_loss 0.65207982 valid_loss 0.67596728 test_loss 0.59544218 \n",
      "企業倫理 | 984 train_loss 0.64839053 valid_loss 0.67199558 test_loss 0.59375685 \n",
      "企業倫理 | 985 train_loss 0.64700794 valid_loss 0.67062318 test_loss 0.59164500 \n",
      "企業倫理 | 986 train_loss 0.64716524 valid_loss 0.67109853 test_loss 0.59169221 \n",
      "企業倫理 | 987 train_loss 0.64717406 valid_loss 0.67092186 test_loss 0.59210980 \n",
      "企業倫理 | 988 train_loss 0.65189874 valid_loss 0.67569286 test_loss 0.59804857 \n",
      "企業倫理 | 989 train_loss 0.64704692 valid_loss 0.67053246 test_loss 0.59157312 \n",
      "企業倫理 | 990 train_loss 0.64717406 valid_loss 0.67070782 test_loss 0.59209526 \n",
      "企業倫理 | 991 train_loss 0.64788079 valid_loss 0.67110187 test_loss 0.59284854 \n",
      "企業倫理 | 992 train_loss 0.64781725 valid_loss 0.67152166 test_loss 0.59308237 \n",
      "企業倫理 | 993 train_loss 0.64715147 valid_loss 0.67059463 test_loss 0.59193033 \n",
      "企業倫理 | 994 train_loss 0.64708704 valid_loss 0.67094636 test_loss 0.59175295 \n",
      "企業倫理 | 995 train_loss 0.64792681 valid_loss 0.67170209 test_loss 0.59322655 \n",
      "企業倫理 | 996 train_loss 0.65120035 valid_loss 0.67544532 test_loss 0.59467000 \n",
      "企業倫理 | 997 train_loss 0.64731002 valid_loss 0.67086309 test_loss 0.59149438 \n",
      "企業倫理 | 998 train_loss 0.65027297 valid_loss 0.67421561 test_loss 0.59389716 \n",
      "企業倫理 | 999 train_loss 0.65204406 valid_loss 0.67577034 test_loss 0.59817976 \n",
      "企業倫理 | 1000 train_loss 0.64700782 valid_loss 0.67043763 test_loss 0.59134519 \n",
      "ReTraining done, RMSE: 0.9484657049179077 => 0.6704117059707642\n",
      "decrease: 0.27805399894714355\n",
      "\n",
      "Now processing model: 大數據的設計思考\n",
      "original RMSE: 0.8016650676727295\n",
      "大數據的設計思考 | 1 train_loss 0.55522573 valid_loss 0.62246299 test_loss 0.59136385 < save\n",
      "大數據的設計思考 | 2 train_loss 0.55139989 valid_loss 0.61650968 test_loss 0.58718812 < save\n",
      "大數據的設計思考 | 3 train_loss 0.54970413 valid_loss 0.61429149 test_loss 0.58498538 < save\n",
      "大數據的設計思考 | 4 train_loss 0.55067772 valid_loss 0.61384988 test_loss 0.58667666 < save\n",
      "大數據的設計思考 | 5 train_loss 0.54750198 valid_loss 0.61225200 test_loss 0.58248889 < save\n",
      "大數據的設計思考 | 6 train_loss 0.54723024 valid_loss 0.60952359 test_loss 0.58195210 < save\n",
      "大數據的設計思考 | 7 train_loss 0.54538345 valid_loss 0.60822558 test_loss 0.57998341 < save\n",
      "大數據的設計思考 | 8 train_loss 0.54446083 valid_loss 0.60809487 test_loss 0.57913023 < save\n",
      "大數據的設計思考 | 9 train_loss 0.54390889 valid_loss 0.60747635 test_loss 0.57834059 < save\n",
      "大數據的設計思考 | 10 train_loss 0.54486471 valid_loss 0.60944086 test_loss 0.57928991 \n",
      "大數據的設計思考 | 11 train_loss 0.54403627 valid_loss 0.60582793 test_loss 0.57871091 < save\n",
      "大數據的設計思考 | 12 train_loss 0.54229206 valid_loss 0.60488349 test_loss 0.57626116 < save\n",
      "大數據的設計思考 | 13 train_loss 0.54430252 valid_loss 0.60476351 test_loss 0.57817459 < save\n",
      "大數據的設計思考 | 14 train_loss 0.54200655 valid_loss 0.60369456 test_loss 0.57616431 < save\n",
      "大數據的設計思考 | 15 train_loss 0.54150975 valid_loss 0.60335547 test_loss 0.57542151 < save\n",
      "大數據的設計思考 | 16 train_loss 0.54130936 valid_loss 0.60401541 test_loss 0.57520151 \n",
      "大數據的設計思考 | 17 train_loss 0.54215389 valid_loss 0.60285431 test_loss 0.57592642 < save\n",
      "大數據的設計思考 | 18 train_loss 0.54190052 valid_loss 0.60271752 test_loss 0.57572550 < save\n",
      "大數據的設計思考 | 19 train_loss 0.54143906 valid_loss 0.60478038 test_loss 0.57549775 \n",
      "大數據的設計思考 | 20 train_loss 0.54244834 valid_loss 0.60617548 test_loss 0.57576203 \n",
      "大數據的設計思考 | 21 train_loss 0.54031944 valid_loss 0.60194081 test_loss 0.57356256 < save\n",
      "大數據的設計思考 | 22 train_loss 0.54117441 valid_loss 0.60167336 test_loss 0.57455570 < save\n",
      "大數據的設計思考 | 23 train_loss 0.54015338 valid_loss 0.60140783 test_loss 0.57315761 < save\n",
      "大數據的設計思考 | 24 train_loss 0.54264927 valid_loss 0.60262167 test_loss 0.57627773 \n",
      "大數據的設計思考 | 25 train_loss 0.54050219 valid_loss 0.60329628 test_loss 0.57371444 \n",
      "大數據的設計思考 | 26 train_loss 0.53971291 valid_loss 0.60120445 test_loss 0.57291311 < save\n",
      "大數據的設計思考 | 27 train_loss 0.54101497 valid_loss 0.60422498 test_loss 0.57422781 \n",
      "大數據的設計思考 | 28 train_loss 0.54163909 valid_loss 0.60123473 test_loss 0.57455230 \n",
      "大數據的設計思考 | 29 train_loss 0.53943449 valid_loss 0.60137528 test_loss 0.57280159 \n",
      "大數據的設計思考 | 30 train_loss 0.54083043 valid_loss 0.60091048 test_loss 0.57409191 < save\n",
      "大數據的設計思考 | 31 train_loss 0.53976798 valid_loss 0.60065734 test_loss 0.57311994 < save\n",
      "大數據的設計思考 | 32 train_loss 0.53914595 valid_loss 0.60108900 test_loss 0.57245553 \n",
      "大數據的設計思考 | 33 train_loss 0.54002547 valid_loss 0.60328156 test_loss 0.57359833 \n",
      "大數據的設計思考 | 34 train_loss 0.53959692 valid_loss 0.60005683 test_loss 0.57253098 < save\n",
      "大數據的設計思考 | 35 train_loss 0.54183650 valid_loss 0.60194987 test_loss 0.57562524 \n",
      "大數據的設計思考 | 36 train_loss 0.54067713 valid_loss 0.60422099 test_loss 0.57380849 \n",
      "大數據的設計思考 | 37 train_loss 0.53897297 valid_loss 0.60100257 test_loss 0.57169765 \n",
      "大數據的設計思考 | 38 train_loss 0.53859264 valid_loss 0.60027027 test_loss 0.57153988 \n",
      "大數據的設計思考 | 39 train_loss 0.53871042 valid_loss 0.60061508 test_loss 0.57220620 \n",
      "大數據的設計思考 | 40 train_loss 0.53865808 valid_loss 0.60081279 test_loss 0.57148653 \n",
      "大數據的設計思考 | 41 train_loss 0.53872246 valid_loss 0.60037094 test_loss 0.57099968 \n",
      "大數據的設計思考 | 42 train_loss 0.53901696 valid_loss 0.60189396 test_loss 0.57212758 \n",
      "大數據的設計思考 | 43 train_loss 0.53846049 valid_loss 0.60048354 test_loss 0.57118946 \n",
      "大數據的設計思考 | 44 train_loss 0.53858548 valid_loss 0.60103792 test_loss 0.57137799 \n",
      "大數據的設計思考 | 45 train_loss 0.53806829 valid_loss 0.59985250 test_loss 0.57091939 < save\n",
      "大數據的設計思考 | 46 train_loss 0.53825623 valid_loss 0.60062003 test_loss 0.57156420 \n",
      "大數據的設計思考 | 47 train_loss 0.54213959 valid_loss 0.60153961 test_loss 0.57554621 \n",
      "大數據的設計思考 | 48 train_loss 0.53802115 valid_loss 0.59895158 test_loss 0.57014257 < save\n",
      "大數據的設計思考 | 49 train_loss 0.54005373 valid_loss 0.60357565 test_loss 0.57268184 \n",
      "大數據的設計思考 | 50 train_loss 0.54157627 valid_loss 0.60544741 test_loss 0.57396215 \n",
      "大數據的設計思考 | 51 train_loss 0.53850937 valid_loss 0.60126054 test_loss 0.57121599 \n",
      "大數據的設計思考 | 52 train_loss 0.53803915 valid_loss 0.60046029 test_loss 0.57083768 \n",
      "大數據的設計思考 | 53 train_loss 0.53811938 valid_loss 0.59902763 test_loss 0.57114977 \n",
      "大數據的設計思考 | 54 train_loss 0.53844005 valid_loss 0.60131335 test_loss 0.57122928 \n",
      "大數據的設計思考 | 55 train_loss 0.53781414 valid_loss 0.59853232 test_loss 0.57052964 < save\n",
      "大數據的設計思考 | 56 train_loss 0.53790981 valid_loss 0.59840018 test_loss 0.57045531 < save\n",
      "大數據的設計思考 | 57 train_loss 0.53766376 valid_loss 0.59940112 test_loss 0.57096255 \n",
      "大數據的設計思考 | 58 train_loss 0.53861308 valid_loss 0.60158437 test_loss 0.57112581 \n",
      "大數據的設計思考 | 59 train_loss 0.53741354 valid_loss 0.59827185 test_loss 0.56954229 < save\n",
      "大數據的設計思考 | 60 train_loss 0.53748912 valid_loss 0.59824300 test_loss 0.56991655 < save\n",
      "大數據的設計思考 | 61 train_loss 0.53774810 valid_loss 0.59829581 test_loss 0.57036716 \n",
      "大數據的設計思考 | 62 train_loss 0.54091829 valid_loss 0.59989947 test_loss 0.57338309 \n",
      "大數據的設計思考 | 63 train_loss 0.53756517 valid_loss 0.59857267 test_loss 0.56920284 \n",
      "大數據的設計思考 | 64 train_loss 0.53763556 valid_loss 0.59963554 test_loss 0.56969994 \n",
      "大數據的設計思考 | 65 train_loss 0.53805232 valid_loss 0.60057837 test_loss 0.57025939 \n",
      "大數據的設計思考 | 66 train_loss 0.53770888 valid_loss 0.60016352 test_loss 0.57007563 \n",
      "大數據的設計思考 | 67 train_loss 0.53956068 valid_loss 0.60309249 test_loss 0.57172197 \n",
      "大數據的設計思考 | 68 train_loss 0.53712505 valid_loss 0.59906369 test_loss 0.56954867 \n",
      "大數據的設計思考 | 69 train_loss 0.53921086 valid_loss 0.60285568 test_loss 0.57170242 \n",
      "大數據的設計思考 | 70 train_loss 0.53702015 valid_loss 0.59880280 test_loss 0.56974041 \n",
      "大數據的設計思考 | 71 train_loss 0.53709078 valid_loss 0.59794229 test_loss 0.56899083 < save\n",
      "大數據的設計思考 | 72 train_loss 0.53838921 valid_loss 0.60114306 test_loss 0.57037830 \n",
      "大數據的設計思考 | 73 train_loss 0.53784955 valid_loss 0.60027289 test_loss 0.56987959 \n",
      "大數據的設計思考 | 74 train_loss 0.53837669 valid_loss 0.60137820 test_loss 0.57062274 \n",
      "大數據的設計思考 | 75 train_loss 0.53723490 valid_loss 0.59979510 test_loss 0.57023370 \n",
      "大數據的設計思考 | 76 train_loss 0.53892934 valid_loss 0.59851491 test_loss 0.57113826 \n",
      "大數據的設計思考 | 77 train_loss 0.54320109 valid_loss 0.60202414 test_loss 0.57612133 \n",
      "大數據的設計思考 | 78 train_loss 0.53741229 valid_loss 0.59757876 test_loss 0.56927818 < save\n",
      "大數據的設計思考 | 79 train_loss 0.53712964 valid_loss 0.59774387 test_loss 0.56940532 \n",
      "大數據的設計思考 | 80 train_loss 0.53676021 valid_loss 0.59823936 test_loss 0.56885326 \n",
      "大數據的設計思考 | 81 train_loss 0.53678495 valid_loss 0.59801000 test_loss 0.56878346 \n",
      "大數據的設計思考 | 82 train_loss 0.53720522 valid_loss 0.59873915 test_loss 0.57027739 \n",
      "大數據的設計思考 | 83 train_loss 0.53850448 valid_loss 0.60178638 test_loss 0.57071662 \n",
      "大數據的設計思考 | 84 train_loss 0.53686118 valid_loss 0.59777707 test_loss 0.56851393 \n",
      "大數據的設計思考 | 85 train_loss 0.53864557 valid_loss 0.59829974 test_loss 0.57073849 \n",
      "大數據的設計思考 | 86 train_loss 0.53711927 valid_loss 0.59931421 test_loss 0.56917685 \n",
      "大數據的設計思考 | 87 train_loss 0.53664565 valid_loss 0.59796703 test_loss 0.56886685 \n",
      "大數據的設計思考 | 88 train_loss 0.53823090 valid_loss 0.60143769 test_loss 0.57059467 \n",
      "大數據的設計思考 | 89 train_loss 0.53755647 valid_loss 0.59761363 test_loss 0.56954604 \n",
      "大數據的設計思考 | 90 train_loss 0.53689080 valid_loss 0.59919113 test_loss 0.56926250 \n",
      "大數據的設計思考 | 91 train_loss 0.53690469 valid_loss 0.59894645 test_loss 0.56893265 \n",
      "大數據的設計思考 | 92 train_loss 0.53791195 valid_loss 0.59811670 test_loss 0.57029247 \n",
      "大數據的設計思考 | 93 train_loss 0.53841645 valid_loss 0.60168964 test_loss 0.57063997 \n",
      "大數據的設計思考 | 94 train_loss 0.53674275 valid_loss 0.59852415 test_loss 0.56947225 \n",
      "大數據的設計思考 | 95 train_loss 0.53689986 valid_loss 0.59884340 test_loss 0.56875288 \n",
      "大數據的設計思考 | 96 train_loss 0.53662699 valid_loss 0.59856385 test_loss 0.56879526 \n",
      "大數據的設計思考 | 97 train_loss 0.53658646 valid_loss 0.59849417 test_loss 0.56867409 \n",
      "大數據的設計思考 | 98 train_loss 0.53694469 valid_loss 0.59819132 test_loss 0.56964731 \n",
      "大數據的設計思考 | 99 train_loss 0.53661275 valid_loss 0.59822488 test_loss 0.56843621 \n",
      "大數據的設計思考 | 100 train_loss 0.53677249 valid_loss 0.59810108 test_loss 0.56935710 \n",
      "大數據的設計思考 | 101 train_loss 0.53654015 valid_loss 0.59814364 test_loss 0.56840181 \n",
      "大數據的設計思考 | 102 train_loss 0.53670144 valid_loss 0.59750819 test_loss 0.56814969 < save\n",
      "大數據的設計思考 | 103 train_loss 0.53684562 valid_loss 0.59826881 test_loss 0.56955111 \n",
      "大數據的設計思考 | 104 train_loss 0.54270422 valid_loss 0.60131866 test_loss 0.57516223 \n",
      "大數據的設計思考 | 105 train_loss 0.53753418 valid_loss 0.59839326 test_loss 0.57035732 \n",
      "大數據的設計思考 | 106 train_loss 0.53880519 valid_loss 0.60251153 test_loss 0.57101417 \n",
      "大數據的設計思考 | 107 train_loss 0.53717798 valid_loss 0.59738702 test_loss 0.56918430 < save\n",
      "大數據的設計思考 | 108 train_loss 0.53639084 valid_loss 0.59805566 test_loss 0.56865507 \n",
      "大數據的設計思考 | 109 train_loss 0.53741723 valid_loss 0.59800541 test_loss 0.56993455 \n",
      "大數據的設計思考 | 110 train_loss 0.53659594 valid_loss 0.59869504 test_loss 0.56925941 \n",
      "大數據的設計思考 | 111 train_loss 0.53678894 valid_loss 0.59727353 test_loss 0.56871849 < save\n",
      "大數據的設計思考 | 112 train_loss 0.53634387 valid_loss 0.59777880 test_loss 0.56844521 \n",
      "大數據的設計思考 | 113 train_loss 0.53763276 valid_loss 0.59794730 test_loss 0.57006145 \n",
      "大數據的設計思考 | 114 train_loss 0.53657597 valid_loss 0.59812135 test_loss 0.56816089 \n",
      "大數據的設計思考 | 115 train_loss 0.53646332 valid_loss 0.59760606 test_loss 0.56873488 \n",
      "大數據的設計思考 | 116 train_loss 0.53856868 valid_loss 0.59820074 test_loss 0.57074696 \n",
      "大數據的設計思考 | 117 train_loss 0.53764307 valid_loss 0.59757566 test_loss 0.56954652 \n",
      "大數據的設計思考 | 118 train_loss 0.53657520 valid_loss 0.59706700 test_loss 0.56828713 < save\n",
      "大數據的設計思考 | 119 train_loss 0.53765315 valid_loss 0.59766740 test_loss 0.56972021 \n",
      "大數據的設計思考 | 120 train_loss 0.53633380 valid_loss 0.59826779 test_loss 0.56848305 \n",
      "大數據的設計思考 | 121 train_loss 0.53624433 valid_loss 0.59764373 test_loss 0.56822753 \n",
      "大數據的設計思考 | 122 train_loss 0.53747946 valid_loss 0.60043204 test_loss 0.56945133 \n",
      "大數據的設計思考 | 123 train_loss 0.54499382 valid_loss 0.61060685 test_loss 0.57697594 \n",
      "大數據的設計思考 | 124 train_loss 0.53632021 valid_loss 0.59734994 test_loss 0.56791991 \n",
      "大數據的設計思考 | 125 train_loss 0.53630155 valid_loss 0.59814531 test_loss 0.56846642 \n",
      "大數據的設計思考 | 126 train_loss 0.53634560 valid_loss 0.59748638 test_loss 0.56842750 \n",
      "大數據的設計思考 | 127 train_loss 0.53674209 valid_loss 0.59951675 test_loss 0.56900460 \n",
      "大數據的設計思考 | 128 train_loss 0.53696311 valid_loss 0.59719032 test_loss 0.56875515 \n",
      "大數據的設計思考 | 129 train_loss 0.53626770 valid_loss 0.59737438 test_loss 0.56783694 \n",
      "大數據的設計思考 | 130 train_loss 0.53648835 valid_loss 0.59704208 test_loss 0.56782275 < save\n",
      "大數據的設計思考 | 131 train_loss 0.54042828 valid_loss 0.59980118 test_loss 0.57284713 \n",
      "大數據的設計思考 | 132 train_loss 0.53881973 valid_loss 0.59805936 test_loss 0.57060993 \n",
      "大數據的設計思考 | 133 train_loss 0.53617167 valid_loss 0.59750611 test_loss 0.56805086 \n",
      "大數據的設計思考 | 134 train_loss 0.53652847 valid_loss 0.59880251 test_loss 0.56922591 \n",
      "大數據的設計思考 | 135 train_loss 0.53655446 valid_loss 0.59707713 test_loss 0.56837368 \n",
      "大數據的設計思考 | 136 train_loss 0.53686148 valid_loss 0.59970480 test_loss 0.56909066 \n",
      "大數據的設計思考 | 137 train_loss 0.53783637 valid_loss 0.60136813 test_loss 0.57001954 \n",
      "大數據的設計思考 | 138 train_loss 0.53625619 valid_loss 0.59825081 test_loss 0.56854916 \n",
      "大數據的設計思考 | 139 train_loss 0.53861248 valid_loss 0.60249978 test_loss 0.57076240 \n",
      "大數據的設計思考 | 140 train_loss 0.53637630 valid_loss 0.59867233 test_loss 0.56840235 \n",
      "大數據的設計思考 | 141 train_loss 0.53618777 valid_loss 0.59767020 test_loss 0.56782717 \n",
      "大數據的設計思考 | 142 train_loss 0.53633070 valid_loss 0.59858596 test_loss 0.56826961 \n",
      "大數據的設計思考 | 143 train_loss 0.53655142 valid_loss 0.59700322 test_loss 0.56812537 < save\n",
      "大數據的設計思考 | 144 train_loss 0.53671986 valid_loss 0.59705496 test_loss 0.56845129 \n",
      "大數據的設計思考 | 145 train_loss 0.53782755 valid_loss 0.60135400 test_loss 0.56995779 \n",
      "大數據的設計思考 | 146 train_loss 0.53609365 valid_loss 0.59793872 test_loss 0.56811869 \n",
      "大數據的設計思考 | 147 train_loss 0.53740752 valid_loss 0.60028321 test_loss 0.56923282 \n",
      "大數據的設計思考 | 148 train_loss 0.53626806 valid_loss 0.59714532 test_loss 0.56753910 \n",
      "大數據的設計思考 | 149 train_loss 0.53719413 valid_loss 0.59751344 test_loss 0.56933856 \n",
      "大數據的設計思考 | 150 train_loss 0.53758615 valid_loss 0.60122555 test_loss 0.57016665 \n",
      "大數據的設計思考 | 151 train_loss 0.53787738 valid_loss 0.60103208 test_loss 0.56950927 \n",
      "大數據的設計思考 | 152 train_loss 0.53678864 valid_loss 0.59676379 test_loss 0.56816906 < save\n",
      "大數據的設計思考 | 153 train_loss 0.53894675 valid_loss 0.59891236 test_loss 0.57157058 \n",
      "大數據的設計思考 | 154 train_loss 0.53627074 valid_loss 0.59690124 test_loss 0.56797969 \n",
      "大數據的設計思考 | 155 train_loss 0.53658938 valid_loss 0.59887803 test_loss 0.56835967 \n",
      "大數據的設計思考 | 156 train_loss 0.53649396 valid_loss 0.59740311 test_loss 0.56872576 \n",
      "大數據的設計思考 | 157 train_loss 0.53763437 valid_loss 0.60086954 test_loss 0.56955045 \n",
      "大數據的設計思考 | 158 train_loss 0.53656763 valid_loss 0.59710103 test_loss 0.56845671 \n",
      "大數據的設計思考 | 159 train_loss 0.53623039 valid_loss 0.59836781 test_loss 0.56807172 \n",
      "大數據的設計思考 | 160 train_loss 0.53649694 valid_loss 0.59744298 test_loss 0.56874818 \n",
      "大數據的設計思考 | 161 train_loss 0.53669894 valid_loss 0.59940255 test_loss 0.56859034 \n",
      "大數據的設計思考 | 162 train_loss 0.53658652 valid_loss 0.59748542 test_loss 0.56882578 \n",
      "大數據的設計思考 | 163 train_loss 0.53597736 valid_loss 0.59744757 test_loss 0.56796938 \n",
      "大數據的設計思考 | 164 train_loss 0.53772736 valid_loss 0.59743834 test_loss 0.56934857 \n",
      "大數據的設計思考 | 165 train_loss 0.53623211 valid_loss 0.59702420 test_loss 0.56811082 \n",
      "大數據的設計思考 | 166 train_loss 0.54049945 valid_loss 0.59902859 test_loss 0.57198352 \n",
      "大數據的設計思考 | 167 train_loss 0.53618491 valid_loss 0.59827167 test_loss 0.56789273 \n",
      "大數據的設計思考 | 168 train_loss 0.53608549 valid_loss 0.59690589 test_loss 0.56771874 \n",
      "大數據的設計思考 | 169 train_loss 0.53618300 valid_loss 0.59828472 test_loss 0.56868368 \n",
      "大數據的設計思考 | 170 train_loss 0.53594464 valid_loss 0.59733456 test_loss 0.56743550 \n",
      "大數據的設計思考 | 171 train_loss 0.53602475 valid_loss 0.59748083 test_loss 0.56738532 \n",
      "大數據的設計思考 | 172 train_loss 0.53592294 valid_loss 0.59765357 test_loss 0.56782377 \n",
      "大數據的設計思考 | 173 train_loss 0.53674358 valid_loss 0.59708935 test_loss 0.56842184 \n",
      "大數據的設計思考 | 174 train_loss 0.53594452 valid_loss 0.59771347 test_loss 0.56765288 \n",
      "大數據的設計思考 | 175 train_loss 0.53670132 valid_loss 0.59956884 test_loss 0.56856501 \n",
      "大數據的設計思考 | 176 train_loss 0.53608352 valid_loss 0.59723556 test_loss 0.56812662 \n",
      "大數據的設計思考 | 177 train_loss 0.54140854 valid_loss 0.60026276 test_loss 0.57358938 \n",
      "大數據的設計思考 | 178 train_loss 0.53629065 valid_loss 0.59782571 test_loss 0.56748420 \n",
      "大數據的設計思考 | 179 train_loss 0.53624982 valid_loss 0.59877473 test_loss 0.56885052 \n",
      "大數據的設計思考 | 180 train_loss 0.53636575 valid_loss 0.59729725 test_loss 0.56852126 \n",
      "大數據的設計思考 | 181 train_loss 0.53781897 valid_loss 0.59800065 test_loss 0.57006532 \n",
      "大數據的設計思考 | 182 train_loss 0.53585261 valid_loss 0.59726495 test_loss 0.56765068 \n",
      "大數據的設計思考 | 183 train_loss 0.53628296 valid_loss 0.59832388 test_loss 0.56780881 \n",
      "大數據的設計思考 | 184 train_loss 0.53586185 valid_loss 0.59706759 test_loss 0.56759143 \n",
      "大數據的設計思考 | 185 train_loss 0.53583556 valid_loss 0.59718961 test_loss 0.56743371 \n",
      "大數據的設計思考 | 186 train_loss 0.53705996 valid_loss 0.59975404 test_loss 0.56864458 \n",
      "大數據的設計思考 | 187 train_loss 0.53580856 valid_loss 0.59736800 test_loss 0.56765479 \n",
      "大數據的設計思考 | 188 train_loss 0.53608972 valid_loss 0.59659767 test_loss 0.56726700 < save\n",
      "大數據的設計思考 | 189 train_loss 0.53715861 valid_loss 0.60021830 test_loss 0.56896710 \n",
      "大數據的設計思考 | 190 train_loss 0.53591961 valid_loss 0.59724373 test_loss 0.56799299 \n",
      "大數據的設計思考 | 191 train_loss 0.53680849 valid_loss 0.59732687 test_loss 0.56889904 \n",
      "大數據的設計思考 | 192 train_loss 0.53580379 valid_loss 0.59724849 test_loss 0.56749243 \n",
      "大數據的設計思考 | 193 train_loss 0.53710592 valid_loss 0.59716707 test_loss 0.56890363 \n",
      "大數據的設計思考 | 194 train_loss 0.53718448 valid_loss 0.59790021 test_loss 0.56972528 \n",
      "大數據的設計思考 | 195 train_loss 0.53668082 valid_loss 0.59895742 test_loss 0.56815803 \n",
      "大數據的設計思考 | 196 train_loss 0.53589100 valid_loss 0.59698594 test_loss 0.56774461 \n",
      "大數據的設計思考 | 197 train_loss 0.53605294 valid_loss 0.59820157 test_loss 0.56845391 \n",
      "大數據的設計思考 | 198 train_loss 0.53644097 valid_loss 0.59887427 test_loss 0.56816548 \n",
      "大數據的設計思考 | 199 train_loss 0.53621423 valid_loss 0.59829420 test_loss 0.56777418 \n",
      "大數據的設計思考 | 200 train_loss 0.53602672 valid_loss 0.59818673 test_loss 0.56783742 \n",
      "大數據的設計思考 | 201 train_loss 0.53577065 valid_loss 0.59712583 test_loss 0.56739312 \n",
      "大數據的設計思考 | 202 train_loss 0.53743619 valid_loss 0.60050374 test_loss 0.56904072 \n",
      "大數據的設計思考 | 203 train_loss 0.53624094 valid_loss 0.59724718 test_loss 0.56827086 \n",
      "大數據的設計思考 | 204 train_loss 0.53574365 valid_loss 0.59694004 test_loss 0.56741244 \n",
      "大數據的設計思考 | 205 train_loss 0.53953111 valid_loss 0.59937263 test_loss 0.57203555 \n",
      "大數據的設計思考 | 206 train_loss 0.53832644 valid_loss 0.59796631 test_loss 0.57027519 \n",
      "大數據的設計思考 | 207 train_loss 0.53599167 valid_loss 0.59810662 test_loss 0.56776261 \n",
      "大數據的設計思考 | 208 train_loss 0.53900713 valid_loss 0.59862936 test_loss 0.57125074 \n",
      "大數據的設計思考 | 209 train_loss 0.53791660 valid_loss 0.60138494 test_loss 0.56970781 \n",
      "大數據的設計思考 | 210 train_loss 0.53744406 valid_loss 0.59709734 test_loss 0.56909531 \n",
      "大數據的設計思考 | 211 train_loss 0.53575081 valid_loss 0.59747070 test_loss 0.56740749 \n",
      "大數據的設計思考 | 212 train_loss 0.53576416 valid_loss 0.59684849 test_loss 0.56712949 \n",
      "大數據的設計思考 | 213 train_loss 0.54039097 valid_loss 0.59920108 test_loss 0.57232845 \n",
      "大數據的設計思考 | 214 train_loss 0.53701353 valid_loss 0.59970647 test_loss 0.56846976 \n",
      "大數據的設計思考 | 215 train_loss 0.53591847 valid_loss 0.59781545 test_loss 0.56821305 \n",
      "大數據的設計思考 | 216 train_loss 0.53610486 valid_loss 0.59644598 test_loss 0.56753546 < save\n",
      "大數據的設計思考 | 217 train_loss 0.53611922 valid_loss 0.59764206 test_loss 0.56723666 \n",
      "大數據的設計思考 | 218 train_loss 0.53651500 valid_loss 0.59686428 test_loss 0.56843060 \n",
      "大數據的設計思考 | 219 train_loss 0.53730059 valid_loss 0.60066146 test_loss 0.56939763 \n",
      "大數據的設計思考 | 220 train_loss 0.53591281 valid_loss 0.59815621 test_loss 0.56825727 \n",
      "大數據的設計思考 | 221 train_loss 0.53630632 valid_loss 0.59686971 test_loss 0.56815755 \n",
      "大數據的設計思考 | 222 train_loss 0.53584367 valid_loss 0.59645736 test_loss 0.56706226 \n",
      "大數據的設計思考 | 223 train_loss 0.53626901 valid_loss 0.59636629 test_loss 0.56756991 < save\n",
      "大數據的設計思考 | 224 train_loss 0.53582656 valid_loss 0.59686339 test_loss 0.56707233 \n",
      "大數據的設計思考 | 225 train_loss 0.53577375 valid_loss 0.59674132 test_loss 0.56750375 \n",
      "大數據的設計思考 | 226 train_loss 0.53735811 valid_loss 0.60013211 test_loss 0.56870329 \n",
      "大數據的設計思考 | 227 train_loss 0.53572989 valid_loss 0.59732753 test_loss 0.56775993 \n",
      "大數據的設計思考 | 228 train_loss 0.53591132 valid_loss 0.59667623 test_loss 0.56749511 \n",
      "大數據的設計思考 | 229 train_loss 0.53827751 valid_loss 0.59781116 test_loss 0.57019269 \n",
      "大數據的設計思考 | 230 train_loss 0.53580248 valid_loss 0.59765625 test_loss 0.56772214 \n",
      "大數據的設計思考 | 231 train_loss 0.53630906 valid_loss 0.59673941 test_loss 0.56816828 \n",
      "大數據的設計思考 | 232 train_loss 0.53573292 valid_loss 0.59712243 test_loss 0.56711924 \n",
      "大數據的設計思考 | 233 train_loss 0.54052389 valid_loss 0.59951085 test_loss 0.57260805 \n",
      "大數據的設計思考 | 234 train_loss 0.53630900 valid_loss 0.59848809 test_loss 0.56773835 \n",
      "大數據的設計思考 | 235 train_loss 0.53890800 valid_loss 0.59794652 test_loss 0.57035530 \n",
      "大數據的設計思考 | 236 train_loss 0.53781831 valid_loss 0.60106593 test_loss 0.56935042 \n",
      "大數據的設計思考 | 237 train_loss 0.53585488 valid_loss 0.59650904 test_loss 0.56738448 \n",
      "大數據的設計思考 | 238 train_loss 0.53568929 valid_loss 0.59685534 test_loss 0.56703085 \n",
      "大數據的設計思考 | 239 train_loss 0.53635085 valid_loss 0.59651440 test_loss 0.56791091 \n",
      "大數據的設計思考 | 240 train_loss 0.53589332 valid_loss 0.59716034 test_loss 0.56700426 \n",
      "大數據的設計思考 | 241 train_loss 0.53690159 valid_loss 0.59674716 test_loss 0.56842971 \n",
      "大數據的設計思考 | 242 train_loss 0.53747350 valid_loss 0.59694815 test_loss 0.56875867 \n",
      "大數據的設計思考 | 243 train_loss 0.53565389 valid_loss 0.59682894 test_loss 0.56734866 \n",
      "大數據的設計思考 | 244 train_loss 0.53611147 valid_loss 0.59680200 test_loss 0.56794751 \n",
      "大數據的設計思考 | 245 train_loss 0.53590316 valid_loss 0.59824938 test_loss 0.56792974 \n",
      "大數據的設計思考 | 246 train_loss 0.53927588 valid_loss 0.60297501 test_loss 0.57066351 \n",
      "大數據的設計思考 | 247 train_loss 0.53693765 valid_loss 0.59713715 test_loss 0.56884885 \n",
      "大數據的設計思考 | 248 train_loss 0.53564131 valid_loss 0.59734571 test_loss 0.56757295 \n",
      "大數據的設計思考 | 249 train_loss 0.53622121 valid_loss 0.59766704 test_loss 0.56865633 \n",
      "大數據的設計思考 | 250 train_loss 0.53663075 valid_loss 0.59684932 test_loss 0.56838083 \n",
      "大數據的設計思考 | 251 train_loss 0.53636765 valid_loss 0.59901911 test_loss 0.56815660 \n",
      "大數據的設計思考 | 252 train_loss 0.53618860 valid_loss 0.59625930 test_loss 0.56758624 < save\n",
      "大數據的設計思考 | 253 train_loss 0.53622633 valid_loss 0.59649569 test_loss 0.56794709 \n",
      "大數據的設計思考 | 254 train_loss 0.53719383 valid_loss 0.60019922 test_loss 0.56876928 \n",
      "大數據的設計思考 | 255 train_loss 0.53627264 valid_loss 0.59888172 test_loss 0.56806153 \n",
      "大數據的設計思考 | 256 train_loss 0.53566206 valid_loss 0.59645391 test_loss 0.56674343 \n",
      "大數據的設計思考 | 257 train_loss 0.53556740 valid_loss 0.59641778 test_loss 0.56704265 \n",
      "大數據的設計思考 | 258 train_loss 0.53620464 valid_loss 0.59671348 test_loss 0.56811774 \n",
      "大數據的設計思考 | 259 train_loss 0.53597063 valid_loss 0.59619033 test_loss 0.56728917 < save\n",
      "大數據的設計思考 | 260 train_loss 0.53608203 valid_loss 0.59859210 test_loss 0.56806779 \n",
      "大數據的設計思考 | 261 train_loss 0.53586257 valid_loss 0.59654921 test_loss 0.56765485 \n",
      "大數據的設計思考 | 262 train_loss 0.53717238 valid_loss 0.59951317 test_loss 0.56829965 \n",
      "大數據的設計思考 | 263 train_loss 0.53552788 valid_loss 0.59672159 test_loss 0.56697106 \n",
      "大數據的設計思考 | 264 train_loss 0.53559470 valid_loss 0.59724206 test_loss 0.56718963 \n",
      "大數據的設計思考 | 265 train_loss 0.53571880 valid_loss 0.59780401 test_loss 0.56768596 \n",
      "大數據的設計思考 | 266 train_loss 0.53603315 valid_loss 0.59619969 test_loss 0.56738329 \n",
      "大數據的設計思考 | 267 train_loss 0.53657866 valid_loss 0.59683555 test_loss 0.56845248 \n",
      "大數據的設計思考 | 268 train_loss 0.53805560 valid_loss 0.59749556 test_loss 0.56979311 \n",
      "大數據的設計思考 | 269 train_loss 0.53663677 valid_loss 0.59732968 test_loss 0.56872845 \n",
      "大數據的設計思考 | 270 train_loss 0.53775430 valid_loss 0.60102552 test_loss 0.56931895 \n",
      "大數據的設計思考 | 271 train_loss 0.53912568 valid_loss 0.59859455 test_loss 0.57116526 \n",
      "大數據的設計思考 | 272 train_loss 0.53578049 valid_loss 0.59799880 test_loss 0.56755745 \n",
      "大數據的設計思考 | 273 train_loss 0.53662866 valid_loss 0.59858871 test_loss 0.56757438 \n",
      "大數據的設計思考 | 274 train_loss 0.54241127 valid_loss 0.60104960 test_loss 0.57447213 \n",
      "大數據的設計思考 | 275 train_loss 0.53885692 valid_loss 0.60239840 test_loss 0.57023036 \n",
      "大數據的設計思考 | 276 train_loss 0.53581363 valid_loss 0.59716433 test_loss 0.56686866 \n",
      "大數據的設計思考 | 277 train_loss 0.53628117 valid_loss 0.59819430 test_loss 0.56746447 \n",
      "大數據的設計思考 | 278 train_loss 0.53563392 valid_loss 0.59737688 test_loss 0.56767565 \n",
      "大數據的設計思考 | 279 train_loss 0.54009789 valid_loss 0.60380828 test_loss 0.57126886 \n",
      "大數據的設計思考 | 280 train_loss 0.53616655 valid_loss 0.59855127 test_loss 0.56781459 \n",
      "大數據的設計思考 | 281 train_loss 0.53565192 valid_loss 0.59663606 test_loss 0.56734347 \n",
      "大數據的設計思考 | 282 train_loss 0.53645319 valid_loss 0.59833419 test_loss 0.56750703 \n",
      "大數據的設計思考 | 283 train_loss 0.53811288 valid_loss 0.60162872 test_loss 0.56982934 \n",
      "大數據的設計思考 | 284 train_loss 0.53625959 valid_loss 0.59667629 test_loss 0.56806284 \n",
      "大數據的設計思考 | 285 train_loss 0.53599572 valid_loss 0.59621263 test_loss 0.56743145 \n",
      "大數據的設計思考 | 286 train_loss 0.53634465 valid_loss 0.59617871 test_loss 0.56777847 < save\n",
      "大數據的設計思考 | 287 train_loss 0.53585237 valid_loss 0.59727716 test_loss 0.56694692 \n",
      "大數據的設計思考 | 288 train_loss 0.53674430 valid_loss 0.59653312 test_loss 0.56832904 \n",
      "大數據的設計思考 | 289 train_loss 0.53733748 valid_loss 0.60016155 test_loss 0.56872779 \n",
      "大數據的設計思考 | 290 train_loss 0.53593802 valid_loss 0.59601736 test_loss 0.56722957 < save\n",
      "大數據的設計思考 | 291 train_loss 0.53654170 valid_loss 0.59927809 test_loss 0.56824446 \n",
      "大數據的設計思考 | 292 train_loss 0.53548336 valid_loss 0.59631020 test_loss 0.56703913 \n",
      "大數據的設計思考 | 293 train_loss 0.53539878 valid_loss 0.59663349 test_loss 0.56687558 \n",
      "大數據的設計思考 | 294 train_loss 0.53833908 valid_loss 0.59757936 test_loss 0.57000786 \n",
      "大數據的設計思考 | 295 train_loss 0.53842258 valid_loss 0.59824556 test_loss 0.57061046 \n",
      "大數據的設計思考 | 296 train_loss 0.53543913 valid_loss 0.59669310 test_loss 0.56685489 \n",
      "大數據的設計思考 | 297 train_loss 0.53674620 valid_loss 0.59955114 test_loss 0.56831992 \n",
      "大數據的設計思考 | 298 train_loss 0.53571326 valid_loss 0.59592229 test_loss 0.56678808 < save\n",
      "大數據的設計思考 | 299 train_loss 0.53662866 valid_loss 0.59876323 test_loss 0.56778973 \n",
      "大數據的設計思考 | 300 train_loss 0.53589112 valid_loss 0.59749848 test_loss 0.56702310 \n",
      "大數據的設計思考 | 301 train_loss 0.53580809 valid_loss 0.59598833 test_loss 0.56721985 \n",
      "大數據的設計思考 | 302 train_loss 0.53577822 valid_loss 0.59620553 test_loss 0.56747133 \n",
      "大數據的設計思考 | 303 train_loss 0.53550291 valid_loss 0.59597439 test_loss 0.56683415 \n",
      "大數據的設計思考 | 304 train_loss 0.53602469 valid_loss 0.59641391 test_loss 0.56776184 \n",
      "大數據的設計思考 | 305 train_loss 0.53853536 valid_loss 0.59837061 test_loss 0.57078642 \n",
      "大數據的設計思考 | 306 train_loss 0.53665012 valid_loss 0.59613270 test_loss 0.56787598 \n",
      "大數據的設計思考 | 307 train_loss 0.53548473 valid_loss 0.59699422 test_loss 0.56684059 \n",
      "大數據的設計思考 | 308 train_loss 0.53546953 valid_loss 0.59586990 test_loss 0.56647021 < save\n",
      "大數據的設計思考 | 309 train_loss 0.53719920 valid_loss 0.60008574 test_loss 0.56884050 \n",
      "大數據的設計思考 | 310 train_loss 0.53582817 valid_loss 0.59768003 test_loss 0.56716728 \n",
      "大數據的設計思考 | 311 train_loss 0.53797263 valid_loss 0.60111564 test_loss 0.56935376 \n",
      "大數據的設計思考 | 312 train_loss 0.53542840 valid_loss 0.59698075 test_loss 0.56721365 \n",
      "大數據的設計思考 | 313 train_loss 0.53989118 valid_loss 0.60430735 test_loss 0.57184583 \n",
      "大數據的設計思考 | 314 train_loss 0.53605407 valid_loss 0.59836334 test_loss 0.56753337 \n",
      "大數據的設計思考 | 315 train_loss 0.53550929 valid_loss 0.59634674 test_loss 0.56718880 \n",
      "大數據的設計思考 | 316 train_loss 0.53524727 valid_loss 0.59629685 test_loss 0.56663096 \n",
      "大數據的設計思考 | 317 train_loss 0.53523791 valid_loss 0.59606975 test_loss 0.56661570 \n",
      "大數據的設計思考 | 318 train_loss 0.53596729 valid_loss 0.59780258 test_loss 0.56721461 \n",
      "大數據的設計思考 | 319 train_loss 0.53523624 valid_loss 0.59636801 test_loss 0.56668562 \n",
      "大數據的設計思考 | 320 train_loss 0.53807008 valid_loss 0.59692806 test_loss 0.56939989 \n",
      "大數據的設計思考 | 321 train_loss 0.53595817 valid_loss 0.59562880 test_loss 0.56702042 < save\n",
      "大數據的設計思考 | 322 train_loss 0.53598559 valid_loss 0.59802741 test_loss 0.56734711 \n",
      "大數據的設計思考 | 323 train_loss 0.53890055 valid_loss 0.59782183 test_loss 0.57064700 \n",
      "大數據的設計思考 | 324 train_loss 0.53577662 valid_loss 0.59557217 test_loss 0.56705320 < save\n",
      "大數據的設計思考 | 325 train_loss 0.53589618 valid_loss 0.59828353 test_loss 0.56791770 \n",
      "大數據的設計思考 | 326 train_loss 0.53632265 valid_loss 0.59891963 test_loss 0.56798685 \n",
      "大數據的設計思考 | 327 train_loss 0.53539741 valid_loss 0.59612703 test_loss 0.56634128 \n",
      "大數據的設計思考 | 328 train_loss 0.53795314 valid_loss 0.59727734 test_loss 0.56985956 \n",
      "大數據的設計思考 | 329 train_loss 0.53538609 valid_loss 0.59694397 test_loss 0.56693494 \n",
      "大數據的設計思考 | 330 train_loss 0.53971756 valid_loss 0.60325402 test_loss 0.57104945 \n",
      "大數據的設計思考 | 331 train_loss 0.53582442 valid_loss 0.59778333 test_loss 0.56741917 \n",
      "大數據的設計思考 | 332 train_loss 0.53705764 valid_loss 0.59940469 test_loss 0.56828117 \n",
      "大數據的設計思考 | 333 train_loss 0.53569746 valid_loss 0.59559846 test_loss 0.56708008 \n",
      "大數據的設計思考 | 334 train_loss 0.53568083 valid_loss 0.59737772 test_loss 0.56703448 \n",
      "大數據的設計思考 | 335 train_loss 0.53865373 valid_loss 0.60121530 test_loss 0.56951231 \n",
      "大數據的設計思考 | 336 train_loss 0.53541672 valid_loss 0.59549713 test_loss 0.56676418 < save\n",
      "大數據的設計思考 | 337 train_loss 0.53527683 valid_loss 0.59592766 test_loss 0.56685925 \n",
      "大數據的設計思考 | 338 train_loss 0.53525543 valid_loss 0.59655660 test_loss 0.56672662 \n",
      "大數據的設計思考 | 339 train_loss 0.53545392 valid_loss 0.59693408 test_loss 0.56682557 \n",
      "大數據的設計思考 | 340 train_loss 0.53516167 valid_loss 0.59629595 test_loss 0.56670207 \n",
      "大數據的設計思考 | 341 train_loss 0.53689134 valid_loss 0.59627002 test_loss 0.56843334 \n",
      "大數據的設計思考 | 342 train_loss 0.53531033 valid_loss 0.59540796 test_loss 0.56650710 < save\n",
      "大數據的設計思考 | 343 train_loss 0.53556281 valid_loss 0.59543419 test_loss 0.56668860 \n",
      "大數據的設計思考 | 344 train_loss 0.53558004 valid_loss 0.59762943 test_loss 0.56737465 \n",
      "大數據的設計思考 | 345 train_loss 0.53696537 valid_loss 0.59977853 test_loss 0.56867266 \n",
      "大數據的設計思考 | 346 train_loss 0.53750801 valid_loss 0.59649414 test_loss 0.56899208 \n",
      "大數據的設計思考 | 347 train_loss 0.53534865 valid_loss 0.59660971 test_loss 0.56725764 \n",
      "大數據的設計思考 | 348 train_loss 0.53523952 valid_loss 0.59547752 test_loss 0.56626987 \n",
      "大數據的設計思考 | 349 train_loss 0.53511745 valid_loss 0.59581202 test_loss 0.56675732 \n",
      "大數據的設計思考 | 350 train_loss 0.53540868 valid_loss 0.59709793 test_loss 0.56704068 \n",
      "大數據的設計思考 | 351 train_loss 0.53557527 valid_loss 0.59578156 test_loss 0.56722492 \n",
      "大數據的設計思考 | 352 train_loss 0.53515506 valid_loss 0.59584904 test_loss 0.56677294 \n",
      "大數據的設計思考 | 353 train_loss 0.53518045 valid_loss 0.59631193 test_loss 0.56657344 \n",
      "大數據的設計思考 | 354 train_loss 0.53803360 valid_loss 0.59786582 test_loss 0.57031506 \n",
      "大數據的設計思考 | 355 train_loss 0.53537077 valid_loss 0.59663832 test_loss 0.56662202 \n",
      "大數據的設計思考 | 356 train_loss 0.53529930 valid_loss 0.59545785 test_loss 0.56676733 \n",
      "大數據的設計思考 | 357 train_loss 0.53524643 valid_loss 0.59639877 test_loss 0.56708163 \n",
      "大數據的設計思考 | 358 train_loss 0.53743553 valid_loss 0.60075444 test_loss 0.56933290 \n",
      "大數據的設計思考 | 359 train_loss 0.53504515 valid_loss 0.59565401 test_loss 0.56619322 \n",
      "大數據的設計思考 | 360 train_loss 0.53593653 valid_loss 0.59783816 test_loss 0.56727386 \n",
      "大數據的設計思考 | 361 train_loss 0.53508407 valid_loss 0.59550828 test_loss 0.56617945 \n",
      "大數據的設計思考 | 362 train_loss 0.53538358 valid_loss 0.59702712 test_loss 0.56701809 \n",
      "大數據的設計思考 | 363 train_loss 0.53577405 valid_loss 0.59766412 test_loss 0.56721210 \n",
      "大數據的設計思考 | 364 train_loss 0.53694266 valid_loss 0.59614360 test_loss 0.56848109 \n",
      "大數據的設計思考 | 365 train_loss 0.53504372 valid_loss 0.59603471 test_loss 0.56650668 \n",
      "大數據的設計思考 | 366 train_loss 0.53692150 valid_loss 0.59602040 test_loss 0.56848300 \n",
      "大數據的設計思考 | 367 train_loss 0.53509516 valid_loss 0.59609580 test_loss 0.56675869 \n",
      "大數據的設計思考 | 368 train_loss 0.53704518 valid_loss 0.59619540 test_loss 0.56866866 \n",
      "大數據的設計思考 | 369 train_loss 0.53527045 valid_loss 0.59524500 test_loss 0.56665617 < save\n",
      "大數據的設計思考 | 370 train_loss 0.53501773 valid_loss 0.59606004 test_loss 0.56659979 \n",
      "大數據的設計思考 | 371 train_loss 0.53531122 valid_loss 0.59691870 test_loss 0.56687307 \n",
      "大數據的設計思考 | 372 train_loss 0.53507900 valid_loss 0.59625351 test_loss 0.56658822 \n",
      "大數據的設計思考 | 373 train_loss 0.53495955 valid_loss 0.59546375 test_loss 0.56634760 \n",
      "大數據的設計思考 | 374 train_loss 0.53702444 valid_loss 0.59596854 test_loss 0.56837881 \n",
      "大數據的設計思考 | 375 train_loss 0.53657854 valid_loss 0.59926671 test_loss 0.56829709 \n",
      "大數據的設計思考 | 376 train_loss 0.53524673 valid_loss 0.59515882 test_loss 0.56654209 < save\n",
      "大數據的設計思考 | 377 train_loss 0.53648204 valid_loss 0.59841174 test_loss 0.56767231 \n",
      "大數據的設計思考 | 378 train_loss 0.53901953 valid_loss 0.59774685 test_loss 0.57081026 \n",
      "大數據的設計思考 | 379 train_loss 0.53593141 valid_loss 0.59823078 test_loss 0.56787419 \n",
      "大數據的設計思考 | 380 train_loss 0.53519988 valid_loss 0.59561622 test_loss 0.56686807 \n",
      "大數據的設計思考 | 381 train_loss 0.53502309 valid_loss 0.59606767 test_loss 0.56654215 \n",
      "大數據的設計思考 | 382 train_loss 0.53644043 valid_loss 0.59883857 test_loss 0.56798166 \n",
      "大數據的設計思考 | 383 train_loss 0.53565454 valid_loss 0.59539241 test_loss 0.56726462 \n",
      "大數據的設計思考 | 384 train_loss 0.53553265 valid_loss 0.59511679 test_loss 0.56680053 < save\n",
      "大數據的設計思考 | 385 train_loss 0.53581345 valid_loss 0.59729582 test_loss 0.56687570 \n",
      "大數據的設計思考 | 386 train_loss 0.53546900 valid_loss 0.59515333 test_loss 0.56679136 \n",
      "大數據的設計思考 | 387 train_loss 0.53713822 valid_loss 0.59679919 test_loss 0.56915295 \n",
      "大數據的設計思考 | 388 train_loss 0.53806013 valid_loss 0.59680516 test_loss 0.56965214 \n",
      "大數據的設計思考 | 389 train_loss 0.53534091 valid_loss 0.59648073 test_loss 0.56646305 \n",
      "大數據的設計思考 | 390 train_loss 0.53502566 valid_loss 0.59610349 test_loss 0.56637156 \n",
      "大數據的設計思考 | 391 train_loss 0.53507119 valid_loss 0.59516740 test_loss 0.56651932 \n",
      "大數據的設計思考 | 392 train_loss 0.53589070 valid_loss 0.59499848 test_loss 0.56700444 < save\n",
      "大數據的設計思考 | 393 train_loss 0.53552353 valid_loss 0.59697312 test_loss 0.56672996 \n",
      "大數據的設計思考 | 394 train_loss 0.53566349 valid_loss 0.59592581 test_loss 0.56758547 \n",
      "大數據的設計思考 | 395 train_loss 0.53852099 valid_loss 0.60164946 test_loss 0.56996399 \n",
      "大數據的設計思考 | 396 train_loss 0.53533030 valid_loss 0.59503996 test_loss 0.56677407 \n",
      "大數據的設計思考 | 397 train_loss 0.53616643 valid_loss 0.59567779 test_loss 0.56793255 \n",
      "大數據的設計思考 | 398 train_loss 0.53495854 valid_loss 0.59595388 test_loss 0.56662863 \n",
      "大數據的設計思考 | 399 train_loss 0.53499675 valid_loss 0.59545124 test_loss 0.56657237 \n",
      "大數據的設計思考 | 400 train_loss 0.53601414 valid_loss 0.59505886 test_loss 0.56724477 \n",
      "大數據的設計思考 | 401 train_loss 0.53493333 valid_loss 0.59505934 test_loss 0.56619459 \n",
      "大數據的設計思考 | 402 train_loss 0.53507674 valid_loss 0.59516066 test_loss 0.56660938 \n",
      "大數據的設計思考 | 403 train_loss 0.53562355 valid_loss 0.59720123 test_loss 0.56697553 \n",
      "大數據的設計思考 | 404 train_loss 0.53496754 valid_loss 0.59588701 test_loss 0.56683272 \n",
      "大數據的設計思考 | 405 train_loss 0.53544056 valid_loss 0.59635788 test_loss 0.56641555 \n",
      "大數據的設計思考 | 406 train_loss 0.53512800 valid_loss 0.59544677 test_loss 0.56606072 \n",
      "大數據的設計思考 | 407 train_loss 0.53483975 valid_loss 0.59514421 test_loss 0.56615108 \n",
      "大數據的設計思考 | 408 train_loss 0.53667068 valid_loss 0.59873617 test_loss 0.56779844 \n",
      "大數據的設計思考 | 409 train_loss 0.53565985 valid_loss 0.59700161 test_loss 0.56675631 \n",
      "大數據的設計思考 | 410 train_loss 0.53864580 valid_loss 0.60191435 test_loss 0.57015157 \n",
      "大數據的設計思考 | 411 train_loss 0.53572738 valid_loss 0.59547317 test_loss 0.56737047 \n",
      "大數據的設計思考 | 412 train_loss 0.53489876 valid_loss 0.59554422 test_loss 0.56613433 \n",
      "大數據的設計思考 | 413 train_loss 0.53481305 valid_loss 0.59540111 test_loss 0.56631160 \n",
      "大數據的設計思考 | 414 train_loss 0.53509724 valid_loss 0.59505934 test_loss 0.56664962 \n",
      "大數據的設計思考 | 415 train_loss 0.53497130 valid_loss 0.59571391 test_loss 0.56607711 \n",
      "大數據的設計思考 | 416 train_loss 0.53480422 valid_loss 0.59515005 test_loss 0.56615323 \n",
      "大數據的設計思考 | 417 train_loss 0.53633428 valid_loss 0.59826815 test_loss 0.56756949 \n",
      "大數據的設計思考 | 418 train_loss 0.53526479 valid_loss 0.59506637 test_loss 0.56678259 \n",
      "大數據的設計思考 | 419 train_loss 0.53485483 valid_loss 0.59581488 test_loss 0.56633019 \n",
      "大數據的設計思考 | 420 train_loss 0.53498298 valid_loss 0.59518182 test_loss 0.56657821 \n",
      "大數據的設計思考 | 421 train_loss 0.53550786 valid_loss 0.59526891 test_loss 0.56715065 \n",
      "大數據的設計思考 | 422 train_loss 0.53602004 valid_loss 0.59496754 test_loss 0.56730872 < save\n",
      "大數據的設計思考 | 423 train_loss 0.53769320 valid_loss 0.59711039 test_loss 0.56990713 \n",
      "大數據的設計思考 | 424 train_loss 0.53561306 valid_loss 0.59733999 test_loss 0.56703162 \n",
      "大數據的設計思考 | 425 train_loss 0.53522974 valid_loss 0.59511119 test_loss 0.56687337 \n",
      "大數據的設計思考 | 426 train_loss 0.53497875 valid_loss 0.59502852 test_loss 0.56638306 \n",
      "大數據的設計思考 | 427 train_loss 0.53686559 valid_loss 0.59868336 test_loss 0.56782430 \n",
      "大數據的設計思考 | 428 train_loss 0.53580117 valid_loss 0.59507966 test_loss 0.56717396 \n",
      "大數據的設計思考 | 429 train_loss 0.53481549 valid_loss 0.59522337 test_loss 0.56612879 \n",
      "大數據的設計思考 | 430 train_loss 0.53753364 valid_loss 0.59591734 test_loss 0.56865621 \n",
      "大數據的設計思考 | 431 train_loss 0.53595901 valid_loss 0.59745246 test_loss 0.56701678 \n",
      "大數據的設計思考 | 432 train_loss 0.53497213 valid_loss 0.59622222 test_loss 0.56657070 \n",
      "大數據的設計思考 | 433 train_loss 0.53520674 valid_loss 0.59499389 test_loss 0.56670105 \n",
      "大數據的設計思考 | 434 train_loss 0.53574133 valid_loss 0.59498388 test_loss 0.56703788 \n",
      "大數據的設計思考 | 435 train_loss 0.53474981 valid_loss 0.59517759 test_loss 0.56600672 \n",
      "大數據的設計思考 | 436 train_loss 0.53501850 valid_loss 0.59482163 test_loss 0.56619000 < save\n",
      "大數據的設計思考 | 437 train_loss 0.53749299 valid_loss 0.59937596 test_loss 0.56838781 \n",
      "大數據的設計思考 | 438 train_loss 0.53504860 valid_loss 0.59493512 test_loss 0.56656420 \n",
      "大數據的設計思考 | 439 train_loss 0.53485435 valid_loss 0.59584737 test_loss 0.56659508 \n",
      "大數據的設計思考 | 440 train_loss 0.53571242 valid_loss 0.59571677 test_loss 0.56767273 \n",
      "大數據的設計思考 | 441 train_loss 0.53474826 valid_loss 0.59493268 test_loss 0.56604218 \n",
      "大數據的設計思考 | 442 train_loss 0.53499240 valid_loss 0.59481484 test_loss 0.56647301 < save\n",
      "大數據的設計思考 | 443 train_loss 0.53750134 valid_loss 0.60034794 test_loss 0.56908303 \n",
      "大數據的設計思考 | 444 train_loss 0.53605419 valid_loss 0.59526229 test_loss 0.56763250 \n",
      "大數據的設計思考 | 445 train_loss 0.53529686 valid_loss 0.59490782 test_loss 0.56692433 \n",
      "大數據的設計思考 | 446 train_loss 0.53494471 valid_loss 0.59476030 test_loss 0.56635535 < save\n",
      "大數據的設計思考 | 447 train_loss 0.53508556 valid_loss 0.59585774 test_loss 0.56702149 \n",
      "大數據的設計思考 | 448 train_loss 0.53666383 valid_loss 0.59907180 test_loss 0.56812310 \n",
      "大數據的設計思考 | 449 train_loss 0.53487629 valid_loss 0.59501845 test_loss 0.56648052 \n",
      "大數據的設計思考 | 450 train_loss 0.53543621 valid_loss 0.59467673 test_loss 0.56670034 < save\n",
      "大數據的設計思考 | 451 train_loss 0.53532368 valid_loss 0.59463984 test_loss 0.56614572 < save\n",
      "大數據的設計思考 | 452 train_loss 0.53689402 valid_loss 0.59617752 test_loss 0.56886667 \n",
      "大數據的設計思考 | 453 train_loss 0.53503716 valid_loss 0.59484333 test_loss 0.56645095 \n",
      "大數據的設計思考 | 454 train_loss 0.53467625 valid_loss 0.59502512 test_loss 0.56611615 \n",
      "大數據的設計思考 | 455 train_loss 0.53472984 valid_loss 0.59551603 test_loss 0.56642157 \n",
      "大數據的設計思考 | 456 train_loss 0.53504318 valid_loss 0.59511125 test_loss 0.56674093 \n",
      "大數據的設計思考 | 457 train_loss 0.53528434 valid_loss 0.59656388 test_loss 0.56649804 \n",
      "大數據的設計思考 | 458 train_loss 0.53517151 valid_loss 0.59491974 test_loss 0.56674802 \n",
      "大數據的設計思考 | 459 train_loss 0.53530365 valid_loss 0.59654254 test_loss 0.56656796 \n",
      "大數據的設計思考 | 460 train_loss 0.53511274 valid_loss 0.59489274 test_loss 0.56657898 \n",
      "大數據的設計思考 | 461 train_loss 0.53470433 valid_loss 0.59486455 test_loss 0.56593913 \n",
      "大數據的設計思考 | 462 train_loss 0.53497916 valid_loss 0.59480178 test_loss 0.56644219 \n",
      "大數據的設計思考 | 463 train_loss 0.53489876 valid_loss 0.59503067 test_loss 0.56655514 \n",
      "大數據的設計思考 | 464 train_loss 0.53471631 valid_loss 0.59548396 test_loss 0.56608993 \n",
      "大數據的設計思考 | 465 train_loss 0.53487766 valid_loss 0.59473658 test_loss 0.56583697 \n",
      "大數據的設計思考 | 466 train_loss 0.53889614 valid_loss 0.60243225 test_loss 0.57076591 \n",
      "大數據的設計思考 | 467 train_loss 0.53829157 valid_loss 0.59661889 test_loss 0.56986099 \n",
      "大數據的設計思考 | 468 train_loss 0.53533053 valid_loss 0.59675217 test_loss 0.56671047 \n",
      "大數據的設計思考 | 469 train_loss 0.53470099 valid_loss 0.59540063 test_loss 0.56619442 \n",
      "大數據的設計思考 | 470 train_loss 0.53562135 valid_loss 0.59751844 test_loss 0.56736016 \n",
      "大數據的設計思考 | 471 train_loss 0.53598791 valid_loss 0.59715760 test_loss 0.56691146 \n",
      "大數據的設計思考 | 472 train_loss 0.53465688 valid_loss 0.59471613 test_loss 0.56595153 \n",
      "大數據的設計思考 | 473 train_loss 0.53528792 valid_loss 0.59659559 test_loss 0.56655401 \n",
      "大數據的設計思考 | 474 train_loss 0.53511828 valid_loss 0.59471267 test_loss 0.56592441 \n",
      "大數據的設計思考 | 475 train_loss 0.53573948 valid_loss 0.59588236 test_loss 0.56779468 \n",
      "大數據的設計思考 | 476 train_loss 0.53460914 valid_loss 0.59489888 test_loss 0.56593657 \n",
      "大數據的設計思考 | 477 train_loss 0.53558242 valid_loss 0.59458226 test_loss 0.56677884 < save\n",
      "大數據的設計思考 | 478 train_loss 0.53506505 valid_loss 0.59461516 test_loss 0.56633222 \n",
      "大數據的設計思考 | 479 train_loss 0.53535199 valid_loss 0.59648007 test_loss 0.56649911 \n",
      "大數據的設計思考 | 480 train_loss 0.53519082 valid_loss 0.59495229 test_loss 0.56684560 \n",
      "大數據的設計思考 | 481 train_loss 0.53627211 valid_loss 0.59606832 test_loss 0.56839412 \n",
      "大數據的設計思考 | 482 train_loss 0.53460467 valid_loss 0.59512079 test_loss 0.56598198 \n",
      "大數據的設計思考 | 483 train_loss 0.53564209 valid_loss 0.59526545 test_loss 0.56738651 \n",
      "大數據的設計思考 | 484 train_loss 0.53575104 valid_loss 0.59473288 test_loss 0.56686884 \n",
      "大數據的設計思考 | 485 train_loss 0.53465337 valid_loss 0.59486133 test_loss 0.56619602 \n",
      "大數據的設計思考 | 486 train_loss 0.53512263 valid_loss 0.59632474 test_loss 0.56633848 \n",
      "大數據的設計思考 | 487 train_loss 0.53463525 valid_loss 0.59521240 test_loss 0.56602573 \n",
      "大數據的設計思考 | 488 train_loss 0.53715813 valid_loss 0.59997135 test_loss 0.56893641 \n",
      "大數據的設計思考 | 489 train_loss 0.53474057 valid_loss 0.59466517 test_loss 0.56624246 \n",
      "大數據的設計思考 | 490 train_loss 0.53474218 valid_loss 0.59458941 test_loss 0.56582379 \n",
      "大數據的設計思考 | 491 train_loss 0.53468430 valid_loss 0.59457284 test_loss 0.56603712 < save\n",
      "大數據的設計思考 | 492 train_loss 0.53581399 valid_loss 0.59492362 test_loss 0.56731784 \n",
      "大數據的設計思考 | 493 train_loss 0.53467399 valid_loss 0.59527254 test_loss 0.56635582 \n",
      "大數據的設計思考 | 494 train_loss 0.53821284 valid_loss 0.59685659 test_loss 0.57011545 \n",
      "大數據的設計思考 | 495 train_loss 0.53569609 valid_loss 0.59532005 test_loss 0.56759280 \n",
      "大數據的設計思考 | 496 train_loss 0.53488994 valid_loss 0.59544098 test_loss 0.56594539 \n",
      "大數據的設計思考 | 497 train_loss 0.53521121 valid_loss 0.59623235 test_loss 0.56639796 \n",
      "大數據的設計思考 | 498 train_loss 0.53561485 valid_loss 0.59485638 test_loss 0.56717372 \n",
      "大數據的設計思考 | 499 train_loss 0.53458619 valid_loss 0.59489357 test_loss 0.56592107 \n",
      "大數據的設計思考 | 500 train_loss 0.53493261 valid_loss 0.59638417 test_loss 0.56683010 \n",
      "大數據的設計思考 | 501 train_loss 0.53497887 valid_loss 0.59489346 test_loss 0.56673646 \n",
      "大數據的設計思考 | 502 train_loss 0.53466082 valid_loss 0.59509283 test_loss 0.56588519 \n",
      "大數據的設計思考 | 503 train_loss 0.53479862 valid_loss 0.59507048 test_loss 0.56585562 \n",
      "大數據的設計思考 | 504 train_loss 0.53590226 valid_loss 0.59476042 test_loss 0.56728876 \n",
      "大數據的設計思考 | 505 train_loss 0.53537673 valid_loss 0.59460938 test_loss 0.56672370 \n",
      "大數據的設計思考 | 506 train_loss 0.53524458 valid_loss 0.59462065 test_loss 0.56673795 \n",
      "大數據的設計思考 | 507 train_loss 0.53499335 valid_loss 0.59600222 test_loss 0.56631148 \n",
      "大數據的設計思考 | 508 train_loss 0.53478181 valid_loss 0.59434247 test_loss 0.56601346 < save\n",
      "大數據的設計思考 | 509 train_loss 0.53456044 valid_loss 0.59477067 test_loss 0.56599885 \n",
      "大數據的設計思考 | 510 train_loss 0.53751075 valid_loss 0.59592831 test_loss 0.56909096 \n",
      "大數據的設計思考 | 511 train_loss 0.53457856 valid_loss 0.59500116 test_loss 0.56621331 \n",
      "大數據的設計思考 | 512 train_loss 0.53462636 valid_loss 0.59470993 test_loss 0.56613910 \n",
      "大數據的設計思考 | 513 train_loss 0.53622949 valid_loss 0.59785855 test_loss 0.56735486 \n",
      "大數據的設計思考 | 514 train_loss 0.53465420 valid_loss 0.59550625 test_loss 0.56614816 \n",
      "大數據的設計思考 | 515 train_loss 0.53776777 valid_loss 0.60046166 test_loss 0.56918997 \n",
      "大數據的設計思考 | 516 train_loss 0.53604037 valid_loss 0.59822989 test_loss 0.56784421 \n",
      "大數據的設計思考 | 517 train_loss 0.53501630 valid_loss 0.59621024 test_loss 0.56643796 \n",
      "大數據的設計思考 | 518 train_loss 0.53514248 valid_loss 0.59431738 test_loss 0.56633854 < save\n",
      "大數據的設計思考 | 519 train_loss 0.53458154 valid_loss 0.59471983 test_loss 0.56620908 \n",
      "大數據的設計思考 | 520 train_loss 0.53887236 valid_loss 0.59735358 test_loss 0.57092166 \n",
      "大數據的設計思考 | 521 train_loss 0.53471416 valid_loss 0.59566897 test_loss 0.56634676 \n",
      "大數據的設計思考 | 522 train_loss 0.53645515 valid_loss 0.59523880 test_loss 0.56808078 \n",
      "大數據的設計思考 | 523 train_loss 0.53468180 valid_loss 0.59440821 test_loss 0.56596601 \n",
      "大數據的設計思考 | 524 train_loss 0.53466606 valid_loss 0.59514093 test_loss 0.56592363 \n",
      "大數據的設計思考 | 525 train_loss 0.53571153 valid_loss 0.59476721 test_loss 0.56720626 \n",
      "大數據的設計思考 | 526 train_loss 0.53482521 valid_loss 0.59454060 test_loss 0.56624371 \n",
      "大數據的設計思考 | 527 train_loss 0.53478408 valid_loss 0.59574991 test_loss 0.56618232 \n",
      "大數據的設計思考 | 528 train_loss 0.53666836 valid_loss 0.59900820 test_loss 0.56818604 \n",
      "大數據的設計思考 | 529 train_loss 0.53470391 valid_loss 0.59561598 test_loss 0.56618345 \n",
      "大數據的設計思考 | 530 train_loss 0.53481704 valid_loss 0.59476697 test_loss 0.56650901 \n",
      "大數據的設計思考 | 531 train_loss 0.53589046 valid_loss 0.59740615 test_loss 0.56715888 \n",
      "大數據的設計思考 | 532 train_loss 0.53589100 valid_loss 0.59721422 test_loss 0.56696254 \n",
      "大數據的設計思考 | 533 train_loss 0.53502345 valid_loss 0.59434646 test_loss 0.56637645 \n",
      "大數據的設計思考 | 534 train_loss 0.53470641 valid_loss 0.59578592 test_loss 0.56664634 \n",
      "大數據的設計思考 | 535 train_loss 0.53668672 valid_loss 0.59546250 test_loss 0.56846881 \n",
      "大數據的設計思考 | 536 train_loss 0.53482109 valid_loss 0.59462768 test_loss 0.56651300 \n",
      "大數據的設計思考 | 537 train_loss 0.53524196 valid_loss 0.59699053 test_loss 0.56706548 \n",
      "大數據的設計思考 | 538 train_loss 0.53571576 valid_loss 0.59710449 test_loss 0.56692350 \n",
      "大數據的設計思考 | 539 train_loss 0.54060656 valid_loss 0.59805739 test_loss 0.57224375 \n",
      "大數據的設計思考 | 540 train_loss 0.53479999 valid_loss 0.59523052 test_loss 0.56674653 \n",
      "大數據的設計思考 | 541 train_loss 0.53502244 valid_loss 0.59428048 test_loss 0.56641084 < save\n",
      "大數據的設計思考 | 542 train_loss 0.53469902 valid_loss 0.59435320 test_loss 0.56615490 \n",
      "大數據的設計思考 | 543 train_loss 0.53820246 valid_loss 0.59654266 test_loss 0.57004112 \n",
      "大數據的設計思考 | 544 train_loss 0.53477532 valid_loss 0.59453493 test_loss 0.56627125 \n",
      "大數據的設計思考 | 545 train_loss 0.53629708 valid_loss 0.59832621 test_loss 0.56773221 \n",
      "大數據的設計思考 | 546 train_loss 0.53631061 valid_loss 0.59814441 test_loss 0.56759757 \n",
      "大數據的設計思考 | 547 train_loss 0.53593618 valid_loss 0.59755313 test_loss 0.56714725 \n",
      "大數據的設計思考 | 548 train_loss 0.53477263 valid_loss 0.59426284 test_loss 0.56609958 < save\n",
      "大數據的設計思考 | 549 train_loss 0.53492445 valid_loss 0.59419560 test_loss 0.56625712 < save\n",
      "大數據的設計思考 | 550 train_loss 0.53450745 valid_loss 0.59469771 test_loss 0.56599504 \n",
      "大數據的設計思考 | 551 train_loss 0.53509176 valid_loss 0.59460527 test_loss 0.56672680 \n",
      "大數據的設計思考 | 552 train_loss 0.53479558 valid_loss 0.59495902 test_loss 0.56659281 \n",
      "大數據的設計思考 | 553 train_loss 0.53520972 valid_loss 0.59469348 test_loss 0.56696701 \n",
      "大數據的設計思考 | 554 train_loss 0.53509921 valid_loss 0.59600484 test_loss 0.56632650 \n",
      "大數據的設計思考 | 555 train_loss 0.53812820 valid_loss 0.60095829 test_loss 0.56961960 \n",
      "大數據的設計思考 | 556 train_loss 0.53465420 valid_loss 0.59533942 test_loss 0.56597078 \n",
      "大數據的設計思考 | 557 train_loss 0.53448302 valid_loss 0.59487557 test_loss 0.56605458 \n",
      "大數據的設計思考 | 558 train_loss 0.53516877 valid_loss 0.59425241 test_loss 0.56644523 \n",
      "大數據的設計思考 | 559 train_loss 0.53468424 valid_loss 0.59564453 test_loss 0.56615061 \n",
      "大數據的設計思考 | 560 train_loss 0.53491139 valid_loss 0.59417927 test_loss 0.56602633 < save\n",
      "大數據的設計思考 | 561 train_loss 0.53567302 valid_loss 0.59730810 test_loss 0.56711268 \n",
      "大數據的設計思考 | 562 train_loss 0.53683805 valid_loss 0.59950590 test_loss 0.56851095 \n",
      "大數據的設計思考 | 563 train_loss 0.53778815 valid_loss 0.60022014 test_loss 0.56893533 \n",
      "大數據的設計思考 | 564 train_loss 0.53489387 valid_loss 0.59631109 test_loss 0.56676197 \n",
      "大數據的設計思考 | 565 train_loss 0.53642082 valid_loss 0.59844899 test_loss 0.56773025 \n",
      "大數據的設計思考 | 566 train_loss 0.53683585 valid_loss 0.59505022 test_loss 0.56802464 \n",
      "大數據的設計思考 | 567 train_loss 0.53452361 valid_loss 0.59434944 test_loss 0.56588203 \n",
      "大數據的設計思考 | 568 train_loss 0.53451443 valid_loss 0.59519684 test_loss 0.56589860 \n",
      "大數據的設計思考 | 569 train_loss 0.53492242 valid_loss 0.59608126 test_loss 0.56632859 \n",
      "大數據的設計思考 | 570 train_loss 0.53493249 valid_loss 0.59434587 test_loss 0.56642365 \n",
      "大數據的設計思考 | 571 train_loss 0.53450227 valid_loss 0.59508944 test_loss 0.56601781 \n",
      "大數據的設計思考 | 572 train_loss 0.53512937 valid_loss 0.59409142 test_loss 0.56625891 < save\n",
      "大數據的設計思考 | 573 train_loss 0.53907502 valid_loss 0.59715277 test_loss 0.57097596 \n",
      "大數據的設計思考 | 574 train_loss 0.53458852 valid_loss 0.59422362 test_loss 0.56592572 \n",
      "大數據的設計思考 | 575 train_loss 0.53444713 valid_loss 0.59507179 test_loss 0.56600261 \n",
      "大數據的設計思考 | 576 train_loss 0.53444815 valid_loss 0.59434092 test_loss 0.56572294 \n",
      "大數據的設計思考 | 577 train_loss 0.53476775 valid_loss 0.59464502 test_loss 0.56647259 \n",
      "大數據的設計思考 | 578 train_loss 0.53599381 valid_loss 0.59761482 test_loss 0.56713456 \n",
      "大數據的設計思考 | 579 train_loss 0.53550810 valid_loss 0.59714800 test_loss 0.56705368 \n",
      "大數據的設計思考 | 580 train_loss 0.53459626 valid_loss 0.59457779 test_loss 0.56627011 \n",
      "大數據的設計思考 | 581 train_loss 0.53722107 valid_loss 0.59920406 test_loss 0.56831878 \n",
      "大數據的設計思考 | 582 train_loss 0.53545558 valid_loss 0.59613109 test_loss 0.56625450 \n",
      "大數據的設計思考 | 583 train_loss 0.53537619 valid_loss 0.59474069 test_loss 0.56708109 \n",
      "大數據的設計思考 | 584 train_loss 0.53460628 valid_loss 0.59555793 test_loss 0.56625938 \n",
      "大數據的設計思考 | 585 train_loss 0.53443468 valid_loss 0.59473926 test_loss 0.56606472 \n",
      "大數據的設計思考 | 586 train_loss 0.53455132 valid_loss 0.59447539 test_loss 0.56601214 \n",
      "大數據的設計思考 | 587 train_loss 0.53463531 valid_loss 0.59564787 test_loss 0.56642681 \n",
      "大數據的設計思考 | 588 train_loss 0.53620243 valid_loss 0.59523839 test_loss 0.56793517 \n",
      "大數據的設計思考 | 589 train_loss 0.53467023 valid_loss 0.59439969 test_loss 0.56609488 \n",
      "大數據的設計思考 | 590 train_loss 0.53603774 valid_loss 0.59788138 test_loss 0.56748664 \n",
      "大數據的設計思考 | 591 train_loss 0.53439826 valid_loss 0.59462547 test_loss 0.56576669 \n",
      "大數據的設計思考 | 592 train_loss 0.53519958 valid_loss 0.59641337 test_loss 0.56644863 \n",
      "大數據的設計思考 | 593 train_loss 0.53510952 valid_loss 0.59613961 test_loss 0.56629467 \n",
      "大數據的設計思考 | 594 train_loss 0.53654963 valid_loss 0.59489572 test_loss 0.56802869 \n",
      "大數據的設計思考 | 595 train_loss 0.53705776 valid_loss 0.59604752 test_loss 0.56903809 \n",
      "大數據的設計思考 | 596 train_loss 0.53447020 valid_loss 0.59473640 test_loss 0.56570327 \n",
      "大數據的設計思考 | 597 train_loss 0.53482318 valid_loss 0.59412527 test_loss 0.56596273 \n",
      "大數據的設計思考 | 598 train_loss 0.53481168 valid_loss 0.59455019 test_loss 0.56652999 \n",
      "大數據的設計思考 | 599 train_loss 0.53440428 valid_loss 0.59463710 test_loss 0.56578660 \n",
      "大數據的設計思考 | 600 train_loss 0.53512949 valid_loss 0.59627348 test_loss 0.56642842 \n",
      "大數據的設計思考 | 601 train_loss 0.53446627 valid_loss 0.59490365 test_loss 0.56609613 \n",
      "大數據的設計思考 | 602 train_loss 0.53455448 valid_loss 0.59515893 test_loss 0.56626964 \n",
      "大數據的設計思考 | 603 train_loss 0.53454608 valid_loss 0.59414464 test_loss 0.56591308 \n",
      "大數據的設計思考 | 604 train_loss 0.53501749 valid_loss 0.59453934 test_loss 0.56671631 \n",
      "大數據的設計思考 | 605 train_loss 0.53519946 valid_loss 0.59432447 test_loss 0.56671464 \n",
      "大數據的設計思考 | 606 train_loss 0.53508437 valid_loss 0.59429479 test_loss 0.56654024 \n",
      "大數據的設計思考 | 607 train_loss 0.53490114 valid_loss 0.59425181 test_loss 0.56631112 \n",
      "大數據的設計思考 | 608 train_loss 0.53488183 valid_loss 0.59506238 test_loss 0.56573540 \n",
      "大數據的設計思考 | 609 train_loss 0.53445631 valid_loss 0.59443903 test_loss 0.56571752 \n",
      "大數據的設計思考 | 610 train_loss 0.53537679 valid_loss 0.59436303 test_loss 0.56679076 \n",
      "大數據的設計思考 | 611 train_loss 0.53619242 valid_loss 0.59504622 test_loss 0.56788796 \n",
      "大數據的設計思考 | 612 train_loss 0.53536403 valid_loss 0.59487689 test_loss 0.56730241 \n",
      "大數據的設計思考 | 613 train_loss 0.53444326 valid_loss 0.59453040 test_loss 0.56599879 \n",
      "大數據的設計思考 | 614 train_loss 0.53536707 valid_loss 0.59671742 test_loss 0.56667012 \n",
      "大數據的設計思考 | 615 train_loss 0.53449559 valid_loss 0.59489262 test_loss 0.56583351 \n",
      "大數據的設計思考 | 616 train_loss 0.53624135 valid_loss 0.59857059 test_loss 0.56810945 \n",
      "大數據的設計思考 | 617 train_loss 0.53505117 valid_loss 0.59640080 test_loss 0.56642777 \n",
      "大數據的設計思考 | 618 train_loss 0.53466791 valid_loss 0.59579813 test_loss 0.56626189 \n",
      "大數據的設計思考 | 619 train_loss 0.53500390 valid_loss 0.59429741 test_loss 0.56649810 \n",
      "大數據的設計思考 | 620 train_loss 0.53695118 valid_loss 0.59907454 test_loss 0.56828475 \n",
      "大數據的設計思考 | 621 train_loss 0.53540844 valid_loss 0.59697551 test_loss 0.56685662 \n",
      "大數據的設計思考 | 622 train_loss 0.53614122 valid_loss 0.59457511 test_loss 0.56737763 \n",
      "大數據的設計思考 | 623 train_loss 0.53556323 valid_loss 0.59487087 test_loss 0.56740242 \n",
      "大數據的設計思考 | 624 train_loss 0.53456247 valid_loss 0.59539157 test_loss 0.56645411 \n",
      "大數據的設計思考 | 625 train_loss 0.54167747 valid_loss 0.60558724 test_loss 0.57310122 \n",
      "大數據的設計思考 | 626 train_loss 0.53444570 valid_loss 0.59512204 test_loss 0.56600827 \n",
      "大數據的設計思考 | 627 train_loss 0.53477931 valid_loss 0.59589845 test_loss 0.56626284 \n",
      "大數據的設計思考 | 628 train_loss 0.53745133 valid_loss 0.59586477 test_loss 0.56925774 \n",
      "大數據的設計思考 | 629 train_loss 0.53475726 valid_loss 0.59532005 test_loss 0.56587172 \n",
      "大數據的設計思考 | 630 train_loss 0.53474855 valid_loss 0.59433490 test_loss 0.56636328 \n",
      "大數據的設計思考 | 631 train_loss 0.53450292 valid_loss 0.59431583 test_loss 0.56599063 \n",
      "大數據的設計思考 | 632 train_loss 0.53444684 valid_loss 0.59512800 test_loss 0.56616026 \n",
      "大數據的設計思考 | 633 train_loss 0.53590018 valid_loss 0.59460509 test_loss 0.56745762 \n",
      "大數據的設計思考 | 634 train_loss 0.53584182 valid_loss 0.59475392 test_loss 0.56759149 \n",
      "大數據的設計思考 | 635 train_loss 0.53563398 valid_loss 0.59725308 test_loss 0.56707156 \n",
      "大數據的設計思考 | 636 train_loss 0.53432524 valid_loss 0.59452564 test_loss 0.56572390 \n",
      "大數據的設計思考 | 637 train_loss 0.53491205 valid_loss 0.59585112 test_loss 0.56620735 \n",
      "大數據的設計思考 | 638 train_loss 0.53499192 valid_loss 0.59579086 test_loss 0.56613541 \n",
      "大數據的設計思考 | 639 train_loss 0.53439420 valid_loss 0.59479421 test_loss 0.56600606 \n",
      "大數據的設計思考 | 640 train_loss 0.53459948 valid_loss 0.59469146 test_loss 0.56638843 \n",
      "大數據的設計思考 | 641 train_loss 0.53510958 valid_loss 0.59600419 test_loss 0.56618911 \n",
      "大數據的設計思考 | 642 train_loss 0.53577226 valid_loss 0.59755796 test_loss 0.56730694 \n",
      "大數據的設計思考 | 643 train_loss 0.53479332 valid_loss 0.59611905 test_loss 0.56658012 \n",
      "大數據的設計思考 | 644 train_loss 0.53483772 valid_loss 0.59390801 test_loss 0.56594628 < save\n",
      "大數據的設計思考 | 645 train_loss 0.53779048 valid_loss 0.59588140 test_loss 0.56950164 \n",
      "大數據的設計思考 | 646 train_loss 0.53473347 valid_loss 0.59493083 test_loss 0.56564242 \n",
      "大數據的設計思考 | 647 train_loss 0.53523594 valid_loss 0.59677577 test_loss 0.56685084 \n",
      "大數據的設計思考 | 648 train_loss 0.53436637 valid_loss 0.59436399 test_loss 0.56586170 \n",
      "大數據的設計思考 | 649 train_loss 0.53553730 valid_loss 0.59708327 test_loss 0.56694502 \n",
      "大數據的設計思考 | 650 train_loss 0.53525710 valid_loss 0.59397399 test_loss 0.56650907 \n",
      "大數據的設計思考 | 651 train_loss 0.53575498 valid_loss 0.59726149 test_loss 0.56699049 \n",
      "大數據的設計思考 | 652 train_loss 0.53435886 valid_loss 0.59422606 test_loss 0.56570697 \n",
      "大數據的設計思考 | 653 train_loss 0.53430194 valid_loss 0.59444064 test_loss 0.56582904 \n",
      "大數據的設計思考 | 654 train_loss 0.53772795 valid_loss 0.60002983 test_loss 0.56891143 \n",
      "大數據的設計思考 | 655 train_loss 0.53519845 valid_loss 0.59658617 test_loss 0.56659210 \n",
      "大數據的設計思考 | 656 train_loss 0.53468287 valid_loss 0.59404862 test_loss 0.56614131 \n",
      "大數據的設計思考 | 657 train_loss 0.53450722 valid_loss 0.59448761 test_loss 0.56623083 \n",
      "大數據的設計思考 | 658 train_loss 0.53451699 valid_loss 0.59480935 test_loss 0.56637323 \n",
      "大數據的設計思考 | 659 train_loss 0.53471589 valid_loss 0.59385210 test_loss 0.56595308 < save\n",
      "大數據的設計思考 | 660 train_loss 0.53511310 valid_loss 0.59440708 test_loss 0.56683928 \n",
      "大數據的設計思考 | 661 train_loss 0.53459549 valid_loss 0.59448147 test_loss 0.56637299 \n",
      "大數據的設計思考 | 662 train_loss 0.53607750 valid_loss 0.59519893 test_loss 0.56797493 \n",
      "大數據的設計思考 | 663 train_loss 0.53541607 valid_loss 0.59686279 test_loss 0.56677979 \n",
      "大數據的設計思考 | 664 train_loss 0.53471059 valid_loss 0.59494972 test_loss 0.56559962 \n",
      "大數據的設計思考 | 665 train_loss 0.53481346 valid_loss 0.59403402 test_loss 0.56615233 \n",
      "大數據的設計思考 | 666 train_loss 0.53456539 valid_loss 0.59526926 test_loss 0.56589127 \n",
      "大數據的設計思考 | 667 train_loss 0.53434092 valid_loss 0.59461701 test_loss 0.56589979 \n",
      "大數據的設計思考 | 668 train_loss 0.53432554 valid_loss 0.59423268 test_loss 0.56558216 \n",
      "大數據的設計思考 | 669 train_loss 0.53500909 valid_loss 0.59627342 test_loss 0.56650066 \n",
      "大數據的設計思考 | 670 train_loss 0.53429800 valid_loss 0.59443581 test_loss 0.56555867 \n",
      "大數據的設計思考 | 671 train_loss 0.53892052 valid_loss 0.60211277 test_loss 0.57059830 \n",
      "大數據的設計思考 | 672 train_loss 0.53439933 valid_loss 0.59439719 test_loss 0.56582832 \n",
      "大數據的設計思考 | 673 train_loss 0.53443742 valid_loss 0.59422743 test_loss 0.56550539 \n",
      "大數據的設計思考 | 674 train_loss 0.53486913 valid_loss 0.59469616 test_loss 0.56569010 \n",
      "大數據的設計思考 | 675 train_loss 0.53457385 valid_loss 0.59566164 test_loss 0.56650209 \n",
      "大數據的設計思考 | 676 train_loss 0.53474051 valid_loss 0.59475780 test_loss 0.56662667 \n",
      "大數據的設計思考 | 677 train_loss 0.53432196 valid_loss 0.59460235 test_loss 0.56573546 \n",
      "大數據的設計思考 | 678 train_loss 0.53502965 valid_loss 0.59603518 test_loss 0.56630123 \n",
      "大數據的設計思考 | 679 train_loss 0.53437155 valid_loss 0.59494174 test_loss 0.56600964 \n",
      "大數據的設計思考 | 680 train_loss 0.53506052 valid_loss 0.59411293 test_loss 0.56642753 \n",
      "大數據的設計思考 | 681 train_loss 0.53431326 valid_loss 0.59415269 test_loss 0.56575078 \n",
      "大數據的設計思考 | 682 train_loss 0.53548753 valid_loss 0.59655106 test_loss 0.56660134 \n",
      "大數據的設計思考 | 683 train_loss 0.53446662 valid_loss 0.59527218 test_loss 0.56595182 \n",
      "大數據的設計思考 | 684 train_loss 0.53535092 valid_loss 0.59682846 test_loss 0.56678730 \n",
      "大數據的設計思考 | 685 train_loss 0.53454441 valid_loss 0.59399027 test_loss 0.56579924 \n",
      "大數據的設計思考 | 686 train_loss 0.53447449 valid_loss 0.59539396 test_loss 0.56613576 \n",
      "大數據的設計思考 | 687 train_loss 0.53431708 valid_loss 0.59473258 test_loss 0.56581390 \n",
      "大數據的設計思考 | 688 train_loss 0.53445834 valid_loss 0.59415340 test_loss 0.56597733 \n",
      "大數據的設計思考 | 689 train_loss 0.53445864 valid_loss 0.59487838 test_loss 0.56632036 \n",
      "大數據的設計思考 | 690 train_loss 0.53436172 valid_loss 0.59438074 test_loss 0.56565773 \n",
      "大數據的設計思考 | 691 train_loss 0.53450912 valid_loss 0.59494460 test_loss 0.56639194 \n",
      "大數據的設計思考 | 692 train_loss 0.53568095 valid_loss 0.59444916 test_loss 0.56720251 \n",
      "大數據的設計思考 | 693 train_loss 0.53463507 valid_loss 0.59539545 test_loss 0.56596822 \n",
      "大數據的設計思考 | 694 train_loss 0.53526968 valid_loss 0.59683484 test_loss 0.56691307 \n",
      "大數據的設計思考 | 695 train_loss 0.53440207 valid_loss 0.59399551 test_loss 0.56570590 \n",
      "大數據的設計思考 | 696 train_loss 0.53430623 valid_loss 0.59429479 test_loss 0.56560224 \n",
      "大數據的設計思考 | 697 train_loss 0.53429180 valid_loss 0.59423536 test_loss 0.56557441 \n",
      "大數據的設計思考 | 698 train_loss 0.53501815 valid_loss 0.59516519 test_loss 0.56591684 \n",
      "大數據的設計思考 | 699 train_loss 0.53461224 valid_loss 0.59421515 test_loss 0.56636733 \n",
      "大數據的設計思考 | 700 train_loss 0.53491408 valid_loss 0.59396321 test_loss 0.56631404 \n",
      "大數據的設計思考 | 701 train_loss 0.53590953 valid_loss 0.59484684 test_loss 0.56770951 \n",
      "大數據的設計思考 | 702 train_loss 0.53432608 valid_loss 0.59473640 test_loss 0.56576681 \n",
      "大數據的設計思考 | 703 train_loss 0.54015994 valid_loss 0.59824675 test_loss 0.57233161 \n",
      "大數據的設計思考 | 704 train_loss 0.53531581 valid_loss 0.59484577 test_loss 0.56725049 \n",
      "大數據的設計思考 | 705 train_loss 0.53479654 valid_loss 0.59489375 test_loss 0.56672251 \n",
      "大數據的設計思考 | 706 train_loss 0.53426647 valid_loss 0.59419465 test_loss 0.56571651 \n",
      "大數據的設計思考 | 707 train_loss 0.53472352 valid_loss 0.59595299 test_loss 0.56629634 \n",
      "大數據的設計思考 | 708 train_loss 0.53459907 valid_loss 0.59379649 test_loss 0.56570947 < save\n",
      "大數據的設計思考 | 709 train_loss 0.53523290 valid_loss 0.59393352 test_loss 0.56653851 \n",
      "大數據的設計思考 | 710 train_loss 0.53432012 valid_loss 0.59444660 test_loss 0.56579119 \n",
      "大數據的設計思考 | 711 train_loss 0.53439748 valid_loss 0.59501141 test_loss 0.56593865 \n",
      "大數據的設計思考 | 712 train_loss 0.53455168 valid_loss 0.59438610 test_loss 0.56546658 \n",
      "大數據的設計思考 | 713 train_loss 0.53441715 valid_loss 0.59419483 test_loss 0.56597435 \n",
      "大數據的設計思考 | 714 train_loss 0.53535831 valid_loss 0.59421808 test_loss 0.56690520 \n",
      "大數據的設計思考 | 715 train_loss 0.53434587 valid_loss 0.59446341 test_loss 0.56606895 \n",
      "大數據的設計思考 | 716 train_loss 0.53440672 valid_loss 0.59382647 test_loss 0.56565601 \n",
      "大數據的設計思考 | 717 train_loss 0.53454059 valid_loss 0.59513420 test_loss 0.56588656 \n",
      "大數據的設計思考 | 718 train_loss 0.53439540 valid_loss 0.59406316 test_loss 0.56559813 \n",
      "大數據的設計思考 | 719 train_loss 0.53430295 valid_loss 0.59441453 test_loss 0.56558353 \n",
      "大數據的設計思考 | 720 train_loss 0.53440481 valid_loss 0.59430212 test_loss 0.56609780 \n",
      "大數據的設計思考 | 721 train_loss 0.53512734 valid_loss 0.59586012 test_loss 0.56616569 \n",
      "大數據的設計思考 | 722 train_loss 0.53435373 valid_loss 0.59469658 test_loss 0.56562102 \n",
      "大數據的設計思考 | 723 train_loss 0.53445661 valid_loss 0.59495533 test_loss 0.56580317 \n",
      "大數據的設計思考 | 724 train_loss 0.53435469 valid_loss 0.59494036 test_loss 0.56607938 \n",
      "大數據的設計思考 | 725 train_loss 0.53792101 valid_loss 0.59625846 test_loss 0.56991798 \n",
      "大數據的設計思考 | 726 train_loss 0.53555954 valid_loss 0.59669632 test_loss 0.56666172 \n",
      "大數據的設計思考 | 727 train_loss 0.53611338 valid_loss 0.59443045 test_loss 0.56749600 \n",
      "大數據的設計思考 | 728 train_loss 0.53441042 valid_loss 0.59386414 test_loss 0.56587124 \n",
      "大數據的設計思考 | 729 train_loss 0.53509730 valid_loss 0.59401822 test_loss 0.56660867 \n",
      "大數據的設計思考 | 730 train_loss 0.53429848 valid_loss 0.59421444 test_loss 0.56565684 \n",
      "大數據的設計思考 | 731 train_loss 0.53489107 valid_loss 0.59398431 test_loss 0.56642991 \n",
      "大數據的設計思考 | 732 train_loss 0.53444231 valid_loss 0.59389013 test_loss 0.56568003 \n",
      "大數據的設計思考 | 733 train_loss 0.53481394 valid_loss 0.59580481 test_loss 0.56627089 \n",
      "大數據的設計思考 | 734 train_loss 0.53438950 valid_loss 0.59511739 test_loss 0.56600457 \n",
      "大數據的設計思考 | 735 train_loss 0.53425729 valid_loss 0.59423947 test_loss 0.56587237 \n",
      "大數據的設計思考 | 736 train_loss 0.53457391 valid_loss 0.59426725 test_loss 0.56630164 \n",
      "大數據的設計思考 | 737 train_loss 0.53493708 valid_loss 0.59596181 test_loss 0.56632400 \n",
      "大數據的設計思考 | 738 train_loss 0.53474003 valid_loss 0.59375286 test_loss 0.56595987 < save\n",
      "大數據的設計思考 | 739 train_loss 0.53993446 valid_loss 0.60314381 test_loss 0.57142395 \n",
      "大數據的設計思考 | 740 train_loss 0.53487134 valid_loss 0.59613407 test_loss 0.56649214 \n",
      "大數據的設計思考 | 741 train_loss 0.53472143 valid_loss 0.59422410 test_loss 0.56644201 \n",
      "大數據的設計思考 | 742 train_loss 0.53660959 valid_loss 0.59840310 test_loss 0.56775206 \n",
      "大數據的設計思考 | 743 train_loss 0.53485423 valid_loss 0.59435964 test_loss 0.56659907 \n",
      "大數據的設計思考 | 744 train_loss 0.53477508 valid_loss 0.59576005 test_loss 0.56617188 \n",
      "大數據的設計思考 | 745 train_loss 0.53603005 valid_loss 0.59486258 test_loss 0.56791759 \n",
      "大數據的設計思考 | 746 train_loss 0.53467953 valid_loss 0.59568936 test_loss 0.56618392 \n",
      "大數據的設計思考 | 747 train_loss 0.53423047 valid_loss 0.59411258 test_loss 0.56568140 \n",
      "大數據的設計思考 | 748 train_loss 0.53432149 valid_loss 0.59401953 test_loss 0.56569761 \n",
      "大數據的設計思考 | 749 train_loss 0.53487724 valid_loss 0.59376734 test_loss 0.56600714 \n",
      "大數據的設計思考 | 750 train_loss 0.53664631 valid_loss 0.59864604 test_loss 0.56798428 \n",
      "大數據的設計思考 | 751 train_loss 0.53530210 valid_loss 0.59662783 test_loss 0.56668919 \n",
      "大數據的設計思考 | 752 train_loss 0.53463012 valid_loss 0.59362483 test_loss 0.56577533 < save\n",
      "大數據的設計思考 | 753 train_loss 0.53591275 valid_loss 0.59754217 test_loss 0.56718141 \n",
      "大數據的設計思考 | 754 train_loss 0.53437501 valid_loss 0.59440720 test_loss 0.56618041 \n",
      "大數據的設計思考 | 755 train_loss 0.53440553 valid_loss 0.59474772 test_loss 0.56569946 \n",
      "大數據的設計思考 | 756 train_loss 0.53436917 valid_loss 0.59489304 test_loss 0.56585377 \n",
      "大數據的設計思考 | 757 train_loss 0.53487551 valid_loss 0.59409457 test_loss 0.56638992 \n",
      "大數據的設計思考 | 758 train_loss 0.53438419 valid_loss 0.59388661 test_loss 0.56552082 \n",
      "大數據的設計思考 | 759 train_loss 0.53425902 valid_loss 0.59440464 test_loss 0.56562334 \n",
      "大數據的設計思考 | 760 train_loss 0.53438270 valid_loss 0.59393078 test_loss 0.56544560 \n",
      "大數據的設計思考 | 761 train_loss 0.53536284 valid_loss 0.59416437 test_loss 0.56696260 \n",
      "大數據的設計思考 | 762 train_loss 0.53440487 valid_loss 0.59516454 test_loss 0.56604248 \n",
      "大數據的設計思考 | 763 train_loss 0.53935623 valid_loss 0.59692305 test_loss 0.57101792 \n",
      "大數據的設計思考 | 764 train_loss 0.53428972 valid_loss 0.59420055 test_loss 0.56551015 \n",
      "大數據的設計思考 | 765 train_loss 0.53913581 valid_loss 0.60239202 test_loss 0.57086796 \n",
      "大數據的設計思考 | 766 train_loss 0.53435272 valid_loss 0.59427142 test_loss 0.56606567 \n",
      "大數據的設計思考 | 767 train_loss 0.53421462 valid_loss 0.59427625 test_loss 0.56569541 \n",
      "大數據的設計思考 | 768 train_loss 0.53568894 valid_loss 0.59710073 test_loss 0.56700212 \n",
      "大數據的設計思考 | 769 train_loss 0.53431952 valid_loss 0.59450668 test_loss 0.56567556 \n",
      "大數據的設計思考 | 770 train_loss 0.53443301 valid_loss 0.59487182 test_loss 0.56578958 \n",
      "大數據的設計思考 | 771 train_loss 0.53688103 valid_loss 0.59894747 test_loss 0.56828809 \n",
      "大數據的設計思考 | 772 train_loss 0.53420609 valid_loss 0.59417135 test_loss 0.56565690 \n",
      "大數據的設計思考 | 773 train_loss 0.53504431 valid_loss 0.59621388 test_loss 0.56651300 \n",
      "大數據的設計思考 | 774 train_loss 0.53595775 valid_loss 0.59456313 test_loss 0.56759566 \n",
      "大數據的設計思考 | 775 train_loss 0.53418595 valid_loss 0.59404230 test_loss 0.56561184 \n",
      "大數據的設計思考 | 776 train_loss 0.53467393 valid_loss 0.59526175 test_loss 0.56589240 \n",
      "大數據的設計思考 | 777 train_loss 0.53468978 valid_loss 0.59391153 test_loss 0.56583059 \n",
      "大數據的設計思考 | 778 train_loss 0.53425121 valid_loss 0.59390414 test_loss 0.56572062 \n",
      "大數據的設計思考 | 779 train_loss 0.53446907 valid_loss 0.59463114 test_loss 0.56563342 \n",
      "大數據的設計思考 | 780 train_loss 0.53489292 valid_loss 0.59500766 test_loss 0.56573540 \n",
      "大數據的設計思考 | 781 train_loss 0.53429323 valid_loss 0.59406799 test_loss 0.56584185 \n",
      "大數據的設計思考 | 782 train_loss 0.53422034 valid_loss 0.59438336 test_loss 0.56565529 \n",
      "大數據的設計思考 | 783 train_loss 0.53423554 valid_loss 0.59412223 test_loss 0.56574649 \n",
      "大數據的設計思考 | 784 train_loss 0.53423095 valid_loss 0.59431875 test_loss 0.56560463 \n",
      "大數據的設計思考 | 785 train_loss 0.53465146 valid_loss 0.59359443 test_loss 0.56563729 < save\n",
      "大數據的設計思考 | 786 train_loss 0.53422260 valid_loss 0.59437037 test_loss 0.56577122 \n",
      "大數據的設計思考 | 787 train_loss 0.53490567 valid_loss 0.59401965 test_loss 0.56652236 \n",
      "大數據的設計思考 | 788 train_loss 0.53425813 valid_loss 0.59455156 test_loss 0.56574750 \n",
      "大數據的設計思考 | 789 train_loss 0.53433788 valid_loss 0.59474188 test_loss 0.56604981 \n",
      "大數據的設計思考 | 790 train_loss 0.53551382 valid_loss 0.59419614 test_loss 0.56695104 \n",
      "大數據的設計思考 | 791 train_loss 0.53729951 valid_loss 0.59923887 test_loss 0.56851685 \n",
      "大數據的設計思考 | 792 train_loss 0.53470564 valid_loss 0.59383136 test_loss 0.56613994 \n",
      "大數據的設計思考 | 793 train_loss 0.53427690 valid_loss 0.59413368 test_loss 0.56580496 \n",
      "大數據的設計思考 | 794 train_loss 0.53428876 valid_loss 0.59371006 test_loss 0.56553781 \n",
      "大數據的設計思考 | 795 train_loss 0.53477621 valid_loss 0.59441298 test_loss 0.56662440 \n",
      "大數據的設計思考 | 796 train_loss 0.53450251 valid_loss 0.59387451 test_loss 0.56594604 \n",
      "大數據的設計思考 | 797 train_loss 0.53431076 valid_loss 0.59390426 test_loss 0.56553459 \n",
      "大數據的設計思考 | 798 train_loss 0.53464854 valid_loss 0.59557277 test_loss 0.56614667 \n",
      "大數據的設計思考 | 799 train_loss 0.53702503 valid_loss 0.59929496 test_loss 0.56850398 \n",
      "大數據的設計思考 | 800 train_loss 0.53429967 valid_loss 0.59393984 test_loss 0.56546700 \n",
      "大數據的設計思考 | 801 train_loss 0.53431720 valid_loss 0.59479827 test_loss 0.56585920 \n",
      "大數據的設計思考 | 802 train_loss 0.53888124 valid_loss 0.59651303 test_loss 0.57056367 \n",
      "大數據的設計思考 | 803 train_loss 0.53543347 valid_loss 0.59442711 test_loss 0.56720918 \n",
      "大數據的設計思考 | 804 train_loss 0.53527415 valid_loss 0.59673762 test_loss 0.56691509 \n",
      "大數據的設計思考 | 805 train_loss 0.53504354 valid_loss 0.59535432 test_loss 0.56586474 \n",
      "大數據的設計思考 | 806 train_loss 0.53459436 valid_loss 0.59529394 test_loss 0.56602931 \n",
      "大數據的設計思考 | 807 train_loss 0.53489661 valid_loss 0.59592867 test_loss 0.56622761 \n",
      "大數據的設計思考 | 808 train_loss 0.53443879 valid_loss 0.59374213 test_loss 0.56583714 \n",
      "大數據的設計思考 | 809 train_loss 0.53455085 valid_loss 0.59536874 test_loss 0.56662315 \n",
      "大數據的設計思考 | 810 train_loss 0.53427446 valid_loss 0.59481817 test_loss 0.56586474 \n",
      "大數據的設計思考 | 811 train_loss 0.53445381 valid_loss 0.59477717 test_loss 0.56569725 \n",
      "大數據的設計思考 | 812 train_loss 0.53614438 valid_loss 0.59802097 test_loss 0.56768650 \n",
      "大數據的設計思考 | 813 train_loss 0.53461242 valid_loss 0.59572756 test_loss 0.56676161 \n",
      "大數據的設計思考 | 814 train_loss 0.53467870 valid_loss 0.59484047 test_loss 0.56573009 \n",
      "大數據的設計思考 | 815 train_loss 0.53495342 valid_loss 0.59588528 test_loss 0.56625986 \n",
      "大數據的設計思考 | 816 train_loss 0.53433722 valid_loss 0.59489459 test_loss 0.56607473 \n",
      "大數據的設計思考 | 817 train_loss 0.53494549 valid_loss 0.59421247 test_loss 0.56667441 \n",
      "大數據的設計思考 | 818 train_loss 0.53420341 valid_loss 0.59421265 test_loss 0.56560081 \n",
      "大數據的設計思考 | 819 train_loss 0.53494233 valid_loss 0.59610742 test_loss 0.56658638 \n",
      "大數據的設計思考 | 820 train_loss 0.53578967 valid_loss 0.59732658 test_loss 0.56724530 \n",
      "大數據的設計思考 | 821 train_loss 0.53500998 valid_loss 0.59615004 test_loss 0.56643474 \n",
      "大數據的設計思考 | 822 train_loss 0.53483653 valid_loss 0.59560817 test_loss 0.56611937 \n",
      "大數據的設計思考 | 823 train_loss 0.53508306 valid_loss 0.59377843 test_loss 0.56630886 \n",
      "大數據的設計思考 | 824 train_loss 0.53436482 valid_loss 0.59433794 test_loss 0.56601512 \n",
      "大數據的設計思考 | 825 train_loss 0.53474224 valid_loss 0.59569448 test_loss 0.56607795 \n",
      "大數據的設計思考 | 826 train_loss 0.53463936 valid_loss 0.59441340 test_loss 0.56652319 \n",
      "大數據的設計思考 | 827 train_loss 0.53473765 valid_loss 0.59570867 test_loss 0.56616193 \n",
      "大數據的設計思考 | 828 train_loss 0.53417754 valid_loss 0.59431893 test_loss 0.56569469 \n",
      "大數據的設計思考 | 829 train_loss 0.53446060 valid_loss 0.59416103 test_loss 0.56618559 \n",
      "大數據的設計思考 | 830 train_loss 0.53508341 valid_loss 0.59655297 test_loss 0.56683391 \n",
      "大數據的設計思考 | 831 train_loss 0.53456891 valid_loss 0.59441894 test_loss 0.56651419 \n",
      "大數據的設計思考 | 832 train_loss 0.53416210 valid_loss 0.59407294 test_loss 0.56561869 \n",
      "大數據的設計思考 | 833 train_loss 0.53506857 valid_loss 0.59495795 test_loss 0.56572396 \n",
      "大數據的設計思考 | 834 train_loss 0.53417319 valid_loss 0.59415942 test_loss 0.56570959 \n",
      "大數據的設計思考 | 835 train_loss 0.53510761 valid_loss 0.59407252 test_loss 0.56668246 \n",
      "大數據的設計思考 | 836 train_loss 0.53627121 valid_loss 0.59456366 test_loss 0.56785393 \n",
      "大數據的設計思考 | 837 train_loss 0.53462154 valid_loss 0.59382117 test_loss 0.56612933 \n",
      "大數據的設計思考 | 838 train_loss 0.53651160 valid_loss 0.59490353 test_loss 0.56818730 \n",
      "大數據的設計思考 | 839 train_loss 0.53717870 valid_loss 0.59556127 test_loss 0.56898737 \n",
      "大數據的設計思考 | 840 train_loss 0.53430206 valid_loss 0.59389681 test_loss 0.56575054 \n",
      "大數據的設計思考 | 841 train_loss 0.53711778 valid_loss 0.59916669 test_loss 0.56839067 \n",
      "大數據的設計思考 | 842 train_loss 0.53459305 valid_loss 0.59366924 test_loss 0.56579787 \n",
      "大數據的設計思考 | 843 train_loss 0.53451020 valid_loss 0.59378469 test_loss 0.56561536 \n",
      "大數據的設計思考 | 844 train_loss 0.53417224 valid_loss 0.59399956 test_loss 0.56559461 \n",
      "大數據的設計思考 | 845 train_loss 0.53451318 valid_loss 0.59496498 test_loss 0.56570518 \n",
      "大數據的設計思考 | 846 train_loss 0.53484416 valid_loss 0.59578776 test_loss 0.56615841 \n",
      "大數據的設計思考 | 847 train_loss 0.53424662 valid_loss 0.59380621 test_loss 0.56563449 \n",
      "大數據的設計思考 | 848 train_loss 0.53647542 valid_loss 0.59814030 test_loss 0.56763965 \n",
      "大數據的設計思考 | 849 train_loss 0.53547257 valid_loss 0.59634185 test_loss 0.56643492 \n",
      "大數據的設計思考 | 850 train_loss 0.53430235 valid_loss 0.59475428 test_loss 0.56598318 \n",
      "大數據的設計思考 | 851 train_loss 0.53463060 valid_loss 0.59557152 test_loss 0.56620461 \n",
      "大數據的設計思考 | 852 train_loss 0.53431326 valid_loss 0.59459805 test_loss 0.56568801 \n",
      "大數據的設計思考 | 853 train_loss 0.53473717 valid_loss 0.59545243 test_loss 0.56595421 \n",
      "大數據的設計思考 | 854 train_loss 0.53500700 valid_loss 0.59597760 test_loss 0.56636637 \n",
      "大數據的設計思考 | 855 train_loss 0.53428161 valid_loss 0.59400147 test_loss 0.56545544 \n",
      "大數據的設計思考 | 856 train_loss 0.53428829 valid_loss 0.59488189 test_loss 0.56599283 \n",
      "大數據的設計思考 | 857 train_loss 0.53466094 valid_loss 0.59523946 test_loss 0.56587505 \n",
      "大數據的設計思考 | 858 train_loss 0.53421909 valid_loss 0.59457046 test_loss 0.56585759 \n",
      "大數據的設計思考 | 859 train_loss 0.53426427 valid_loss 0.59402651 test_loss 0.56540942 \n",
      "大數據的設計思考 | 860 train_loss 0.53518242 valid_loss 0.59662652 test_loss 0.56686932 \n",
      "大數據的設計思考 | 861 train_loss 0.53478622 valid_loss 0.59372765 test_loss 0.56623483 \n",
      "大數據的設計思考 | 862 train_loss 0.53431040 valid_loss 0.59465939 test_loss 0.56578207 \n",
      "大數據的設計思考 | 863 train_loss 0.53442007 valid_loss 0.59370953 test_loss 0.56582695 \n",
      "大數據的設計思考 | 864 train_loss 0.53433424 valid_loss 0.59440380 test_loss 0.56614673 \n",
      "大數據的設計思考 | 865 train_loss 0.53428185 valid_loss 0.59469354 test_loss 0.56579906 \n",
      "大數據的設計思考 | 866 train_loss 0.53444099 valid_loss 0.59470570 test_loss 0.56626958 \n",
      "大數據的設計思考 | 867 train_loss 0.53455520 valid_loss 0.59535736 test_loss 0.56593668 \n",
      "大數據的設計思考 | 868 train_loss 0.53517979 valid_loss 0.59375650 test_loss 0.56648028 \n",
      "大數據的設計思考 | 869 train_loss 0.53443456 valid_loss 0.59393406 test_loss 0.56607181 \n",
      "大數據的設計思考 | 870 train_loss 0.53421116 valid_loss 0.59419191 test_loss 0.56547165 \n",
      "大數據的設計思考 | 871 train_loss 0.53526658 valid_loss 0.59406263 test_loss 0.56682962 \n",
      "大數據的設計思考 | 872 train_loss 0.53509122 valid_loss 0.59396428 test_loss 0.56673509 \n",
      "大數據的設計思考 | 873 train_loss 0.53586149 valid_loss 0.59436738 test_loss 0.56756067 \n",
      "大數據的設計思考 | 874 train_loss 0.53432798 valid_loss 0.59474874 test_loss 0.56604880 \n",
      "大數據的設計思考 | 875 train_loss 0.53553683 valid_loss 0.59667772 test_loss 0.56661958 \n",
      "大數據的設計思考 | 876 train_loss 0.53484684 valid_loss 0.59587437 test_loss 0.56637013 \n",
      "大數據的設計思考 | 877 train_loss 0.53424096 valid_loss 0.59458148 test_loss 0.56584346 \n",
      "大數據的設計思考 | 878 train_loss 0.53814363 valid_loss 0.59581453 test_loss 0.56969553 \n",
      "大數據的設計思考 | 879 train_loss 0.53484738 valid_loss 0.59580946 test_loss 0.56628698 \n",
      "大數據的設計思考 | 880 train_loss 0.53570902 valid_loss 0.59741437 test_loss 0.56723249 \n",
      "大數據的設計思考 | 881 train_loss 0.53415328 valid_loss 0.59390867 test_loss 0.56549454 \n",
      "大數據的設計思考 | 882 train_loss 0.53486580 valid_loss 0.59370548 test_loss 0.56617934 \n",
      "大數據的設計思考 | 883 train_loss 0.53440046 valid_loss 0.59370923 test_loss 0.56552219 \n",
      "大數據的設計思考 | 884 train_loss 0.53474408 valid_loss 0.59572792 test_loss 0.56628942 \n",
      "大數據的設計思考 | 885 train_loss 0.53477967 valid_loss 0.59518826 test_loss 0.56586492 \n",
      "大數據的設計思考 | 886 train_loss 0.53472888 valid_loss 0.59556234 test_loss 0.56620622 \n",
      "大數據的設計思考 | 887 train_loss 0.53445929 valid_loss 0.59401256 test_loss 0.56612283 \n",
      "大數據的設計思考 | 888 train_loss 0.53437757 valid_loss 0.59506077 test_loss 0.56592393 \n",
      "大數據的設計思考 | 889 train_loss 0.53441161 valid_loss 0.59504533 test_loss 0.56600869 \n",
      "大數據的設計思考 | 890 train_loss 0.53663558 valid_loss 0.59471124 test_loss 0.56807774 \n",
      "大數據的設計思考 | 891 train_loss 0.53532892 valid_loss 0.59394622 test_loss 0.56700385 \n",
      "大數據的設計思考 | 892 train_loss 0.53561199 valid_loss 0.59648365 test_loss 0.56665951 \n",
      "大數據的設計思考 | 893 train_loss 0.53438562 valid_loss 0.59436494 test_loss 0.56631064 \n",
      "大數據的設計思考 | 894 train_loss 0.53497130 valid_loss 0.59356898 test_loss 0.56616420 < save\n",
      "大數據的設計思考 | 895 train_loss 0.53489393 valid_loss 0.59396702 test_loss 0.56656855 \n",
      "大數據的設計思考 | 896 train_loss 0.53452063 valid_loss 0.59483576 test_loss 0.56568784 \n",
      "大數據的設計思考 | 897 train_loss 0.53429371 valid_loss 0.59461951 test_loss 0.56562603 \n",
      "大數據的設計思考 | 898 train_loss 0.53543395 valid_loss 0.59408867 test_loss 0.56697512 \n",
      "大數據的設計思考 | 899 train_loss 0.53495812 valid_loss 0.59367913 test_loss 0.56599635 \n",
      "大數據的設計思考 | 900 train_loss 0.53440022 valid_loss 0.59376615 test_loss 0.56537533 \n",
      "大數據的設計思考 | 901 train_loss 0.53466702 valid_loss 0.59346902 test_loss 0.56577897 < save\n",
      "大數據的設計思考 | 902 train_loss 0.53632694 valid_loss 0.59432590 test_loss 0.56772172 \n",
      "大數據的設計思考 | 903 train_loss 0.53427130 valid_loss 0.59433204 test_loss 0.56551450 \n",
      "大數據的設計思考 | 904 train_loss 0.53416473 valid_loss 0.59398305 test_loss 0.56572324 \n",
      "大數據的設計思考 | 905 train_loss 0.53453964 valid_loss 0.59538752 test_loss 0.56611848 \n",
      "大數據的設計思考 | 906 train_loss 0.53464067 valid_loss 0.59348750 test_loss 0.56569296 \n",
      "大數據的設計思考 | 907 train_loss 0.53525734 valid_loss 0.59403437 test_loss 0.56693286 \n",
      "大數據的設計思考 | 908 train_loss 0.53502315 valid_loss 0.59379375 test_loss 0.56618309 \n",
      "大數據的設計思考 | 909 train_loss 0.53492314 valid_loss 0.59453744 test_loss 0.56686711 \n",
      "大數據的設計思考 | 910 train_loss 0.53474873 valid_loss 0.59537017 test_loss 0.56591636 \n",
      "大數據的設計思考 | 911 train_loss 0.53416938 valid_loss 0.59424311 test_loss 0.56552076 \n",
      "大數據的設計思考 | 912 train_loss 0.53433788 valid_loss 0.59363073 test_loss 0.56546873 \n",
      "大數據的設計思考 | 913 train_loss 0.53457367 valid_loss 0.59500122 test_loss 0.56580782 \n",
      "大數據的設計思考 | 914 train_loss 0.53420454 valid_loss 0.59424222 test_loss 0.56592649 \n",
      "大數據的設計思考 | 915 train_loss 0.53419751 valid_loss 0.59404683 test_loss 0.56588155 \n",
      "大數據的設計思考 | 916 train_loss 0.53485358 valid_loss 0.59383637 test_loss 0.56628734 \n",
      "大數據的設計思考 | 917 train_loss 0.53414935 valid_loss 0.59401906 test_loss 0.56557900 \n",
      "大數據的設計思考 | 918 train_loss 0.53685403 valid_loss 0.59484863 test_loss 0.56826627 \n",
      "大數據的設計思考 | 919 train_loss 0.53426790 valid_loss 0.59473401 test_loss 0.56580049 \n",
      "大數據的設計思考 | 920 train_loss 0.53483075 valid_loss 0.59498250 test_loss 0.56563336 \n",
      "大數據的設計思考 | 921 train_loss 0.53417778 valid_loss 0.59412462 test_loss 0.56567037 \n",
      "大數據的設計思考 | 922 train_loss 0.53417593 valid_loss 0.59438479 test_loss 0.56570011 \n",
      "大數據的設計思考 | 923 train_loss 0.53438348 valid_loss 0.59437382 test_loss 0.56547332 \n",
      "大數據的設計思考 | 924 train_loss 0.53463113 valid_loss 0.59402913 test_loss 0.56545520 \n",
      "大數據的設計思考 | 925 train_loss 0.53524685 valid_loss 0.59669477 test_loss 0.56688476 \n",
      "大數據的設計思考 | 926 train_loss 0.53480023 valid_loss 0.59361392 test_loss 0.56609845 \n",
      "大數據的設計思考 | 927 train_loss 0.53475058 valid_loss 0.59574938 test_loss 0.56627423 \n",
      "大數據的設計思考 | 928 train_loss 0.53550315 valid_loss 0.59715056 test_loss 0.56709665 \n",
      "大數據的設計思考 | 929 train_loss 0.53433770 valid_loss 0.59368145 test_loss 0.56572866 \n",
      "大數據的設計思考 | 930 train_loss 0.53438717 valid_loss 0.59373128 test_loss 0.56578684 \n",
      "大數據的設計思考 | 931 train_loss 0.53461218 valid_loss 0.59527928 test_loss 0.56586617 \n",
      "大數據的設計思考 | 932 train_loss 0.53430521 valid_loss 0.59390759 test_loss 0.56585896 \n",
      "大數據的設計思考 | 933 train_loss 0.53438157 valid_loss 0.59435880 test_loss 0.56626242 \n",
      "大數據的設計思考 | 934 train_loss 0.53412360 valid_loss 0.59410399 test_loss 0.56554973 \n",
      "大數據的設計思考 | 935 train_loss 0.53555560 valid_loss 0.59714419 test_loss 0.56711024 \n",
      "大數據的設計思考 | 936 train_loss 0.53420967 valid_loss 0.59432709 test_loss 0.56564879 \n",
      "大數據的設計思考 | 937 train_loss 0.53430003 valid_loss 0.59486967 test_loss 0.56592715 \n",
      "大數據的設計思考 | 938 train_loss 0.53430605 valid_loss 0.59373033 test_loss 0.56578183 \n",
      "大數據的設計思考 | 939 train_loss 0.53443956 valid_loss 0.59390712 test_loss 0.56600189 \n",
      "大數據的設計思考 | 940 train_loss 0.53594029 valid_loss 0.59414732 test_loss 0.56722975 \n",
      "大數據的設計思考 | 941 train_loss 0.53520989 valid_loss 0.59662265 test_loss 0.56676155 \n",
      "大數據的設計思考 | 942 train_loss 0.53457767 valid_loss 0.59539366 test_loss 0.56601715 \n",
      "大數據的設計思考 | 943 train_loss 0.53414518 valid_loss 0.59425700 test_loss 0.56579220 \n",
      "大數據的設計思考 | 944 train_loss 0.53509611 valid_loss 0.59403098 test_loss 0.56682539 \n",
      "大數據的設計思考 | 945 train_loss 0.53518665 valid_loss 0.59603691 test_loss 0.56632388 \n",
      "大數據的設計思考 | 946 train_loss 0.53487825 valid_loss 0.59347999 test_loss 0.56603056 \n",
      "大數據的設計思考 | 947 train_loss 0.53458828 valid_loss 0.59522319 test_loss 0.56597519 \n",
      "大數據的設計思考 | 948 train_loss 0.53431463 valid_loss 0.59370589 test_loss 0.56554860 \n",
      "大數據的設計思考 | 949 train_loss 0.53470790 valid_loss 0.59561259 test_loss 0.56603527 \n",
      "大數據的設計思考 | 950 train_loss 0.53427738 valid_loss 0.59465373 test_loss 0.56589818 \n",
      "大數據的設計思考 | 951 train_loss 0.53485787 valid_loss 0.59566438 test_loss 0.56617707 \n",
      "大數據的設計思考 | 952 train_loss 0.53420919 valid_loss 0.59393507 test_loss 0.56580687 \n",
      "大數據的設計思考 | 953 train_loss 0.53428507 valid_loss 0.59371942 test_loss 0.56545913 \n",
      "大數據的設計思考 | 954 train_loss 0.53488827 valid_loss 0.59579372 test_loss 0.56619477 \n",
      "大數據的設計思考 | 955 train_loss 0.53428638 valid_loss 0.59449089 test_loss 0.56551433 \n",
      "大數據的設計思考 | 956 train_loss 0.53418088 valid_loss 0.59377074 test_loss 0.56570536 \n",
      "大數據的設計思考 | 957 train_loss 0.53415805 valid_loss 0.59394574 test_loss 0.56573206 \n",
      "大數據的設計思考 | 958 train_loss 0.53422344 valid_loss 0.59364969 test_loss 0.56554717 \n",
      "大數據的設計思考 | 959 train_loss 0.53847158 valid_loss 0.60066736 test_loss 0.56959063 \n",
      "大數據的設計思考 | 960 train_loss 0.53415096 valid_loss 0.59419811 test_loss 0.56578189 \n",
      "大數據的設計思考 | 961 train_loss 0.53483385 valid_loss 0.59372979 test_loss 0.56618172 \n",
      "大數據的設計思考 | 962 train_loss 0.53468877 valid_loss 0.59364295 test_loss 0.56622511 \n",
      "大數據的設計思考 | 963 train_loss 0.53502530 valid_loss 0.59366208 test_loss 0.56625623 \n",
      "大數據的設計思考 | 964 train_loss 0.53415138 valid_loss 0.59376979 test_loss 0.56548649 \n",
      "大數據的設計思考 | 965 train_loss 0.53476804 valid_loss 0.59586310 test_loss 0.56675893 \n",
      "大數據的設計思考 | 966 train_loss 0.53435719 valid_loss 0.59408736 test_loss 0.56534803 \n",
      "大數據的設計思考 | 967 train_loss 0.53525013 valid_loss 0.59438956 test_loss 0.56713915 \n",
      "大數據的設計思考 | 968 train_loss 0.53425699 valid_loss 0.59457302 test_loss 0.56564522 \n",
      "大數據的設計思考 | 969 train_loss 0.53455424 valid_loss 0.59362763 test_loss 0.56601918 \n",
      "大數據的設計思考 | 970 train_loss 0.53441983 valid_loss 0.59380591 test_loss 0.56595778 \n",
      "大數據的設計思考 | 971 train_loss 0.53449434 valid_loss 0.59354228 test_loss 0.56581515 \n",
      "大數據的設計思考 | 972 train_loss 0.53808779 valid_loss 0.59572667 test_loss 0.56975585 \n",
      "大數據的設計思考 | 973 train_loss 0.53425002 valid_loss 0.59463477 test_loss 0.56595784 \n",
      "大數據的設計思考 | 974 train_loss 0.53454274 valid_loss 0.59531611 test_loss 0.56614262 \n",
      "大數據的設計思考 | 975 train_loss 0.53643763 valid_loss 0.59464473 test_loss 0.56811601 \n",
      "大數據的設計思考 | 976 train_loss 0.53450167 valid_loss 0.59515154 test_loss 0.56586039 \n",
      "大數據的設計思考 | 977 train_loss 0.53490567 valid_loss 0.59580433 test_loss 0.56615627 \n",
      "大數據的設計思考 | 978 train_loss 0.53545153 valid_loss 0.59640014 test_loss 0.56659067 \n",
      "大數據的設計思考 | 979 train_loss 0.53410566 valid_loss 0.59397721 test_loss 0.56543797 \n",
      "大數據的設計思考 | 980 train_loss 0.53551197 valid_loss 0.59710324 test_loss 0.56718463 \n",
      "大數據的設計思考 | 981 train_loss 0.53460795 valid_loss 0.59387273 test_loss 0.56636512 \n",
      "大數據的設計思考 | 982 train_loss 0.53417867 valid_loss 0.59412253 test_loss 0.56581312 \n",
      "大數據的設計思考 | 983 train_loss 0.53443557 valid_loss 0.59354931 test_loss 0.56570172 \n",
      "大數據的設計思考 | 984 train_loss 0.53456873 valid_loss 0.59367031 test_loss 0.56615555 \n",
      "大數據的設計思考 | 985 train_loss 0.53435260 valid_loss 0.59492147 test_loss 0.56624520 \n",
      "大數據的設計思考 | 986 train_loss 0.53548616 valid_loss 0.59445566 test_loss 0.56743634 \n",
      "大數據的設計思考 | 987 train_loss 0.53519374 valid_loss 0.59399408 test_loss 0.56688529 \n",
      "大數據的設計思考 | 988 train_loss 0.53421509 valid_loss 0.59434396 test_loss 0.56551218 \n",
      "大數據的設計思考 | 989 train_loss 0.53475881 valid_loss 0.59518385 test_loss 0.56575197 \n",
      "大數據的設計思考 | 990 train_loss 0.53451836 valid_loss 0.59337121 test_loss 0.56583369 < save\n",
      "大數據的設計思考 | 991 train_loss 0.53417629 valid_loss 0.59383869 test_loss 0.56558597 \n",
      "大數據的設計思考 | 992 train_loss 0.53428453 valid_loss 0.59433359 test_loss 0.56545484 \n",
      "大數據的設計思考 | 993 train_loss 0.53727305 valid_loss 0.59508574 test_loss 0.56876183 \n",
      "大數據的設計思考 | 994 train_loss 0.53439832 valid_loss 0.59503949 test_loss 0.56616282 \n",
      "大數據的設計思考 | 995 train_loss 0.53449321 valid_loss 0.59369344 test_loss 0.56589568 \n",
      "大數據的設計思考 | 996 train_loss 0.53568769 valid_loss 0.59414822 test_loss 0.56722039 \n",
      "大數據的設計思考 | 997 train_loss 0.53418857 valid_loss 0.59367311 test_loss 0.56552559 \n",
      "大數據的設計思考 | 998 train_loss 0.53422070 valid_loss 0.59416062 test_loss 0.56590849 \n",
      "大數據的設計思考 | 999 train_loss 0.53521198 valid_loss 0.59393829 test_loss 0.56673503 \n",
      "大數據的設計思考 | 1000 train_loss 0.53420568 valid_loss 0.59444785 test_loss 0.56568474 \n",
      "ReTraining done, RMSE: 0.8016650676727295 => 0.5933712124824524\n",
      "decrease: 0.2082938551902771\n",
      "\n",
      "Now processing model: 教育物來學\n",
      "original RMSE: 0.8450517058372498\n",
      "教育物來學 | 1 train_loss 0.69869053 valid_loss 0.64815283 test_loss 0.70078939 < save\n",
      "教育物來學 | 2 train_loss 0.68070275 valid_loss 0.62908286 test_loss 0.68263519 < save\n",
      "教育物來學 | 3 train_loss 0.66769516 valid_loss 0.61242342 test_loss 0.66789168 < save\n",
      "教育物來學 | 4 train_loss 0.66427314 valid_loss 0.60788625 test_loss 0.66392785 < save\n",
      "教育物來學 | 5 train_loss 0.66491801 valid_loss 0.60657334 test_loss 0.66406256 < save\n",
      "教育物來學 | 6 train_loss 0.66091406 valid_loss 0.60427696 test_loss 0.66042244 < save\n",
      "教育物來學 | 7 train_loss 0.66096669 valid_loss 0.60286629 test_loss 0.66035295 < save\n",
      "教育物來學 | 8 train_loss 0.66122258 valid_loss 0.60590369 test_loss 0.66112435 \n",
      "教育物來學 | 9 train_loss 0.65971947 valid_loss 0.60385299 test_loss 0.65953630 \n",
      "教育物來學 | 10 train_loss 0.65907502 valid_loss 0.60037935 test_loss 0.65839547 < save\n",
      "教育物來學 | 11 train_loss 0.65925670 valid_loss 0.60341704 test_loss 0.65923643 \n",
      "教育物來學 | 12 train_loss 0.65897870 valid_loss 0.60002643 test_loss 0.65829718 < save\n",
      "教育物來學 | 13 train_loss 0.66482103 valid_loss 0.60415167 test_loss 0.66341406 \n",
      "教育物來學 | 14 train_loss 0.65731347 valid_loss 0.59901053 test_loss 0.65660882 < save\n",
      "教育物來學 | 15 train_loss 0.65765411 valid_loss 0.59892058 test_loss 0.65683585 < save\n",
      "教育物來學 | 16 train_loss 0.65824509 valid_loss 0.60269397 test_loss 0.65828729 \n",
      "教育物來學 | 17 train_loss 0.65606099 valid_loss 0.59806848 test_loss 0.65547806 < save\n",
      "教育物來學 | 18 train_loss 0.65629882 valid_loss 0.59975481 test_loss 0.65617836 \n",
      "教育物來學 | 19 train_loss 0.65592468 valid_loss 0.59835142 test_loss 0.65513784 \n",
      "教育物來學 | 20 train_loss 0.65762717 valid_loss 0.60212314 test_loss 0.65775353 \n",
      "教育物來學 | 21 train_loss 0.65668535 valid_loss 0.59761506 test_loss 0.65581357 < save\n",
      "教育物來學 | 22 train_loss 0.65514517 valid_loss 0.59763128 test_loss 0.65505427 \n",
      "教育物來學 | 23 train_loss 0.65476102 valid_loss 0.59711224 test_loss 0.65437007 < save\n",
      "教育物來學 | 24 train_loss 0.65634465 valid_loss 0.60062259 test_loss 0.65642470 \n",
      "教育物來學 | 25 train_loss 0.65442437 valid_loss 0.59672892 test_loss 0.65390319 < save\n",
      "教育物來學 | 26 train_loss 0.65449351 valid_loss 0.59639806 test_loss 0.65380114 < save\n",
      "教育物來學 | 27 train_loss 0.65527403 valid_loss 0.59696585 test_loss 0.65435970 \n",
      "教育物來學 | 28 train_loss 0.65379465 valid_loss 0.59669530 test_loss 0.65350461 \n",
      "教育物來學 | 29 train_loss 0.65431225 valid_loss 0.59620744 test_loss 0.65346354 < save\n",
      "教育物來學 | 30 train_loss 0.65351140 valid_loss 0.59600192 test_loss 0.65300816 < save\n",
      "教育物來學 | 31 train_loss 0.65354836 valid_loss 0.59569204 test_loss 0.65308970 < save\n",
      "教育物來學 | 32 train_loss 0.65446818 valid_loss 0.59677058 test_loss 0.65361458 \n",
      "教育物來學 | 33 train_loss 0.65472388 valid_loss 0.59910619 test_loss 0.65498710 \n",
      "教育物來學 | 34 train_loss 0.65433878 valid_loss 0.59622425 test_loss 0.65347761 \n",
      "教育物來學 | 35 train_loss 0.65301228 valid_loss 0.59521574 test_loss 0.65270317 < save\n",
      "教育物來學 | 36 train_loss 0.65428305 valid_loss 0.59867775 test_loss 0.65459436 \n",
      "教育物來學 | 37 train_loss 0.65298206 valid_loss 0.59495598 test_loss 0.65264684 < save\n",
      "教育物來學 | 38 train_loss 0.65285939 valid_loss 0.59646362 test_loss 0.65303266 \n",
      "教育物來學 | 39 train_loss 0.65718043 valid_loss 0.60300088 test_loss 0.65764695 \n",
      "教育物來學 | 40 train_loss 0.65259528 valid_loss 0.59498650 test_loss 0.65210623 \n",
      "教育物來學 | 41 train_loss 0.65238589 valid_loss 0.59495550 test_loss 0.65173173 < save\n",
      "教育物來學 | 42 train_loss 0.65273082 valid_loss 0.59496284 test_loss 0.65213215 \n",
      "教育物來學 | 43 train_loss 0.65291291 valid_loss 0.59741151 test_loss 0.65300173 \n",
      "教育物來學 | 44 train_loss 0.65752292 valid_loss 0.59788913 test_loss 0.65630490 \n",
      "教育物來學 | 45 train_loss 0.65184331 valid_loss 0.59522200 test_loss 0.65133375 \n",
      "教育物來學 | 46 train_loss 0.65209460 valid_loss 0.59453672 test_loss 0.65163332 < save\n",
      "教育物來學 | 47 train_loss 0.65282148 valid_loss 0.59488809 test_loss 0.65198332 \n",
      "教育物來學 | 48 train_loss 0.65182024 valid_loss 0.59592652 test_loss 0.65154320 \n",
      "教育物來學 | 49 train_loss 0.65172172 valid_loss 0.59464878 test_loss 0.65112770 \n",
      "教育物來學 | 50 train_loss 0.65184748 valid_loss 0.59590733 test_loss 0.65176755 \n",
      "教育物來學 | 51 train_loss 0.65173024 valid_loss 0.59533334 test_loss 0.65182668 \n",
      "教育物來學 | 52 train_loss 0.65133214 valid_loss 0.59488416 test_loss 0.65105677 \n",
      "教育物來學 | 53 train_loss 0.65465987 valid_loss 0.60034621 test_loss 0.65496933 \n",
      "教育物來學 | 54 train_loss 0.65227598 valid_loss 0.59442848 test_loss 0.65153772 < save\n",
      "教育物來學 | 55 train_loss 0.65192294 valid_loss 0.59610778 test_loss 0.65202796 \n",
      "教育物來學 | 56 train_loss 0.65120929 valid_loss 0.59487712 test_loss 0.65109384 \n",
      "教育物來學 | 57 train_loss 0.65174872 valid_loss 0.59580678 test_loss 0.65188444 \n",
      "教育物來學 | 58 train_loss 0.65475553 valid_loss 0.59612596 test_loss 0.65368146 \n",
      "教育物來學 | 59 train_loss 0.65162998 valid_loss 0.59537375 test_loss 0.65189582 \n",
      "教育物來學 | 60 train_loss 0.65817684 valid_loss 0.59879059 test_loss 0.65683365 \n",
      "教育物來學 | 61 train_loss 0.65204388 valid_loss 0.59699494 test_loss 0.65194631 \n",
      "教育物來學 | 62 train_loss 0.65129864 valid_loss 0.59582472 test_loss 0.65077299 \n",
      "教育物來學 | 63 train_loss 0.65115362 valid_loss 0.59545499 test_loss 0.65086520 \n",
      "教育物來學 | 64 train_loss 0.65100384 valid_loss 0.59466672 test_loss 0.65079093 \n",
      "教育物來學 | 65 train_loss 0.65100271 valid_loss 0.59472263 test_loss 0.65067744 \n",
      "教育物來學 | 66 train_loss 0.65115124 valid_loss 0.59459102 test_loss 0.65053999 \n",
      "教育物來學 | 67 train_loss 0.65184039 valid_loss 0.59428620 test_loss 0.65116960 < save\n",
      "教育物來學 | 68 train_loss 0.65306264 valid_loss 0.59859407 test_loss 0.65327471 \n",
      "教育物來學 | 69 train_loss 0.65087169 valid_loss 0.59443361 test_loss 0.65059453 \n",
      "教育物來學 | 70 train_loss 0.65714616 valid_loss 0.59797412 test_loss 0.65574038 \n",
      "教育物來學 | 71 train_loss 0.65419763 valid_loss 0.59964615 test_loss 0.65491068 \n",
      "教育物來學 | 72 train_loss 0.65105814 valid_loss 0.59445041 test_loss 0.65038913 \n",
      "教育物來學 | 73 train_loss 0.65339679 valid_loss 0.59530896 test_loss 0.65227175 \n",
      "教育物來學 | 74 train_loss 0.65077931 valid_loss 0.59461963 test_loss 0.65038240 \n",
      "教育物來學 | 75 train_loss 0.65082353 valid_loss 0.59433585 test_loss 0.65039599 \n",
      "教育物來學 | 76 train_loss 0.65502316 valid_loss 0.59660870 test_loss 0.65376639 \n",
      "教育物來學 | 77 train_loss 0.65297526 valid_loss 0.59524888 test_loss 0.65187567 \n",
      "教育物來學 | 78 train_loss 0.65107888 valid_loss 0.59439206 test_loss 0.65038288 \n",
      "教育物來學 | 79 train_loss 0.65117711 valid_loss 0.59432364 test_loss 0.65051585 \n",
      "教育物來學 | 80 train_loss 0.65083390 valid_loss 0.59491360 test_loss 0.65074134 \n",
      "教育物來學 | 81 train_loss 0.65069026 valid_loss 0.59463817 test_loss 0.65028805 \n",
      "教育物來學 | 82 train_loss 0.65109026 valid_loss 0.59423244 test_loss 0.65044218 < save\n",
      "教育物來學 | 83 train_loss 0.65068805 valid_loss 0.59448928 test_loss 0.65027034 \n",
      "教育物來學 | 84 train_loss 0.65146637 valid_loss 0.59664327 test_loss 0.65139538 \n",
      "教育物來學 | 85 train_loss 0.65091068 valid_loss 0.59511298 test_loss 0.65011621 \n",
      "教育物來學 | 86 train_loss 0.65098286 valid_loss 0.59482569 test_loss 0.65023696 \n",
      "教育物來學 | 87 train_loss 0.65860683 valid_loss 0.60602218 test_loss 0.65929365 \n",
      "教育物來學 | 88 train_loss 0.65184325 valid_loss 0.59432811 test_loss 0.65122885 \n",
      "教育物來學 | 89 train_loss 0.65068698 valid_loss 0.59478617 test_loss 0.65006971 \n",
      "教育物來學 | 90 train_loss 0.65129232 valid_loss 0.59636754 test_loss 0.65111148 \n",
      "教育物來學 | 91 train_loss 0.65138847 valid_loss 0.59643763 test_loss 0.65143883 \n",
      "教育物來學 | 92 train_loss 0.65089864 valid_loss 0.59402806 test_loss 0.65046245 < save\n",
      "教育物來學 | 93 train_loss 0.65061140 valid_loss 0.59486556 test_loss 0.65038079 \n",
      "教育物來學 | 94 train_loss 0.65064102 valid_loss 0.59520340 test_loss 0.65026331 \n",
      "教育物來學 | 95 train_loss 0.65280545 valid_loss 0.59522116 test_loss 0.65174174 \n",
      "教育物來學 | 96 train_loss 0.65080786 valid_loss 0.59411871 test_loss 0.65042436 \n",
      "教育物來學 | 97 train_loss 0.65584373 valid_loss 0.60225189 test_loss 0.65663868 \n",
      "教育物來學 | 98 train_loss 0.65062267 valid_loss 0.59440470 test_loss 0.65014541 \n",
      "教育物來學 | 99 train_loss 0.65066028 valid_loss 0.59532249 test_loss 0.65035844 \n",
      "教育物來學 | 100 train_loss 0.65177512 valid_loss 0.59678835 test_loss 0.65207177 \n",
      "教育物來學 | 101 train_loss 0.65087539 valid_loss 0.59603417 test_loss 0.65041053 \n",
      "教育物來學 | 102 train_loss 0.65157276 valid_loss 0.59437948 test_loss 0.65079331 \n",
      "教育物來學 | 103 train_loss 0.65080929 valid_loss 0.59552610 test_loss 0.65057874 \n",
      "教育物來學 | 104 train_loss 0.65155160 valid_loss 0.59670568 test_loss 0.65159243 \n",
      "教育物來學 | 105 train_loss 0.65077633 valid_loss 0.59401035 test_loss 0.65020984 < save\n",
      "教育物來學 | 106 train_loss 0.65078658 valid_loss 0.59478515 test_loss 0.64995509 \n",
      "教育物來學 | 107 train_loss 0.65051067 valid_loss 0.59454226 test_loss 0.65000796 \n",
      "教育物來學 | 108 train_loss 0.65112114 valid_loss 0.59616166 test_loss 0.65109795 \n",
      "教育物來學 | 109 train_loss 0.65092933 valid_loss 0.59415394 test_loss 0.65054220 \n",
      "教育物來學 | 110 train_loss 0.65061098 valid_loss 0.59469044 test_loss 0.65054423 \n",
      "教育物來學 | 111 train_loss 0.65047401 valid_loss 0.59453011 test_loss 0.64994180 \n",
      "教育物來學 | 112 train_loss 0.65043265 valid_loss 0.59457701 test_loss 0.64999503 \n",
      "教育物來學 | 113 train_loss 0.65050000 valid_loss 0.59456784 test_loss 0.65032357 \n",
      "教育物來學 | 114 train_loss 0.65044272 valid_loss 0.59468269 test_loss 0.64991510 \n",
      "教育物來學 | 115 train_loss 0.65045762 valid_loss 0.59450400 test_loss 0.65012544 \n",
      "教育物來學 | 116 train_loss 0.65106279 valid_loss 0.59407979 test_loss 0.65050465 \n",
      "教育物來學 | 117 train_loss 0.65214264 valid_loss 0.59800845 test_loss 0.65216124 \n",
      "教育物來學 | 118 train_loss 0.65072083 valid_loss 0.59435034 test_loss 0.65002477 \n",
      "教育物來學 | 119 train_loss 0.65307254 valid_loss 0.59552217 test_loss 0.65192711 \n",
      "教育物來學 | 120 train_loss 0.65149575 valid_loss 0.59719890 test_loss 0.65121841 \n",
      "教育物來學 | 121 train_loss 0.65339822 valid_loss 0.59568602 test_loss 0.65217119 \n",
      "教育物來學 | 122 train_loss 0.65083659 valid_loss 0.59571511 test_loss 0.65072477 \n",
      "教育物來學 | 123 train_loss 0.65287960 valid_loss 0.59880185 test_loss 0.65323156 \n",
      "教育物來學 | 124 train_loss 0.65057015 valid_loss 0.59522539 test_loss 0.65043545 \n",
      "教育物來學 | 125 train_loss 0.65086049 valid_loss 0.59423530 test_loss 0.65011239 \n",
      "教育物來學 | 126 train_loss 0.65053004 valid_loss 0.59472138 test_loss 0.64980215 \n",
      "教育物來學 | 127 train_loss 0.65091020 valid_loss 0.59613562 test_loss 0.65076768 \n",
      "教育物來學 | 128 train_loss 0.65138739 valid_loss 0.59457690 test_loss 0.65063965 \n",
      "教育物來學 | 129 train_loss 0.65239865 valid_loss 0.59510630 test_loss 0.65131682 \n",
      "教育物來學 | 130 train_loss 0.65081680 valid_loss 0.59583265 test_loss 0.65081853 \n",
      "教育物來學 | 131 train_loss 0.65073836 valid_loss 0.59559143 test_loss 0.65070838 \n",
      "教育物來學 | 132 train_loss 0.65048414 valid_loss 0.59530723 test_loss 0.65024006 \n",
      "教育物來學 | 133 train_loss 0.65165144 valid_loss 0.59707856 test_loss 0.65181708 \n",
      "教育物來學 | 134 train_loss 0.65053684 valid_loss 0.59437549 test_loss 0.65036130 \n",
      "教育物來學 | 135 train_loss 0.65129578 valid_loss 0.59687495 test_loss 0.65116650 \n",
      "教育物來學 | 136 train_loss 0.65070122 valid_loss 0.59494650 test_loss 0.64978451 \n",
      "教育物來學 | 137 train_loss 0.65035266 valid_loss 0.59461111 test_loss 0.64997107 \n",
      "教育物來學 | 138 train_loss 0.65327764 valid_loss 0.59611666 test_loss 0.65180153 \n",
      "教育物來學 | 139 train_loss 0.65038729 valid_loss 0.59505290 test_loss 0.64983886 \n",
      "教育物來學 | 140 train_loss 0.65031385 valid_loss 0.59481311 test_loss 0.64984995 \n",
      "教育物來學 | 141 train_loss 0.65093875 valid_loss 0.59420604 test_loss 0.65030038 \n",
      "教育物來學 | 142 train_loss 0.65091699 valid_loss 0.59597063 test_loss 0.65089756 \n",
      "教育物來學 | 143 train_loss 0.65038258 valid_loss 0.59500355 test_loss 0.65007442 \n",
      "教育物來學 | 144 train_loss 0.65072036 valid_loss 0.59427851 test_loss 0.65010643 \n",
      "教育物來學 | 145 train_loss 0.65678406 valid_loss 0.60445970 test_loss 0.65698308 \n",
      "教育物來學 | 146 train_loss 0.65045142 valid_loss 0.59427786 test_loss 0.65001565 \n",
      "教育物來學 | 147 train_loss 0.65035331 valid_loss 0.59487236 test_loss 0.65003014 \n",
      "教育物來學 | 148 train_loss 0.65048301 valid_loss 0.59556770 test_loss 0.65000254 \n",
      "教育物來學 | 149 train_loss 0.65209639 valid_loss 0.59476620 test_loss 0.65111405 \n",
      "教育物來學 | 150 train_loss 0.65035415 valid_loss 0.59468734 test_loss 0.64970171 \n",
      "教育物來學 | 151 train_loss 0.65033627 valid_loss 0.59488916 test_loss 0.64964801 \n",
      "教育物來學 | 152 train_loss 0.65032804 valid_loss 0.59453022 test_loss 0.64980745 \n",
      "教育物來學 | 153 train_loss 0.65057766 valid_loss 0.59579760 test_loss 0.65018559 \n",
      "教育物來學 | 154 train_loss 0.65117109 valid_loss 0.59647554 test_loss 0.65126967 \n",
      "教育物來學 | 155 train_loss 0.65192449 valid_loss 0.59493518 test_loss 0.65095407 \n",
      "教育物來學 | 156 train_loss 0.65071404 valid_loss 0.59434330 test_loss 0.65056539 \n",
      "教育物來學 | 157 train_loss 0.65038657 valid_loss 0.59442508 test_loss 0.65001893 \n",
      "教育物來學 | 158 train_loss 0.65391260 valid_loss 0.59624428 test_loss 0.65269524 \n",
      "教育物來學 | 159 train_loss 0.65031546 valid_loss 0.59469724 test_loss 0.64972448 \n",
      "教育物來學 | 160 train_loss 0.65035731 valid_loss 0.59439844 test_loss 0.64982504 \n",
      "教育物來學 | 161 train_loss 0.65033710 valid_loss 0.59515887 test_loss 0.65002030 \n",
      "教育物來學 | 162 train_loss 0.65028501 valid_loss 0.59458435 test_loss 0.64963853 \n",
      "教育物來學 | 163 train_loss 0.65037417 valid_loss 0.59469378 test_loss 0.65021122 \n",
      "教育物來學 | 164 train_loss 0.65038717 valid_loss 0.59541112 test_loss 0.64980125 \n",
      "教育物來學 | 165 train_loss 0.65058255 valid_loss 0.59434932 test_loss 0.65044427 \n",
      "教育物來學 | 166 train_loss 0.65085620 valid_loss 0.59623110 test_loss 0.65074861 \n",
      "教育物來學 | 167 train_loss 0.65110719 valid_loss 0.59634167 test_loss 0.65121967 \n",
      "教育物來學 | 168 train_loss 0.65035826 valid_loss 0.59545267 test_loss 0.64983171 \n",
      "教育物來學 | 169 train_loss 0.65039271 valid_loss 0.59515589 test_loss 0.65013552 \n",
      "教育物來學 | 170 train_loss 0.65207112 valid_loss 0.59761918 test_loss 0.65246576 \n",
      "教育物來學 | 171 train_loss 0.65061778 valid_loss 0.59418362 test_loss 0.65016204 \n",
      "教育物來學 | 172 train_loss 0.65035182 valid_loss 0.59515512 test_loss 0.65014088 \n",
      "教育物來學 | 173 train_loss 0.65025359 valid_loss 0.59462583 test_loss 0.64987838 \n",
      "教育物來學 | 174 train_loss 0.65209192 valid_loss 0.59811652 test_loss 0.65228868 \n",
      "教育物來學 | 175 train_loss 0.65076935 valid_loss 0.59440041 test_loss 0.65005291 \n",
      "教育物來學 | 176 train_loss 0.65029907 valid_loss 0.59516478 test_loss 0.65010071 \n",
      "教育物來學 | 177 train_loss 0.65020859 valid_loss 0.59469342 test_loss 0.64993089 \n",
      "教育物來學 | 178 train_loss 0.65721041 valid_loss 0.59894580 test_loss 0.65533942 \n",
      "教育物來學 | 179 train_loss 0.65130627 valid_loss 0.59712458 test_loss 0.65108705 \n",
      "教育物來學 | 180 train_loss 0.65160447 valid_loss 0.59500080 test_loss 0.65045428 \n",
      "教育物來學 | 181 train_loss 0.65048850 valid_loss 0.59506905 test_loss 0.64952272 \n",
      "教育物來學 | 182 train_loss 0.65049690 valid_loss 0.59521353 test_loss 0.65047926 \n",
      "教育物來學 | 183 train_loss 0.65021414 valid_loss 0.59453779 test_loss 0.64979947 \n",
      "教育物來學 | 184 train_loss 0.65027231 valid_loss 0.59529519 test_loss 0.64969748 \n",
      "教育物來學 | 185 train_loss 0.65448523 valid_loss 0.59659058 test_loss 0.65329599 \n",
      "教育物來學 | 186 train_loss 0.65021908 valid_loss 0.59476310 test_loss 0.64982712 \n",
      "教育物來學 | 187 train_loss 0.65209359 valid_loss 0.59499341 test_loss 0.65089387 \n",
      "教育物來學 | 188 train_loss 0.65166092 valid_loss 0.59779787 test_loss 0.65142143 \n",
      "教育物來學 | 189 train_loss 0.65184087 valid_loss 0.59810221 test_loss 0.65172613 \n",
      "教育物來學 | 190 train_loss 0.65051818 valid_loss 0.59491664 test_loss 0.64969838 \n",
      "教育物來學 | 191 train_loss 0.65033633 valid_loss 0.59462303 test_loss 0.65009165 \n",
      "教育物來學 | 192 train_loss 0.65056962 valid_loss 0.59443045 test_loss 0.64988953 \n",
      "教育物來學 | 193 train_loss 0.65039629 valid_loss 0.59578699 test_loss 0.64979637 \n",
      "教育物來學 | 194 train_loss 0.65023488 valid_loss 0.59453607 test_loss 0.64991283 \n",
      "教育物來學 | 195 train_loss 0.65493560 valid_loss 0.60223663 test_loss 0.65503615 \n",
      "教育物來學 | 196 train_loss 0.65120429 valid_loss 0.59694886 test_loss 0.65113640 \n",
      "教育物來學 | 197 train_loss 0.65037161 valid_loss 0.59444559 test_loss 0.64978099 \n",
      "教育物來學 | 198 train_loss 0.65049005 valid_loss 0.59535229 test_loss 0.64954698 \n",
      "教育物來學 | 199 train_loss 0.65294784 valid_loss 0.59574884 test_loss 0.65168047 \n",
      "教育物來學 | 200 train_loss 0.65032184 valid_loss 0.59533042 test_loss 0.65016687 \n",
      "教育物來學 | 201 train_loss 0.65805703 valid_loss 0.60600483 test_loss 0.65873414 \n",
      "教育物來學 | 202 train_loss 0.65016294 valid_loss 0.59477520 test_loss 0.64976782 \n",
      "教育物來學 | 203 train_loss 0.65071517 valid_loss 0.59442639 test_loss 0.64992446 \n",
      "教育物來學 | 204 train_loss 0.65208566 valid_loss 0.59798235 test_loss 0.65231025 \n",
      "教育物來學 | 205 train_loss 0.65104181 valid_loss 0.59448606 test_loss 0.64997536 \n",
      "教育物來學 | 206 train_loss 0.65061140 valid_loss 0.59441864 test_loss 0.64984506 \n",
      "教育物來學 | 207 train_loss 0.65020818 valid_loss 0.59519762 test_loss 0.64985412 \n",
      "教育物來學 | 208 train_loss 0.65192211 valid_loss 0.59788853 test_loss 0.65209645 \n",
      "教育物來學 | 209 train_loss 0.65020788 valid_loss 0.59522629 test_loss 0.64952832 \n",
      "教育物來學 | 210 train_loss 0.65097356 valid_loss 0.59674555 test_loss 0.65085953 \n",
      "教育物來學 | 211 train_loss 0.65111506 valid_loss 0.59683096 test_loss 0.65103555 \n",
      "Accuracy of validation is CRASH !!\n",
      "ReTraining done, RMSE: 0.8450517058372498 => 0.5940103530883789\n",
      "decrease: 0.25104135274887085\n",
      "\n",
      "Now processing model: 文學與創新\n",
      "original RMSE: 0.8166195154190063\n",
      "文學與創新 | 1 train_loss 0.64998126 valid_loss 0.65048420 test_loss 0.63101453 < save\n",
      "文學與創新 | 2 train_loss 0.65288669 valid_loss 0.65307045 test_loss 0.63606584 \n",
      "文學與創新 | 3 train_loss 0.64469314 valid_loss 0.64669716 test_loss 0.62390870 < save\n",
      "文學與創新 | 4 train_loss 0.64330375 valid_loss 0.64511603 test_loss 0.62228972 < save\n",
      "文學與創新 | 5 train_loss 0.64299881 valid_loss 0.64498234 test_loss 0.62243915 < save\n",
      "文學與創新 | 6 train_loss 0.64344686 valid_loss 0.64496326 test_loss 0.62373590 < save\n",
      "文學與創新 | 7 train_loss 0.64072758 valid_loss 0.64265472 test_loss 0.61904615 < save\n",
      "文學與創新 | 8 train_loss 0.64005786 valid_loss 0.64219320 test_loss 0.61780274 < save\n",
      "文學與創新 | 9 train_loss 0.64011598 valid_loss 0.64250726 test_loss 0.61905611 \n",
      "文學與創新 | 10 train_loss 0.63880390 valid_loss 0.64068544 test_loss 0.61678231 < save\n",
      "文學與創新 | 11 train_loss 0.63900882 valid_loss 0.64112276 test_loss 0.61563253 \n",
      "文學與創新 | 12 train_loss 0.63805956 valid_loss 0.64044374 test_loss 0.61491317 < save\n",
      "文學與創新 | 13 train_loss 0.63779324 valid_loss 0.64058834 test_loss 0.61483020 \n",
      "文學與創新 | 14 train_loss 0.63773137 valid_loss 0.64020914 test_loss 0.61396909 < save\n",
      "文學與創新 | 15 train_loss 0.63669330 valid_loss 0.63910621 test_loss 0.61520457 < save\n",
      "文學與創新 | 16 train_loss 0.63737434 valid_loss 0.64037341 test_loss 0.61303145 \n",
      "文學與創新 | 17 train_loss 0.63571143 valid_loss 0.63826162 test_loss 0.61219859 < save\n",
      "文學與創新 | 18 train_loss 0.63489646 valid_loss 0.63738590 test_loss 0.61229616 < save\n",
      "文學與創新 | 19 train_loss 0.64205128 valid_loss 0.64495695 test_loss 0.62342465 \n",
      "文學與創新 | 20 train_loss 0.63478774 valid_loss 0.63758236 test_loss 0.61075872 \n",
      "文學與創新 | 21 train_loss 0.63551080 valid_loss 0.63845497 test_loss 0.61055785 \n",
      "文學與創新 | 22 train_loss 0.63413984 valid_loss 0.63711870 test_loss 0.61269516 < save\n",
      "文學與創新 | 23 train_loss 0.63449955 valid_loss 0.63787991 test_loss 0.60960472 \n",
      "文學與創新 | 24 train_loss 0.63250005 valid_loss 0.63571531 test_loss 0.60958266 < save\n",
      "文學與創新 | 25 train_loss 0.63211626 valid_loss 0.63561267 test_loss 0.60925817 < save\n",
      "文學與創新 | 26 train_loss 0.63274181 valid_loss 0.63615894 test_loss 0.61123121 \n",
      "文學與創新 | 27 train_loss 0.63167310 valid_loss 0.63467783 test_loss 0.60931331 < save\n",
      "文學與創新 | 28 train_loss 0.63266861 valid_loss 0.63615787 test_loss 0.61158562 \n",
      "文學與創新 | 29 train_loss 0.63344634 valid_loss 0.63715070 test_loss 0.61291111 \n",
      "文學與創新 | 30 train_loss 0.63041830 valid_loss 0.63455284 test_loss 0.60706514 < save\n",
      "文學與創新 | 31 train_loss 0.63568836 valid_loss 0.64045185 test_loss 0.61609364 \n",
      "文學與創新 | 32 train_loss 0.62998444 valid_loss 0.63469946 test_loss 0.60616255 \n",
      "文學與創新 | 33 train_loss 0.62945217 valid_loss 0.63362509 test_loss 0.60622174 < save\n",
      "文學與創新 | 34 train_loss 0.62966239 valid_loss 0.63412154 test_loss 0.60515761 \n",
      "文學與創新 | 35 train_loss 0.62909722 valid_loss 0.63274503 test_loss 0.60627502 < save\n",
      "文學與創新 | 36 train_loss 0.63041741 valid_loss 0.63470691 test_loss 0.60930312 \n",
      "文學與創新 | 37 train_loss 0.62929076 valid_loss 0.63401717 test_loss 0.60757381 \n",
      "文學與創新 | 38 train_loss 0.62810516 valid_loss 0.63207942 test_loss 0.60532981 < save\n",
      "文學與創新 | 39 train_loss 0.62766492 valid_loss 0.63255084 test_loss 0.60401058 \n",
      "文學與創新 | 40 train_loss 0.62739730 valid_loss 0.63226241 test_loss 0.60402173 \n",
      "文學與創新 | 41 train_loss 0.62991601 valid_loss 0.63510901 test_loss 0.60369980 \n",
      "文學與創新 | 42 train_loss 0.62725085 valid_loss 0.63271183 test_loss 0.60329527 \n",
      "文學與創新 | 43 train_loss 0.62692201 valid_loss 0.63174331 test_loss 0.60265315 < save\n",
      "文學與創新 | 44 train_loss 0.62625647 valid_loss 0.63164645 test_loss 0.60242897 < save\n",
      "文學與創新 | 45 train_loss 0.62589473 valid_loss 0.63089895 test_loss 0.60246944 < save\n",
      "文學與創新 | 46 train_loss 0.62602633 valid_loss 0.63110220 test_loss 0.60379004 \n",
      "文學與創新 | 47 train_loss 0.62547010 valid_loss 0.63065654 test_loss 0.60271496 < save\n",
      "文學與創新 | 48 train_loss 0.62643522 valid_loss 0.63219613 test_loss 0.60116851 \n",
      "文學與創新 | 49 train_loss 0.62496030 valid_loss 0.63058609 test_loss 0.60216129 < save\n",
      "文學與創新 | 50 train_loss 0.62544149 valid_loss 0.63110846 test_loss 0.60357487 \n",
      "文學與創新 | 51 train_loss 0.62432665 valid_loss 0.63044912 test_loss 0.60055649 < save\n",
      "文學與創新 | 52 train_loss 0.62416202 valid_loss 0.62978113 test_loss 0.59998512 < save\n",
      "文學與創新 | 53 train_loss 0.62415957 valid_loss 0.62980551 test_loss 0.59986877 \n",
      "文學與創新 | 54 train_loss 0.62508887 valid_loss 0.63183635 test_loss 0.59951639 \n",
      "文學與創新 | 55 train_loss 0.62325639 valid_loss 0.62945908 test_loss 0.60037482 < save\n",
      "文學與創新 | 56 train_loss 0.62681919 valid_loss 0.63338917 test_loss 0.60649216 \n",
      "文學與創新 | 57 train_loss 0.62560654 valid_loss 0.63192922 test_loss 0.60489571 \n",
      "文學與創新 | 58 train_loss 0.62286937 valid_loss 0.62893748 test_loss 0.59850240 < save\n",
      "文學與創新 | 59 train_loss 0.62218583 valid_loss 0.62837231 test_loss 0.59877437 < save\n",
      "文學與創新 | 60 train_loss 0.62403172 valid_loss 0.62984586 test_loss 0.60322279 \n",
      "文學與創新 | 61 train_loss 0.62172347 valid_loss 0.62785733 test_loss 0.59887356 < save\n",
      "文學與創新 | 62 train_loss 0.62169552 valid_loss 0.62779135 test_loss 0.59784758 < save\n",
      "文學與創新 | 63 train_loss 0.62166965 valid_loss 0.62830204 test_loss 0.59729439 \n",
      "文學與創新 | 64 train_loss 0.62107837 valid_loss 0.62785208 test_loss 0.59739143 \n",
      "文學與創新 | 65 train_loss 0.62109929 valid_loss 0.62783486 test_loss 0.59684682 \n",
      "文學與創新 | 66 train_loss 0.62796187 valid_loss 0.63558620 test_loss 0.60014993 \n",
      "文學與創新 | 67 train_loss 0.62253761 valid_loss 0.63003570 test_loss 0.59675968 \n",
      "文學與創新 | 68 train_loss 0.62016726 valid_loss 0.62673509 test_loss 0.59667301 < save\n",
      "文學與創新 | 69 train_loss 0.62146616 valid_loss 0.62799764 test_loss 0.60038102 \n",
      "文學與創新 | 70 train_loss 0.61993587 valid_loss 0.62695289 test_loss 0.59764445 \n",
      "文學與創新 | 71 train_loss 0.61968452 valid_loss 0.62683874 test_loss 0.59710813 \n",
      "文學與創新 | 72 train_loss 0.62023580 valid_loss 0.62664211 test_loss 0.59867126 < save\n",
      "文學與創新 | 73 train_loss 0.62004721 valid_loss 0.62694669 test_loss 0.59557432 \n",
      "文學與創新 | 74 train_loss 0.61914730 valid_loss 0.62680250 test_loss 0.59539986 \n",
      "文學與創新 | 75 train_loss 0.61897314 valid_loss 0.62628442 test_loss 0.59663785 < save\n",
      "文學與創新 | 76 train_loss 0.61856318 valid_loss 0.62552416 test_loss 0.59558803 < save\n",
      "文學與創新 | 77 train_loss 0.61900097 valid_loss 0.62640119 test_loss 0.59481138 \n",
      "文學與創新 | 78 train_loss 0.61823177 valid_loss 0.62554163 test_loss 0.59495944 \n",
      "文學與創新 | 79 train_loss 0.61842924 valid_loss 0.62589473 test_loss 0.59436142 \n",
      "文學與創新 | 80 train_loss 0.61869049 valid_loss 0.62618083 test_loss 0.59711635 \n",
      "文學與創新 | 81 train_loss 0.61794305 valid_loss 0.62558967 test_loss 0.59565622 \n",
      "文學與創新 | 82 train_loss 0.61984253 valid_loss 0.62743205 test_loss 0.59446043 \n",
      "文學與創新 | 83 train_loss 0.61753041 valid_loss 0.62553900 test_loss 0.59454352 \n",
      "文學與創新 | 84 train_loss 0.61746234 valid_loss 0.62522477 test_loss 0.59365380 < save\n",
      "文學與創新 | 85 train_loss 0.62203372 valid_loss 0.63032585 test_loss 0.60214686 \n",
      "文學與創新 | 86 train_loss 0.61724019 valid_loss 0.62543827 test_loss 0.59469771 \n",
      "文學與創新 | 87 train_loss 0.61716735 valid_loss 0.62494439 test_loss 0.59523278 < save\n",
      "文學與創新 | 88 train_loss 0.61761403 valid_loss 0.62514520 test_loss 0.59635973 \n",
      "文學與創新 | 89 train_loss 0.61779624 valid_loss 0.62591219 test_loss 0.59331501 \n",
      "文學與創新 | 90 train_loss 0.61646277 valid_loss 0.62441885 test_loss 0.59346473 < save\n",
      "文學與創新 | 91 train_loss 0.61636937 valid_loss 0.62441105 test_loss 0.59323180 < save\n",
      "文學與創新 | 92 train_loss 0.61891645 valid_loss 0.62646067 test_loss 0.59871995 \n",
      "文學與創新 | 93 train_loss 0.61632121 valid_loss 0.62411505 test_loss 0.59443104 < save\n",
      "文學與創新 | 94 train_loss 0.61604571 valid_loss 0.62367260 test_loss 0.59375978 < save\n",
      "文學與創新 | 95 train_loss 0.61670208 valid_loss 0.62502521 test_loss 0.59244108 \n",
      "文學與創新 | 96 train_loss 0.61857826 valid_loss 0.62678403 test_loss 0.59825593 \n",
      "文學與創新 | 97 train_loss 0.61796236 valid_loss 0.62572157 test_loss 0.59769768 \n",
      "文學與創新 | 98 train_loss 0.61583853 valid_loss 0.62391186 test_loss 0.59406078 \n",
      "文學與創新 | 99 train_loss 0.61555523 valid_loss 0.62398428 test_loss 0.59219319 \n",
      "文學與創新 | 100 train_loss 0.61741877 valid_loss 0.62567002 test_loss 0.59263527 \n",
      "文學與創新 | 101 train_loss 0.62379336 valid_loss 0.63215095 test_loss 0.60541791 \n",
      "文學與創新 | 102 train_loss 0.61531061 valid_loss 0.62347734 test_loss 0.59340572 < save\n",
      "文學與創新 | 103 train_loss 0.62134838 valid_loss 0.62966764 test_loss 0.60249668 \n",
      "文學與創新 | 104 train_loss 0.61510831 valid_loss 0.62286073 test_loss 0.59332931 < save\n",
      "文學與創新 | 105 train_loss 0.61587930 valid_loss 0.62417346 test_loss 0.59185547 \n",
      "文學與創新 | 106 train_loss 0.61540926 valid_loss 0.62408096 test_loss 0.59380507 \n",
      "文學與創新 | 107 train_loss 0.61509693 valid_loss 0.62387687 test_loss 0.59324116 \n",
      "文學與創新 | 108 train_loss 0.61521244 valid_loss 0.62289417 test_loss 0.59225833 \n",
      "文學與創新 | 109 train_loss 0.61476457 valid_loss 0.62313461 test_loss 0.59151149 \n",
      "文學與創新 | 110 train_loss 0.61501682 valid_loss 0.62340623 test_loss 0.59360778 \n",
      "文學與創新 | 111 train_loss 0.61686933 valid_loss 0.62568772 test_loss 0.59643781 \n",
      "文學與創新 | 112 train_loss 0.61416250 valid_loss 0.62278581 test_loss 0.59174365 < save\n",
      "文學與創新 | 113 train_loss 0.61565942 valid_loss 0.62427670 test_loss 0.59152120 \n",
      "文學與創新 | 114 train_loss 0.61554366 valid_loss 0.62397599 test_loss 0.59143299 \n",
      "文學與創新 | 115 train_loss 0.61629879 valid_loss 0.62511230 test_loss 0.59162676 \n",
      "文學與創新 | 116 train_loss 0.61468291 valid_loss 0.62270361 test_loss 0.59362304 < save\n",
      "文學與創新 | 117 train_loss 0.61707741 valid_loss 0.62604964 test_loss 0.59193701 \n",
      "文學與創新 | 118 train_loss 0.61367989 valid_loss 0.62253159 test_loss 0.59142685 < save\n",
      "文學與創新 | 119 train_loss 0.61789775 valid_loss 0.62694967 test_loss 0.59242743 \n",
      "文學與創新 | 120 train_loss 0.61600584 valid_loss 0.62432826 test_loss 0.59628689 \n",
      "文學與創新 | 121 train_loss 0.61553669 valid_loss 0.62459111 test_loss 0.59514076 \n",
      "文學與創新 | 122 train_loss 0.61418790 valid_loss 0.62275761 test_loss 0.59320623 \n",
      "文學與創新 | 123 train_loss 0.61374044 valid_loss 0.62346929 test_loss 0.59104651 \n",
      "文學與創新 | 124 train_loss 0.61529231 valid_loss 0.62479126 test_loss 0.59455734 \n",
      "文學與創新 | 125 train_loss 0.61318856 valid_loss 0.62204826 test_loss 0.59131253 < save\n",
      "文學與創新 | 126 train_loss 0.61308360 valid_loss 0.62183440 test_loss 0.59092623 < save\n",
      "文學與創新 | 127 train_loss 0.61349452 valid_loss 0.62233388 test_loss 0.59037679 \n",
      "文學與創新 | 128 train_loss 0.61398786 valid_loss 0.62341756 test_loss 0.59008086 \n",
      "文學與創新 | 129 train_loss 0.61501747 valid_loss 0.62465173 test_loss 0.59025717 \n",
      "文學與創新 | 130 train_loss 0.61293280 valid_loss 0.62178397 test_loss 0.59054017 < save\n",
      "文學與創新 | 131 train_loss 0.61293274 valid_loss 0.62242413 test_loss 0.59067529 \n",
      "文學與創新 | 132 train_loss 0.61272293 valid_loss 0.62186831 test_loss 0.59063751 \n",
      "文學與創新 | 133 train_loss 0.61313277 valid_loss 0.62295866 test_loss 0.58967388 \n",
      "文學與創新 | 134 train_loss 0.61285061 valid_loss 0.62198704 test_loss 0.58994746 \n",
      "文學與創新 | 135 train_loss 0.61271876 valid_loss 0.62211096 test_loss 0.59120709 \n",
      "文學與創新 | 136 train_loss 0.61589682 valid_loss 0.62498170 test_loss 0.59641838 \n",
      "文學與創新 | 137 train_loss 0.61263418 valid_loss 0.62228483 test_loss 0.59063333 \n",
      "文學與創新 | 138 train_loss 0.61310893 valid_loss 0.62286466 test_loss 0.59174758 \n",
      "文學與創新 | 139 train_loss 0.61229712 valid_loss 0.62188423 test_loss 0.58992994 \n",
      "文學與創新 | 140 train_loss 0.61241508 valid_loss 0.62206167 test_loss 0.58957189 \n",
      "文學與創新 | 141 train_loss 0.61312240 valid_loss 0.62280500 test_loss 0.58951217 \n",
      "文學與創新 | 142 train_loss 0.61285251 valid_loss 0.62195802 test_loss 0.58992422 \n",
      "文學與創新 | 143 train_loss 0.61209697 valid_loss 0.62129718 test_loss 0.59000707 < save\n",
      "文學與創新 | 144 train_loss 0.61249042 valid_loss 0.62111259 test_loss 0.59178901 < save\n",
      "文學與創新 | 145 train_loss 0.61515433 valid_loss 0.62507582 test_loss 0.59516913 \n",
      "文學與創新 | 146 train_loss 0.62074143 valid_loss 0.63174218 test_loss 0.59360480 \n",
      "文學與創新 | 147 train_loss 0.61221355 valid_loss 0.62249517 test_loss 0.58980668 \n",
      "文學與創新 | 148 train_loss 0.61183536 valid_loss 0.62128526 test_loss 0.58940351 \n",
      "文學與創新 | 149 train_loss 0.61264563 valid_loss 0.62220365 test_loss 0.58939767 \n",
      "文學與創新 | 150 train_loss 0.61370969 valid_loss 0.62300307 test_loss 0.59016806 \n",
      "文學與創新 | 151 train_loss 0.61318868 valid_loss 0.62329358 test_loss 0.59226793 \n",
      "文學與創新 | 152 train_loss 0.61172867 valid_loss 0.62179631 test_loss 0.58891475 \n",
      "文學與創新 | 153 train_loss 0.61476797 valid_loss 0.62376231 test_loss 0.59581161 \n",
      "文學與創新 | 154 train_loss 0.61164659 valid_loss 0.62137830 test_loss 0.58902228 \n",
      "文學與創新 | 155 train_loss 0.61235732 valid_loss 0.62172133 test_loss 0.59207118 \n",
      "文學與創新 | 156 train_loss 0.61376548 valid_loss 0.62397897 test_loss 0.58957857 \n",
      "文學與創新 | 157 train_loss 0.61197549 valid_loss 0.62113506 test_loss 0.58975756 \n",
      "文學與創新 | 158 train_loss 0.61924124 valid_loss 0.62993073 test_loss 0.59310067 \n",
      "文學與創新 | 159 train_loss 0.61161548 valid_loss 0.62144244 test_loss 0.58896434 \n",
      "文學與創新 | 160 train_loss 0.61161679 valid_loss 0.62095022 test_loss 0.58961964 < save\n",
      "文學與創新 | 161 train_loss 0.61442643 valid_loss 0.62380838 test_loss 0.59501660 \n",
      "文學與創新 | 162 train_loss 0.61599422 valid_loss 0.62479860 test_loss 0.59797627 \n",
      "文學與創新 | 163 train_loss 0.61122912 valid_loss 0.62044585 test_loss 0.58983678 < save\n",
      "文學與創新 | 164 train_loss 0.61183715 valid_loss 0.62181288 test_loss 0.58868724 \n",
      "文學與創新 | 165 train_loss 0.61114401 valid_loss 0.62057459 test_loss 0.58950198 \n",
      "文學與創新 | 166 train_loss 0.61147863 valid_loss 0.62127638 test_loss 0.58870316 \n",
      "文學與創新 | 167 train_loss 0.61521459 valid_loss 0.62643373 test_loss 0.58989340 \n",
      "文學與創新 | 168 train_loss 0.61176324 valid_loss 0.62156934 test_loss 0.59110290 \n",
      "文學與創新 | 169 train_loss 0.61190325 valid_loss 0.62111211 test_loss 0.58934790 \n",
      "文學與創新 | 170 train_loss 0.61122614 valid_loss 0.62122416 test_loss 0.59026712 \n",
      "文學與創新 | 171 train_loss 0.61086679 valid_loss 0.62024611 test_loss 0.58957076 < save\n",
      "文學與創新 | 172 train_loss 0.61078799 valid_loss 0.62055135 test_loss 0.58894956 \n",
      "文學與創新 | 173 train_loss 0.61191648 valid_loss 0.62166452 test_loss 0.59188354 \n",
      "文學與創新 | 174 train_loss 0.61073977 valid_loss 0.62005705 test_loss 0.58961082 < save\n",
      "文學與創新 | 175 train_loss 0.61067468 valid_loss 0.62074476 test_loss 0.58949393 \n",
      "文學與創新 | 176 train_loss 0.61074328 valid_loss 0.61990619 test_loss 0.59015846 < save\n",
      "文學與創新 | 177 train_loss 0.61325502 valid_loss 0.62343180 test_loss 0.58937877 \n",
      "文學與創新 | 178 train_loss 0.61101383 valid_loss 0.62153590 test_loss 0.58839184 \n",
      "文學與創新 | 179 train_loss 0.61051488 valid_loss 0.62075871 test_loss 0.58869082 \n",
      "文學與創新 | 180 train_loss 0.61061257 valid_loss 0.61974365 test_loss 0.59031147 < save\n",
      "文學與創新 | 181 train_loss 0.61053622 valid_loss 0.62053955 test_loss 0.58831984 \n",
      "文學與創新 | 182 train_loss 0.61070782 valid_loss 0.62075573 test_loss 0.59026176 \n",
      "文學與創新 | 183 train_loss 0.61018944 valid_loss 0.61989886 test_loss 0.58933884 \n",
      "文學與創新 | 184 train_loss 0.61003953 valid_loss 0.61884153 test_loss 0.58924675 < save\n",
      "文學與創新 | 185 train_loss 0.61039454 valid_loss 0.61956686 test_loss 0.59057289 \n",
      "文學與創新 | 186 train_loss 0.61425376 valid_loss 0.62399685 test_loss 0.58968264 \n",
      "文學與創新 | 187 train_loss 0.61121225 valid_loss 0.62020993 test_loss 0.59252596 \n",
      "文學與創新 | 188 train_loss 0.60888135 valid_loss 0.61812687 test_loss 0.58848250 < save\n",
      "文學與創新 | 189 train_loss 0.60853255 valid_loss 0.61780691 test_loss 0.58795601 < save\n",
      "文學與創新 | 190 train_loss 0.60934532 valid_loss 0.61838442 test_loss 0.59014326 \n",
      "文學與創新 | 191 train_loss 0.60956335 valid_loss 0.61945277 test_loss 0.58658427 \n",
      "文學與創新 | 192 train_loss 0.60938895 valid_loss 0.61909086 test_loss 0.58668661 \n",
      "文學與創新 | 193 train_loss 0.60797441 valid_loss 0.61765003 test_loss 0.58738953 < save\n",
      "文學與創新 | 194 train_loss 0.61184698 valid_loss 0.62117898 test_loss 0.59365529 \n",
      "文學與創新 | 195 train_loss 0.60785407 valid_loss 0.61746937 test_loss 0.58637536 < save\n",
      "文學與創新 | 196 train_loss 0.60772604 valid_loss 0.61740285 test_loss 0.58706844 < save\n",
      "文學與創新 | 197 train_loss 0.60843277 valid_loss 0.61831677 test_loss 0.58864599 \n",
      "文學與創新 | 198 train_loss 0.60785782 valid_loss 0.61836988 test_loss 0.58600920 \n",
      "文學與創新 | 199 train_loss 0.61180413 valid_loss 0.62017524 test_loss 0.59466571 \n",
      "文學與創新 | 200 train_loss 0.60817623 valid_loss 0.61902267 test_loss 0.58587623 \n",
      "文學與創新 | 201 train_loss 0.60780847 valid_loss 0.61816591 test_loss 0.58716750 \n",
      "文學與創新 | 202 train_loss 0.60905164 valid_loss 0.61976105 test_loss 0.58570969 \n",
      "文學與創新 | 203 train_loss 0.60733581 valid_loss 0.61680520 test_loss 0.58658600 < save\n",
      "文學與創新 | 204 train_loss 0.60783482 valid_loss 0.61700064 test_loss 0.58852386 \n",
      "文學與創新 | 205 train_loss 0.60925335 valid_loss 0.61914003 test_loss 0.59033382 \n",
      "文學與創新 | 206 train_loss 0.60916114 valid_loss 0.61866397 test_loss 0.59043276 \n",
      "文學與創新 | 207 train_loss 0.61289608 valid_loss 0.62351382 test_loss 0.58778757 \n",
      "文學與創新 | 208 train_loss 0.60699409 valid_loss 0.61662835 test_loss 0.58620059 < save\n",
      "文學與創新 | 209 train_loss 0.60718334 valid_loss 0.61651069 test_loss 0.58635885 < save\n",
      "文學與創新 | 210 train_loss 0.60868162 valid_loss 0.61851794 test_loss 0.58575422 \n",
      "文學與創新 | 211 train_loss 0.60685736 valid_loss 0.61633861 test_loss 0.58635736 < save\n",
      "文學與創新 | 212 train_loss 0.60702682 valid_loss 0.61624932 test_loss 0.58732361 < save\n",
      "文學與創新 | 213 train_loss 0.61265278 valid_loss 0.62326831 test_loss 0.58768994 \n",
      "文學與創新 | 214 train_loss 0.60702604 valid_loss 0.61653554 test_loss 0.58586425 \n",
      "文學與創新 | 215 train_loss 0.60840845 valid_loss 0.61787999 test_loss 0.58600420 \n",
      "文學與創新 | 216 train_loss 0.60667425 valid_loss 0.61606675 test_loss 0.58635867 < save\n",
      "文學與創新 | 217 train_loss 0.60665679 valid_loss 0.61639720 test_loss 0.58663243 \n",
      "文學與創新 | 218 train_loss 0.60859990 valid_loss 0.61857998 test_loss 0.58572239 \n",
      "文學與創新 | 219 train_loss 0.60729289 valid_loss 0.61694717 test_loss 0.58794904 \n",
      "文學與創新 | 220 train_loss 0.60760969 valid_loss 0.61627549 test_loss 0.58864194 \n",
      "文學與創新 | 221 train_loss 0.60730600 valid_loss 0.61747664 test_loss 0.58503306 \n",
      "文學與創新 | 222 train_loss 0.60645807 valid_loss 0.61625928 test_loss 0.58583039 \n",
      "文學與創新 | 223 train_loss 0.60691971 valid_loss 0.61678332 test_loss 0.58693564 \n",
      "文學與創新 | 224 train_loss 0.60679859 valid_loss 0.61721659 test_loss 0.58502638 \n",
      "文學與創新 | 225 train_loss 0.60972267 valid_loss 0.62018472 test_loss 0.58595324 \n",
      "文學與創新 | 226 train_loss 0.60877776 valid_loss 0.61859936 test_loss 0.58585334 \n",
      "文學與創新 | 227 train_loss 0.60766453 valid_loss 0.61617672 test_loss 0.58947998 \n",
      "文學與創新 | 228 train_loss 0.60918695 valid_loss 0.61878073 test_loss 0.59093821 \n",
      "文學與創新 | 229 train_loss 0.60622132 valid_loss 0.61562127 test_loss 0.58656657 < save\n",
      "文學與創新 | 230 train_loss 0.60622758 valid_loss 0.61583370 test_loss 0.58535075 \n",
      "文學與創新 | 231 train_loss 0.60704076 valid_loss 0.61674553 test_loss 0.58778524 \n",
      "文學與創新 | 232 train_loss 0.60730422 valid_loss 0.61732262 test_loss 0.58532405 \n",
      "文學與創新 | 233 train_loss 0.60602343 valid_loss 0.61603022 test_loss 0.58570790 \n",
      "文學與創新 | 234 train_loss 0.60653853 valid_loss 0.61588728 test_loss 0.58568853 \n",
      "文學與創新 | 235 train_loss 0.60612303 valid_loss 0.61575967 test_loss 0.58630234 \n",
      "文學與創新 | 236 train_loss 0.60665917 valid_loss 0.61593556 test_loss 0.58786297 \n",
      "文學與創新 | 237 train_loss 0.60597694 valid_loss 0.61551702 test_loss 0.58582872 < save\n",
      "文學與創新 | 238 train_loss 0.60597235 valid_loss 0.61589086 test_loss 0.58502579 \n",
      "文學與創新 | 239 train_loss 0.60802746 valid_loss 0.61765200 test_loss 0.58970189 \n",
      "文學與創新 | 240 train_loss 0.60687673 valid_loss 0.61702234 test_loss 0.58502096 \n",
      "文學與創新 | 241 train_loss 0.60783154 valid_loss 0.61685663 test_loss 0.59018612 \n",
      "文學與創新 | 242 train_loss 0.60947472 valid_loss 0.61840820 test_loss 0.59239763 \n",
      "文學與創新 | 243 train_loss 0.60603243 valid_loss 0.61592269 test_loss 0.58467734 \n",
      "文學與創新 | 244 train_loss 0.60560417 valid_loss 0.61536545 test_loss 0.58529538 < save\n",
      "文學與創新 | 245 train_loss 0.60562468 valid_loss 0.61519414 test_loss 0.58608693 < save\n",
      "文學與創新 | 246 train_loss 0.60720807 valid_loss 0.61758411 test_loss 0.58478814 \n",
      "文學與創新 | 247 train_loss 0.60549110 valid_loss 0.61515981 test_loss 0.58547771 < save\n",
      "文學與創新 | 248 train_loss 0.60853648 valid_loss 0.61875421 test_loss 0.58557981 \n",
      "文學與創新 | 249 train_loss 0.60704756 valid_loss 0.61656708 test_loss 0.58535939 \n",
      "文學與創新 | 250 train_loss 0.60862106 valid_loss 0.61799145 test_loss 0.59122556 \n",
      "文學與創新 | 251 train_loss 0.60614222 valid_loss 0.61588240 test_loss 0.58725935 \n",
      "文學與創新 | 252 train_loss 0.60557073 valid_loss 0.61544746 test_loss 0.58499318 \n",
      "文學與創新 | 253 train_loss 0.60540015 valid_loss 0.61553955 test_loss 0.58517015 \n",
      "文學與創新 | 254 train_loss 0.60844761 valid_loss 0.61903036 test_loss 0.58529580 \n",
      "文學與創新 | 255 train_loss 0.60639542 valid_loss 0.61552697 test_loss 0.58848846 \n",
      "文學與創新 | 256 train_loss 0.60560519 valid_loss 0.61538076 test_loss 0.58480388 \n",
      "文學與創新 | 257 train_loss 0.60844123 valid_loss 0.61799479 test_loss 0.59083277 \n",
      "文學與創新 | 258 train_loss 0.60643715 valid_loss 0.61595130 test_loss 0.58528858 \n",
      "文學與創新 | 259 train_loss 0.60986215 valid_loss 0.62025017 test_loss 0.58660066 \n",
      "文學與創新 | 260 train_loss 0.60569942 valid_loss 0.61503541 test_loss 0.58710456 < save\n",
      "文學與創新 | 261 train_loss 0.60521358 valid_loss 0.61550701 test_loss 0.58448189 \n",
      "文學與創新 | 262 train_loss 0.60515404 valid_loss 0.61502665 test_loss 0.58482879 < save\n",
      "文學與創新 | 263 train_loss 0.60649884 valid_loss 0.61607671 test_loss 0.58827478 \n",
      "文學與創新 | 264 train_loss 0.60567331 valid_loss 0.61496538 test_loss 0.58727193 < save\n",
      "文學與創新 | 265 train_loss 0.60528946 valid_loss 0.61516780 test_loss 0.58627069 \n",
      "文學與創新 | 266 train_loss 0.60499555 valid_loss 0.61444271 test_loss 0.58564138 < save\n",
      "文學與創新 | 267 train_loss 0.60552329 valid_loss 0.61550492 test_loss 0.58652490 \n",
      "文學與創新 | 268 train_loss 0.60506970 valid_loss 0.61464167 test_loss 0.58519703 \n",
      "文學與創新 | 269 train_loss 0.60501045 valid_loss 0.61457264 test_loss 0.58573490 \n",
      "文學與創新 | 270 train_loss 0.60480523 valid_loss 0.61485231 test_loss 0.58497953 \n",
      "文學與創新 | 271 train_loss 0.60731435 valid_loss 0.61680835 test_loss 0.58989966 \n",
      "文學與創新 | 272 train_loss 0.60595751 valid_loss 0.61498839 test_loss 0.58820063 \n",
      "文學與創新 | 273 train_loss 0.61019874 valid_loss 0.62081718 test_loss 0.58692241 \n",
      "文學與創新 | 274 train_loss 0.60911357 valid_loss 0.61984086 test_loss 0.58567005 \n",
      "文學與創新 | 275 train_loss 0.60472023 valid_loss 0.61446369 test_loss 0.58528948 \n",
      "文學與創新 | 276 train_loss 0.60701853 valid_loss 0.61790538 test_loss 0.58472240 \n",
      "文學與創新 | 277 train_loss 0.60487962 valid_loss 0.61438847 test_loss 0.58617771 < save\n",
      "文學與創新 | 278 train_loss 0.60587239 valid_loss 0.61650729 test_loss 0.58436531 \n",
      "文學與創新 | 279 train_loss 0.60563135 valid_loss 0.61628741 test_loss 0.58431429 \n",
      "文學與創新 | 280 train_loss 0.60461813 valid_loss 0.61490810 test_loss 0.58430141 \n",
      "文學與創新 | 281 train_loss 0.60596991 valid_loss 0.61646318 test_loss 0.58425468 \n",
      "文學與創新 | 282 train_loss 0.60468841 valid_loss 0.61441547 test_loss 0.58465445 \n",
      "文學與創新 | 283 train_loss 0.60464644 valid_loss 0.61438406 test_loss 0.58470297 < save\n",
      "文學與創新 | 284 train_loss 0.60477614 valid_loss 0.61430484 test_loss 0.58634251 < save\n",
      "文學與創新 | 285 train_loss 0.60580152 valid_loss 0.61540312 test_loss 0.58767730 \n",
      "文學與創新 | 286 train_loss 0.60593539 valid_loss 0.61680549 test_loss 0.58391684 \n",
      "文學與創新 | 287 train_loss 0.60553241 valid_loss 0.61551988 test_loss 0.58707035 \n",
      "文學與創新 | 288 train_loss 0.60490692 valid_loss 0.61511403 test_loss 0.58399570 \n",
      "文學與創新 | 289 train_loss 0.60475385 valid_loss 0.61470735 test_loss 0.58445805 \n",
      "文學與創新 | 290 train_loss 0.60570127 valid_loss 0.61496454 test_loss 0.58829880 \n",
      "文學與創新 | 291 train_loss 0.60435504 valid_loss 0.61436319 test_loss 0.58532941 \n",
      "文學與創新 | 292 train_loss 0.60531670 valid_loss 0.61504900 test_loss 0.58726883 \n",
      "文學與創新 | 293 train_loss 0.61258727 valid_loss 0.62432790 test_loss 0.58789635 \n",
      "文學與創新 | 294 train_loss 0.60418952 valid_loss 0.61456335 test_loss 0.58437628 \n",
      "文學與創新 | 295 train_loss 0.60433227 valid_loss 0.61449242 test_loss 0.58511013 \n",
      "文學與創新 | 296 train_loss 0.60567933 valid_loss 0.61508387 test_loss 0.58830452 \n",
      "文學與創新 | 297 train_loss 0.60412472 valid_loss 0.61401057 test_loss 0.58513033 < save\n",
      "文學與創新 | 298 train_loss 0.60455078 valid_loss 0.61420530 test_loss 0.58632374 \n",
      "文學與創新 | 299 train_loss 0.60400587 valid_loss 0.61407965 test_loss 0.58423555 \n",
      "文學與創新 | 300 train_loss 0.60711235 valid_loss 0.61679506 test_loss 0.58951271 \n",
      "文學與創新 | 301 train_loss 0.60876244 valid_loss 0.61783993 test_loss 0.59235632 \n",
      "文學與創新 | 302 train_loss 0.60597843 valid_loss 0.61671102 test_loss 0.58401108 \n",
      "文學與創新 | 303 train_loss 0.60431606 valid_loss 0.61389631 test_loss 0.58577454 < save\n",
      "文學與創新 | 304 train_loss 0.60388303 valid_loss 0.61422187 test_loss 0.58432174 \n",
      "文學與創新 | 305 train_loss 0.60445523 valid_loss 0.61527914 test_loss 0.58358771 \n",
      "文學與創新 | 306 train_loss 0.60478288 valid_loss 0.61462486 test_loss 0.58653879 \n",
      "文學與創新 | 307 train_loss 0.60702360 valid_loss 0.61797953 test_loss 0.58492517 \n",
      "文學與創新 | 308 train_loss 0.60573912 valid_loss 0.61488688 test_loss 0.58880597 \n",
      "文學與創新 | 309 train_loss 0.60562432 valid_loss 0.61506492 test_loss 0.58828753 \n",
      "文學與創新 | 310 train_loss 0.60463047 valid_loss 0.61436194 test_loss 0.58461654 \n",
      "文學與創新 | 311 train_loss 0.60372907 valid_loss 0.61354411 test_loss 0.58459097 < save\n",
      "文學與創新 | 312 train_loss 0.60502356 valid_loss 0.61573851 test_loss 0.58363813 \n",
      "文學與創新 | 313 train_loss 0.60381055 valid_loss 0.61364257 test_loss 0.58447886 \n",
      "文學與創新 | 314 train_loss 0.60367417 valid_loss 0.61363584 test_loss 0.58464897 \n",
      "文學與創新 | 315 train_loss 0.60382861 valid_loss 0.61407310 test_loss 0.58469921 \n",
      "文學與創新 | 316 train_loss 0.60402656 valid_loss 0.61477691 test_loss 0.58341664 \n",
      "文學與創新 | 317 train_loss 0.60460436 valid_loss 0.61437267 test_loss 0.58664405 \n",
      "文學與創新 | 318 train_loss 0.60508007 valid_loss 0.61468035 test_loss 0.58752245 \n",
      "文學與創新 | 319 train_loss 0.60364944 valid_loss 0.61361194 test_loss 0.58502185 \n",
      "文學與創新 | 320 train_loss 0.60398924 valid_loss 0.61399877 test_loss 0.58544874 \n",
      "文學與創新 | 321 train_loss 0.60368013 valid_loss 0.61341530 test_loss 0.58542830 < save\n",
      "文學與創新 | 322 train_loss 0.60608721 valid_loss 0.61585629 test_loss 0.58862025 \n",
      "文學與創新 | 323 train_loss 0.60354447 valid_loss 0.61321074 test_loss 0.58495468 < save\n",
      "文學與創新 | 324 train_loss 0.60346848 valid_loss 0.61325336 test_loss 0.58449346 \n",
      "文學與創新 | 325 train_loss 0.60459667 valid_loss 0.61534709 test_loss 0.58340371 \n",
      "文學與創新 | 326 train_loss 0.60339379 valid_loss 0.61355615 test_loss 0.58399898 \n",
      "文學與創新 | 327 train_loss 0.60513687 valid_loss 0.61540914 test_loss 0.58674765 \n",
      "文學與創新 | 328 train_loss 0.60529196 valid_loss 0.61569250 test_loss 0.58415306 \n",
      "文學與創新 | 329 train_loss 0.60651517 valid_loss 0.61582828 test_loss 0.59001625 \n",
      "文學與創新 | 330 train_loss 0.60578144 valid_loss 0.61638016 test_loss 0.58403307 \n",
      "文學與創新 | 331 train_loss 0.60504985 valid_loss 0.61482334 test_loss 0.58772153 \n",
      "文學與創新 | 332 train_loss 0.60346323 valid_loss 0.61373508 test_loss 0.58356637 \n",
      "文學與創新 | 333 train_loss 0.60386324 valid_loss 0.61433613 test_loss 0.58322513 \n",
      "文學與創新 | 334 train_loss 0.60962206 valid_loss 0.62079692 test_loss 0.58632851 \n",
      "文學與創新 | 335 train_loss 0.60419202 valid_loss 0.61497456 test_loss 0.58317685 \n",
      "文學與創新 | 336 train_loss 0.60384202 valid_loss 0.61332417 test_loss 0.58570081 \n",
      "文學與創新 | 337 train_loss 0.60335207 valid_loss 0.61360407 test_loss 0.58332092 \n",
      "文學與創新 | 338 train_loss 0.60610205 valid_loss 0.61675745 test_loss 0.58431822 \n",
      "文學與創新 | 339 train_loss 0.60671580 valid_loss 0.61609453 test_loss 0.59025186 \n",
      "文學與創新 | 340 train_loss 0.60479385 valid_loss 0.61421174 test_loss 0.58775449 \n",
      "文學與創新 | 341 train_loss 0.60358083 valid_loss 0.61313838 test_loss 0.58548188 < save\n",
      "文學與創新 | 342 train_loss 0.60422719 valid_loss 0.61432058 test_loss 0.58337849 \n",
      "文學與創新 | 343 train_loss 0.60433811 valid_loss 0.61472899 test_loss 0.58381146 \n",
      "文學與創新 | 344 train_loss 0.60332012 valid_loss 0.61292714 test_loss 0.58394086 < save\n",
      "文學與創新 | 345 train_loss 0.60321891 valid_loss 0.61267614 test_loss 0.58423650 < save\n",
      "文學與創新 | 346 train_loss 0.60528886 valid_loss 0.61581618 test_loss 0.58367276 \n",
      "文學與創新 | 347 train_loss 0.60834491 valid_loss 0.61722583 test_loss 0.59265053 \n",
      "文學與創新 | 348 train_loss 0.60403711 valid_loss 0.61363137 test_loss 0.58612162 \n",
      "文學與創新 | 349 train_loss 0.60501349 valid_loss 0.61411929 test_loss 0.58803266 \n",
      "文學與創新 | 350 train_loss 0.60450077 valid_loss 0.61505359 test_loss 0.58356422 \n",
      "文學與創新 | 351 train_loss 0.60374749 valid_loss 0.61322474 test_loss 0.58568573 \n",
      "文學與創新 | 352 train_loss 0.60617149 valid_loss 0.61696589 test_loss 0.58418846 \n",
      "文學與創新 | 353 train_loss 0.60488182 valid_loss 0.61414170 test_loss 0.58781093 \n",
      "文學與創新 | 354 train_loss 0.60344231 valid_loss 0.61408657 test_loss 0.58268577 \n",
      "文學與創新 | 355 train_loss 0.60294461 valid_loss 0.61276799 test_loss 0.58422869 \n",
      "文學與創新 | 356 train_loss 0.60294282 valid_loss 0.61330670 test_loss 0.58325493 \n",
      "文學與創新 | 357 train_loss 0.60412508 valid_loss 0.61373401 test_loss 0.58675712 \n",
      "文學與創新 | 358 train_loss 0.60414952 valid_loss 0.61486781 test_loss 0.58302236 \n",
      "文學與創新 | 359 train_loss 0.60464972 valid_loss 0.61410016 test_loss 0.58758074 \n",
      "文學與創新 | 360 train_loss 0.60287243 valid_loss 0.61268610 test_loss 0.58345717 \n",
      "文學與創新 | 361 train_loss 0.60372716 valid_loss 0.61353451 test_loss 0.58584404 \n",
      "文學與創新 | 362 train_loss 0.60287547 valid_loss 0.61275136 test_loss 0.58342081 \n",
      "文學與創新 | 363 train_loss 0.60292584 valid_loss 0.61311334 test_loss 0.58297908 \n",
      "文學與創新 | 364 train_loss 0.60398400 valid_loss 0.61488467 test_loss 0.58297133 \n",
      "文學與創新 | 365 train_loss 0.60300809 valid_loss 0.61303812 test_loss 0.58453816 \n",
      "文學與創新 | 366 train_loss 0.60317075 valid_loss 0.61351424 test_loss 0.58299243 \n",
      "文學與創新 | 367 train_loss 0.60277748 valid_loss 0.61264527 test_loss 0.58384860 < save\n",
      "文學與創新 | 368 train_loss 0.60328925 valid_loss 0.61358744 test_loss 0.58300114 \n",
      "文學與創新 | 369 train_loss 0.60728145 valid_loss 0.61727589 test_loss 0.58538377 \n",
      "文學與創新 | 370 train_loss 0.60272777 valid_loss 0.61232477 test_loss 0.58391660 < save\n",
      "文學與創新 | 371 train_loss 0.60273141 valid_loss 0.61241198 test_loss 0.58359581 \n",
      "文學與創新 | 372 train_loss 0.60508329 valid_loss 0.61382711 test_loss 0.58865148 \n",
      "文學與創新 | 373 train_loss 0.60461891 valid_loss 0.61385286 test_loss 0.58771259 \n",
      "文學與創新 | 374 train_loss 0.60263705 valid_loss 0.61256015 test_loss 0.58366197 \n",
      "文學與創新 | 375 train_loss 0.60259664 valid_loss 0.61252320 test_loss 0.58346474 \n",
      "文學與創新 | 376 train_loss 0.60383952 valid_loss 0.61382705 test_loss 0.58583874 \n",
      "文學與創新 | 377 train_loss 0.60320115 valid_loss 0.61371875 test_loss 0.58265638 \n",
      "文學與創新 | 378 train_loss 0.60369915 valid_loss 0.61253405 test_loss 0.58678246 \n",
      "文學與創新 | 379 train_loss 0.60259175 valid_loss 0.61231029 test_loss 0.58369935 < save\n",
      "文學與創新 | 380 train_loss 0.60719800 valid_loss 0.61630702 test_loss 0.59113133 \n",
      "文學與創新 | 381 train_loss 0.60273224 valid_loss 0.61255395 test_loss 0.58325183 \n",
      "文學與創新 | 382 train_loss 0.60251826 valid_loss 0.61257958 test_loss 0.58322102 \n",
      "文學與創新 | 383 train_loss 0.60288632 valid_loss 0.61230266 test_loss 0.58371496 < save\n",
      "文學與創新 | 384 train_loss 0.60299259 valid_loss 0.61330992 test_loss 0.58284622 \n",
      "文學與創新 | 385 train_loss 0.60409319 valid_loss 0.61357826 test_loss 0.58727485 \n",
      "文學與創新 | 386 train_loss 0.60307604 valid_loss 0.61304885 test_loss 0.58312291 \n",
      "文學與創新 | 387 train_loss 0.60525686 valid_loss 0.61484748 test_loss 0.58820641 \n",
      "文學與創新 | 388 train_loss 0.60473043 valid_loss 0.61522317 test_loss 0.58358788 \n",
      "文學與創新 | 389 train_loss 0.60257488 valid_loss 0.61200380 test_loss 0.58447987 < save\n",
      "文學與創新 | 390 train_loss 0.60433388 valid_loss 0.61374903 test_loss 0.58761412 \n",
      "文學與創新 | 391 train_loss 0.60296535 valid_loss 0.61298329 test_loss 0.58454531 \n",
      "文學與創新 | 392 train_loss 0.60249150 valid_loss 0.61200559 test_loss 0.58404285 \n",
      "文學與創新 | 393 train_loss 0.60410619 valid_loss 0.61395693 test_loss 0.58673114 \n",
      "文學與創新 | 394 train_loss 0.60270065 valid_loss 0.61227709 test_loss 0.58482099 \n",
      "文學與創新 | 395 train_loss 0.60268950 valid_loss 0.61188298 test_loss 0.58541334 < save\n",
      "文學與創新 | 396 train_loss 0.60390288 valid_loss 0.61357409 test_loss 0.58655548 \n",
      "文學與創新 | 397 train_loss 0.60308915 valid_loss 0.61251712 test_loss 0.58560687 \n",
      "文學與創新 | 398 train_loss 0.60263950 valid_loss 0.61206591 test_loss 0.58502287 \n",
      "文學與創新 | 399 train_loss 0.60538769 valid_loss 0.61422443 test_loss 0.58956921 \n",
      "文學與創新 | 400 train_loss 0.60426325 valid_loss 0.61321938 test_loss 0.58804047 \n",
      "文學與創新 | 401 train_loss 0.60245717 valid_loss 0.61228877 test_loss 0.58262366 \n",
      "文學與創新 | 402 train_loss 0.60404086 valid_loss 0.61454058 test_loss 0.58297461 \n",
      "文學與創新 | 403 train_loss 0.60548925 valid_loss 0.61683285 test_loss 0.58352000 \n",
      "文學與創新 | 404 train_loss 0.60463285 valid_loss 0.61511898 test_loss 0.58359730 \n",
      "文學與創新 | 405 train_loss 0.60617781 valid_loss 0.61543095 test_loss 0.58997256 \n",
      "文學與創新 | 406 train_loss 0.60437781 valid_loss 0.61488718 test_loss 0.58326703 \n",
      "文學與創新 | 407 train_loss 0.60373282 valid_loss 0.61331135 test_loss 0.58645999 \n",
      "文學與創新 | 408 train_loss 0.60241157 valid_loss 0.61177838 test_loss 0.58447939 < save\n",
      "文學與創新 | 409 train_loss 0.60216528 valid_loss 0.61167896 test_loss 0.58339542 < save\n",
      "文學與創新 | 410 train_loss 0.60243106 valid_loss 0.61180627 test_loss 0.58478117 \n",
      "文學與創新 | 411 train_loss 0.60249102 valid_loss 0.61197865 test_loss 0.58499134 \n",
      "文學與創新 | 412 train_loss 0.60558057 valid_loss 0.61476994 test_loss 0.58910102 \n",
      "文學與創新 | 413 train_loss 0.60285872 valid_loss 0.61269706 test_loss 0.58275360 \n",
      "文學與創新 | 414 train_loss 0.60213703 valid_loss 0.61176294 test_loss 0.58384764 \n",
      "文學與創新 | 415 train_loss 0.60332680 valid_loss 0.61371875 test_loss 0.58293688 \n",
      "文學與創新 | 416 train_loss 0.60219657 valid_loss 0.61213684 test_loss 0.58270735 \n",
      "文學與創新 | 417 train_loss 0.60446972 valid_loss 0.61529386 test_loss 0.58303398 \n",
      "文學與創新 | 418 train_loss 0.60397720 valid_loss 0.61355048 test_loss 0.58688807 \n",
      "文學與創新 | 419 train_loss 0.60379493 valid_loss 0.61316949 test_loss 0.58680284 \n",
      "文學與創新 | 420 train_loss 0.60233605 valid_loss 0.61247474 test_loss 0.58343619 \n",
      "文學與創新 | 421 train_loss 0.60209239 valid_loss 0.61239916 test_loss 0.58294511 \n",
      "文學與創新 | 422 train_loss 0.60330141 valid_loss 0.61247438 test_loss 0.58664900 \n",
      "文學與創新 | 423 train_loss 0.60326058 valid_loss 0.61283314 test_loss 0.58603340 \n",
      "文學與創新 | 424 train_loss 0.60216779 valid_loss 0.61202645 test_loss 0.58377320 \n",
      "文學與創新 | 425 train_loss 0.60225862 valid_loss 0.61140841 test_loss 0.58379269 < save\n",
      "文學與創新 | 426 train_loss 0.60197401 valid_loss 0.61180776 test_loss 0.58267361 \n",
      "文學與創新 | 427 train_loss 0.60235542 valid_loss 0.61260384 test_loss 0.58236420 \n",
      "文學與創新 | 428 train_loss 0.60192072 valid_loss 0.61182135 test_loss 0.58275431 \n",
      "文學與創新 | 429 train_loss 0.60260272 valid_loss 0.61260676 test_loss 0.58449423 \n",
      "文學與創新 | 430 train_loss 0.60186952 valid_loss 0.61208332 test_loss 0.58291650 \n",
      "文學與創新 | 431 train_loss 0.60197842 valid_loss 0.61164021 test_loss 0.58285695 \n",
      "文學與創新 | 432 train_loss 0.60386389 valid_loss 0.61314189 test_loss 0.58696592 \n",
      "文學與創新 | 433 train_loss 0.60186929 valid_loss 0.61134976 test_loss 0.58321583 < save\n",
      "文學與創新 | 434 train_loss 0.60311174 valid_loss 0.61315560 test_loss 0.58530021 \n",
      "文學與創新 | 435 train_loss 0.60214299 valid_loss 0.61121833 test_loss 0.58463126 < save\n",
      "文學與創新 | 436 train_loss 0.60235995 valid_loss 0.61214513 test_loss 0.58265066 \n",
      "文學與創新 | 437 train_loss 0.60190117 valid_loss 0.61176240 test_loss 0.58381808 \n",
      "文學與創新 | 438 train_loss 0.60243505 valid_loss 0.61300862 test_loss 0.58283180 \n",
      "文學與創新 | 439 train_loss 0.60367626 valid_loss 0.61379075 test_loss 0.58293843 \n",
      "文學與創新 | 440 train_loss 0.60264623 valid_loss 0.61278075 test_loss 0.58266062 \n",
      "文學與創新 | 441 train_loss 0.60212320 valid_loss 0.61240149 test_loss 0.58235323 \n",
      "文學與創新 | 442 train_loss 0.60599083 valid_loss 0.61712110 test_loss 0.58384752 \n",
      "文學與創新 | 443 train_loss 0.60436708 valid_loss 0.61506468 test_loss 0.58304238 \n",
      "文學與創新 | 444 train_loss 0.60322785 valid_loss 0.61266577 test_loss 0.58623379 \n",
      "文學與創新 | 445 train_loss 0.60477692 valid_loss 0.61530191 test_loss 0.58357120 \n",
      "文學與創新 | 446 train_loss 0.60228848 valid_loss 0.61236912 test_loss 0.58223784 \n",
      "文學與創新 | 447 train_loss 0.60441250 valid_loss 0.61507887 test_loss 0.58295250 \n",
      "文學與創新 | 448 train_loss 0.60205787 valid_loss 0.61253530 test_loss 0.58236521 \n",
      "文學與創新 | 449 train_loss 0.60172868 valid_loss 0.61207610 test_loss 0.58229673 \n",
      "文學與創新 | 450 train_loss 0.60302025 valid_loss 0.61276126 test_loss 0.58547372 \n",
      "文學與創新 | 451 train_loss 0.60171807 valid_loss 0.61159760 test_loss 0.58344084 \n",
      "文學與創新 | 452 train_loss 0.60226077 valid_loss 0.61196560 test_loss 0.58297139 \n",
      "文學與創新 | 453 train_loss 0.60172236 valid_loss 0.61206555 test_loss 0.58252978 \n",
      "文學與創新 | 454 train_loss 0.60154444 valid_loss 0.61106193 test_loss 0.58332086 < save\n",
      "文學與創新 | 455 train_loss 0.60212904 valid_loss 0.61262047 test_loss 0.58199096 \n",
      "文學與創新 | 456 train_loss 0.60155684 valid_loss 0.61155075 test_loss 0.58297706 \n",
      "文學與創新 | 457 train_loss 0.60175771 valid_loss 0.61125600 test_loss 0.58413517 \n",
      "文學與創新 | 458 train_loss 0.60357141 valid_loss 0.61442846 test_loss 0.58246529 \n",
      "文學與創新 | 459 train_loss 0.60455620 valid_loss 0.61490262 test_loss 0.58367258 \n",
      "文學與創新 | 460 train_loss 0.60224974 valid_loss 0.61220330 test_loss 0.58258337 \n",
      "文學與創新 | 461 train_loss 0.60187775 valid_loss 0.61185896 test_loss 0.58233297 \n",
      "文學與創新 | 462 train_loss 0.60181105 valid_loss 0.61129922 test_loss 0.58402652 \n",
      "文學與創新 | 463 train_loss 0.60147095 valid_loss 0.61143196 test_loss 0.58266139 \n",
      "文學與創新 | 464 train_loss 0.60278523 valid_loss 0.61280656 test_loss 0.58275092 \n",
      "文學與創新 | 465 train_loss 0.60171425 valid_loss 0.61122268 test_loss 0.58405459 \n",
      "文學與創新 | 466 train_loss 0.60331100 valid_loss 0.61297590 test_loss 0.58368486 \n",
      "文學與創新 | 467 train_loss 0.60142261 valid_loss 0.61097682 test_loss 0.58307683 < save\n",
      "文學與創新 | 468 train_loss 0.60164499 valid_loss 0.61132246 test_loss 0.58349311 \n",
      "文學與創新 | 469 train_loss 0.60434270 valid_loss 0.61396325 test_loss 0.58745593 \n",
      "文學與創新 | 470 train_loss 0.60204357 valid_loss 0.61121559 test_loss 0.58508980 \n",
      "文學與創新 | 471 train_loss 0.60192692 valid_loss 0.61164993 test_loss 0.58249176 \n",
      "文學與創新 | 472 train_loss 0.60322845 valid_loss 0.61355704 test_loss 0.58266467 \n",
      "文學與創新 | 473 train_loss 0.60325450 valid_loss 0.61368990 test_loss 0.58273566 \n",
      "文學與創新 | 474 train_loss 0.60227627 valid_loss 0.61140990 test_loss 0.58517659 \n",
      "文學與創新 | 475 train_loss 0.60288131 valid_loss 0.61354357 test_loss 0.58205986 \n",
      "文學與創新 | 476 train_loss 0.60162097 valid_loss 0.61138129 test_loss 0.58390975 \n",
      "文學與創新 | 477 train_loss 0.60187805 valid_loss 0.61194670 test_loss 0.58205700 \n",
      "文學與創新 | 478 train_loss 0.60162944 valid_loss 0.61113417 test_loss 0.58424872 \n",
      "文學與創新 | 479 train_loss 0.60208243 valid_loss 0.61181843 test_loss 0.58432305 \n",
      "文學與創新 | 480 train_loss 0.60127646 valid_loss 0.61080909 test_loss 0.58324790 < save\n",
      "文學與創新 | 481 train_loss 0.60349381 valid_loss 0.61265075 test_loss 0.58716327 \n",
      "文學與創新 | 482 train_loss 0.60164309 valid_loss 0.61151361 test_loss 0.58211541 \n",
      "文學與創新 | 483 train_loss 0.60165513 valid_loss 0.61196351 test_loss 0.58209801 \n",
      "文學與創新 | 484 train_loss 0.60141712 valid_loss 0.61090934 test_loss 0.58287710 \n",
      "文學與創新 | 485 train_loss 0.60179788 valid_loss 0.61177683 test_loss 0.58234984 \n",
      "文學與創新 | 486 train_loss 0.60138625 valid_loss 0.61139923 test_loss 0.58230418 \n",
      "文學與創新 | 487 train_loss 0.60174209 valid_loss 0.61170149 test_loss 0.58255702 \n",
      "文學與創新 | 488 train_loss 0.60132205 valid_loss 0.61046880 test_loss 0.58368814 < save\n",
      "文學與創新 | 489 train_loss 0.60214490 valid_loss 0.61269665 test_loss 0.58206916 \n",
      "文學與創新 | 490 train_loss 0.60119319 valid_loss 0.61065429 test_loss 0.58304250 \n",
      "文學與創新 | 491 train_loss 0.60466933 valid_loss 0.61532003 test_loss 0.58344126 \n",
      "文學與創新 | 492 train_loss 0.60199630 valid_loss 0.61173505 test_loss 0.58275861 \n",
      "文學與創新 | 493 train_loss 0.60119975 valid_loss 0.61084783 test_loss 0.58291006 \n",
      "文學與創新 | 494 train_loss 0.60165882 valid_loss 0.61163670 test_loss 0.58318150 \n",
      "文學與創新 | 495 train_loss 0.60175169 valid_loss 0.61100054 test_loss 0.58463728 \n",
      "文學與創新 | 496 train_loss 0.60137427 valid_loss 0.61100692 test_loss 0.58250719 \n",
      "文學與創新 | 497 train_loss 0.60445648 valid_loss 0.61473680 test_loss 0.58367974 \n",
      "文學與創新 | 498 train_loss 0.60328096 valid_loss 0.61403257 test_loss 0.58230901 \n",
      "文學與創新 | 499 train_loss 0.60144848 valid_loss 0.61089319 test_loss 0.58386391 \n",
      "文學與創新 | 500 train_loss 0.60246402 valid_loss 0.61190337 test_loss 0.58569598 \n",
      "文學與創新 | 501 train_loss 0.60487014 valid_loss 0.61533499 test_loss 0.58313572 \n",
      "文學與創新 | 502 train_loss 0.60151654 valid_loss 0.61088544 test_loss 0.58259743 \n",
      "文學與創新 | 503 train_loss 0.60139382 valid_loss 0.61080980 test_loss 0.58389306 \n",
      "文學與創新 | 504 train_loss 0.60107964 valid_loss 0.61067200 test_loss 0.58229226 \n",
      "文學與創新 | 505 train_loss 0.60114175 valid_loss 0.61080718 test_loss 0.58312553 \n",
      "文學與創新 | 506 train_loss 0.60957426 valid_loss 0.61840719 test_loss 0.59451616 \n",
      "文學與創新 | 507 train_loss 0.60112745 valid_loss 0.61063695 test_loss 0.58323014 \n",
      "文學與創新 | 508 train_loss 0.60162455 valid_loss 0.61060607 test_loss 0.58475155 \n",
      "文學與創新 | 509 train_loss 0.60621315 valid_loss 0.61496568 test_loss 0.59109390 \n",
      "文學與創新 | 510 train_loss 0.60117173 valid_loss 0.61072654 test_loss 0.58280289 \n",
      "文學與創新 | 511 train_loss 0.60412121 valid_loss 0.61492193 test_loss 0.58254755 \n",
      "文學與創新 | 512 train_loss 0.60101694 valid_loss 0.61054438 test_loss 0.58306485 \n",
      "文學與創新 | 513 train_loss 0.60165328 valid_loss 0.61062175 test_loss 0.58499753 \n",
      "文學與創新 | 514 train_loss 0.60491586 valid_loss 0.61424971 test_loss 0.58847708 \n",
      "文學與創新 | 515 train_loss 0.60803473 valid_loss 0.61851901 test_loss 0.58577204 \n",
      "文學與創新 | 516 train_loss 0.60107386 valid_loss 0.61053157 test_loss 0.58302319 \n",
      "文學與創新 | 517 train_loss 0.60141003 valid_loss 0.61148053 test_loss 0.58184975 \n",
      "文學與創新 | 518 train_loss 0.60130572 valid_loss 0.61018926 test_loss 0.58424985 < save\n",
      "文學與創新 | 519 train_loss 0.60090882 valid_loss 0.61041957 test_loss 0.58253020 \n",
      "文學與創新 | 520 train_loss 0.60121596 valid_loss 0.61030924 test_loss 0.58406508 \n",
      "文學與創新 | 521 train_loss 0.60191065 valid_loss 0.61089063 test_loss 0.58528727 \n",
      "文學與創新 | 522 train_loss 0.60379261 valid_loss 0.61262351 test_loss 0.58748788 \n",
      "文學與創新 | 523 train_loss 0.60130519 valid_loss 0.61086428 test_loss 0.58341599 \n",
      "文學與創新 | 524 train_loss 0.60182422 valid_loss 0.61184710 test_loss 0.58193159 \n",
      "文學與創新 | 525 train_loss 0.60114622 valid_loss 0.61109620 test_loss 0.58187193 \n",
      "文學與創新 | 526 train_loss 0.60183412 valid_loss 0.61235231 test_loss 0.58164114 \n",
      "文學與創新 | 527 train_loss 0.60176915 valid_loss 0.61152363 test_loss 0.58400571 \n",
      "文學與創新 | 528 train_loss 0.60111117 valid_loss 0.61082137 test_loss 0.58286572 \n",
      "文學與創新 | 529 train_loss 0.60198551 valid_loss 0.61222035 test_loss 0.58207619 \n",
      "文學與創新 | 530 train_loss 0.60284328 valid_loss 0.61232448 test_loss 0.58553666 \n",
      "文學與創新 | 531 train_loss 0.60088319 valid_loss 0.61033845 test_loss 0.58220619 \n",
      "文學與創新 | 532 train_loss 0.60544735 valid_loss 0.61614943 test_loss 0.58395332 \n",
      "文學與創新 | 533 train_loss 0.60294425 valid_loss 0.61215818 test_loss 0.58658415 \n",
      "文學與創新 | 534 train_loss 0.60115582 valid_loss 0.61066025 test_loss 0.58261377 \n",
      "文學與創新 | 535 train_loss 0.60135716 valid_loss 0.61110437 test_loss 0.58293486 \n",
      "文學與創新 | 536 train_loss 0.61003947 valid_loss 0.62040079 test_loss 0.58707231 \n",
      "文學與創新 | 537 train_loss 0.60190660 valid_loss 0.61087078 test_loss 0.58485162 \n",
      "文學與創新 | 538 train_loss 0.60237575 valid_loss 0.61211771 test_loss 0.58261079 \n",
      "文學與創新 | 539 train_loss 0.60163194 valid_loss 0.61087191 test_loss 0.58441240 \n",
      "文學與創新 | 540 train_loss 0.60172307 valid_loss 0.61094660 test_loss 0.58254147 \n",
      "文學與創新 | 541 train_loss 0.60106039 valid_loss 0.61066037 test_loss 0.58203167 \n",
      "文學與創新 | 542 train_loss 0.60080618 valid_loss 0.61032718 test_loss 0.58223087 \n",
      "文學與創新 | 543 train_loss 0.60268730 valid_loss 0.61210686 test_loss 0.58322340 \n",
      "文學與創新 | 544 train_loss 0.60224265 valid_loss 0.61137944 test_loss 0.58549619 \n",
      "文學與創新 | 545 train_loss 0.60096955 valid_loss 0.61001319 test_loss 0.58356595 < save\n",
      "文學與創新 | 546 train_loss 0.60106498 valid_loss 0.61033714 test_loss 0.58344722 \n",
      "文學與創新 | 547 train_loss 0.60077721 valid_loss 0.61029863 test_loss 0.58261752 \n",
      "文學與創新 | 548 train_loss 0.60221833 valid_loss 0.61179316 test_loss 0.58264875 \n",
      "文學與創新 | 549 train_loss 0.60077238 valid_loss 0.61051059 test_loss 0.58242369 \n",
      "文學與創新 | 550 train_loss 0.60127330 valid_loss 0.61074978 test_loss 0.58396685 \n",
      "文學與創新 | 551 train_loss 0.60082728 valid_loss 0.61079878 test_loss 0.58206302 \n",
      "文學與創新 | 552 train_loss 0.60142982 valid_loss 0.61169475 test_loss 0.58250445 \n",
      "文學與創新 | 553 train_loss 0.60086477 valid_loss 0.61034429 test_loss 0.58279383 \n",
      "文學與創新 | 554 train_loss 0.60116357 valid_loss 0.61007971 test_loss 0.58414048 \n",
      "文學與創新 | 555 train_loss 0.60102481 valid_loss 0.61065459 test_loss 0.58192813 \n",
      "文學與創新 | 556 train_loss 0.60180116 valid_loss 0.61141604 test_loss 0.58280700 \n",
      "文學與創新 | 557 train_loss 0.60148895 valid_loss 0.61132860 test_loss 0.58193815 \n",
      "文學與創新 | 558 train_loss 0.60273010 valid_loss 0.61299944 test_loss 0.58221847 \n",
      "文學與創新 | 559 train_loss 0.60145092 valid_loss 0.61049783 test_loss 0.58461297 \n",
      "文學與創新 | 560 train_loss 0.60190600 valid_loss 0.61052936 test_loss 0.58559060 \n",
      "文學與創新 | 561 train_loss 0.60109395 valid_loss 0.61069465 test_loss 0.58320361 \n",
      "文學與創新 | 562 train_loss 0.60087830 valid_loss 0.61027932 test_loss 0.58217627 \n",
      "文學與創新 | 563 train_loss 0.60356200 valid_loss 0.61201686 test_loss 0.58810222 \n",
      "文學與創新 | 564 train_loss 0.60517347 valid_loss 0.61531657 test_loss 0.58392674 \n",
      "文學與創新 | 565 train_loss 0.60073543 valid_loss 0.61053789 test_loss 0.58188444 \n",
      "文學與創新 | 566 train_loss 0.60300386 valid_loss 0.61244226 test_loss 0.58622986 \n",
      "文學與創新 | 567 train_loss 0.60072559 valid_loss 0.61054868 test_loss 0.58255553 \n",
      "文學與創新 | 568 train_loss 0.60168296 valid_loss 0.61111689 test_loss 0.58250690 \n",
      "文學與創新 | 569 train_loss 0.60212308 valid_loss 0.61187863 test_loss 0.58240467 \n",
      "文學與創新 | 570 train_loss 0.60078162 valid_loss 0.61050081 test_loss 0.58190417 \n",
      "文學與創新 | 571 train_loss 0.60123014 valid_loss 0.60993445 test_loss 0.58445710 < save\n",
      "文學與創新 | 572 train_loss 0.60098374 valid_loss 0.61020643 test_loss 0.58416128 \n",
      "文學與創新 | 573 train_loss 0.60054183 valid_loss 0.61006278 test_loss 0.58213955 \n",
      "文學與創新 | 574 train_loss 0.60051930 valid_loss 0.60995966 test_loss 0.58220148 \n",
      "文學與創新 | 575 train_loss 0.60172355 valid_loss 0.61176938 test_loss 0.58164519 \n",
      "文學與創新 | 576 train_loss 0.60053933 valid_loss 0.60991257 test_loss 0.58269048 < save\n",
      "文學與創新 | 577 train_loss 0.60107976 valid_loss 0.61066306 test_loss 0.58190346 \n",
      "文學與創新 | 578 train_loss 0.60059530 valid_loss 0.61010498 test_loss 0.58253205 \n",
      "文學與創新 | 579 train_loss 0.60485452 valid_loss 0.61331064 test_loss 0.58980525 \n",
      "文學與創新 | 580 train_loss 0.60944390 valid_loss 0.61865252 test_loss 0.59442657 \n",
      "文學與創新 | 581 train_loss 0.60063344 valid_loss 0.60978037 test_loss 0.58307916 < save\n",
      "文學與創新 | 582 train_loss 0.60101014 valid_loss 0.60992032 test_loss 0.58388782 \n",
      "文學與創新 | 583 train_loss 0.60244596 valid_loss 0.61264390 test_loss 0.58208019 \n",
      "文學與創新 | 584 train_loss 0.60061669 valid_loss 0.60989112 test_loss 0.58322376 \n",
      "文學與創新 | 585 train_loss 0.60074568 valid_loss 0.61054331 test_loss 0.58200234 \n",
      "文學與創新 | 586 train_loss 0.60255712 valid_loss 0.61164874 test_loss 0.58594453 \n",
      "文學與創新 | 587 train_loss 0.60557938 valid_loss 0.61411190 test_loss 0.59040815 \n",
      "文學與創新 | 588 train_loss 0.60055476 valid_loss 0.61013728 test_loss 0.58162427 \n",
      "文學與創新 | 589 train_loss 0.60118413 valid_loss 0.61030304 test_loss 0.58407009 \n",
      "文學與創新 | 590 train_loss 0.60164231 valid_loss 0.61053509 test_loss 0.58493668 \n",
      "文學與創新 | 591 train_loss 0.60233510 valid_loss 0.61148566 test_loss 0.58582997 \n",
      "文學與創新 | 592 train_loss 0.60227376 valid_loss 0.61219501 test_loss 0.58235341 \n",
      "文學與創新 | 593 train_loss 0.60493213 valid_loss 0.61530375 test_loss 0.58358151 \n",
      "文學與創新 | 594 train_loss 0.60090077 valid_loss 0.60978925 test_loss 0.58406913 \n",
      "文學與創新 | 595 train_loss 0.60273528 valid_loss 0.61132866 test_loss 0.58688462 \n",
      "文學與創新 | 596 train_loss 0.60160249 valid_loss 0.61068290 test_loss 0.58292109 \n",
      "文學與創新 | 597 train_loss 0.60047406 valid_loss 0.60990226 test_loss 0.58206046 \n",
      "文學與創新 | 598 train_loss 0.60101187 valid_loss 0.61099893 test_loss 0.58167028 \n",
      "文學與創新 | 599 train_loss 0.60255760 valid_loss 0.61201370 test_loss 0.58602554 \n",
      "文學與創新 | 600 train_loss 0.60087723 valid_loss 0.61088932 test_loss 0.58189315 \n",
      "文學與創新 | 601 train_loss 0.60074812 valid_loss 0.61096752 test_loss 0.58147764 \n",
      "文學與創新 | 602 train_loss 0.60042381 valid_loss 0.61032003 test_loss 0.58170819 \n",
      "文學與創新 | 603 train_loss 0.60039437 valid_loss 0.60976130 test_loss 0.58269769 < save\n",
      "文學與創新 | 604 train_loss 0.60068750 valid_loss 0.61003780 test_loss 0.58304358 \n",
      "文學與創新 | 605 train_loss 0.60054785 valid_loss 0.61029953 test_loss 0.58180058 \n",
      "文學與創新 | 606 train_loss 0.60321993 valid_loss 0.61336136 test_loss 0.58275872 \n",
      "文學與創新 | 607 train_loss 0.60441190 valid_loss 0.61299694 test_loss 0.58894271 \n",
      "文學與創新 | 608 train_loss 0.60092121 valid_loss 0.61001337 test_loss 0.58385324 \n",
      "文學與創新 | 609 train_loss 0.60119420 valid_loss 0.60997999 test_loss 0.58443463 \n",
      "文學與創新 | 610 train_loss 0.60061806 valid_loss 0.61004919 test_loss 0.58211076 \n",
      "文學與創新 | 611 train_loss 0.60177618 valid_loss 0.61137217 test_loss 0.58239257 \n",
      "文學與創新 | 612 train_loss 0.60029233 valid_loss 0.60959941 test_loss 0.58236504 < save\n",
      "文學與創新 | 613 train_loss 0.60212225 valid_loss 0.61161333 test_loss 0.58253914 \n",
      "文學與創新 | 614 train_loss 0.60182333 valid_loss 0.61072689 test_loss 0.58531785 \n",
      "文學與創新 | 615 train_loss 0.60047925 valid_loss 0.61013365 test_loss 0.58215439 \n",
      "文學與創新 | 616 train_loss 0.60060000 valid_loss 0.61015898 test_loss 0.58210838 \n",
      "文學與創新 | 617 train_loss 0.60041744 valid_loss 0.60930759 test_loss 0.58297735 < save\n",
      "文學與創新 | 618 train_loss 0.60025430 valid_loss 0.60975230 test_loss 0.58201456 \n",
      "文學與創新 | 619 train_loss 0.60025901 valid_loss 0.60962534 test_loss 0.58230168 \n",
      "文學與創新 | 620 train_loss 0.60049307 valid_loss 0.60951769 test_loss 0.58321339 \n",
      "文學與創新 | 621 train_loss 0.60052389 valid_loss 0.60966867 test_loss 0.58241487 \n",
      "文學與創新 | 622 train_loss 0.60127205 valid_loss 0.60985309 test_loss 0.58487576 \n",
      "文學與創新 | 623 train_loss 0.60270488 valid_loss 0.61306494 test_loss 0.58193237 \n",
      "文學與創新 | 624 train_loss 0.60027122 valid_loss 0.60974663 test_loss 0.58239180 \n",
      "文學與創新 | 625 train_loss 0.60626984 valid_loss 0.61552048 test_loss 0.59042615 \n",
      "文學與創新 | 626 train_loss 0.61095279 valid_loss 0.61959893 test_loss 0.59672022 \n",
      "文學與創新 | 627 train_loss 0.60068744 valid_loss 0.61020219 test_loss 0.58179235 \n",
      "文學與創新 | 628 train_loss 0.60029858 valid_loss 0.60999125 test_loss 0.58185750 \n",
      "文學與創新 | 629 train_loss 0.60090369 valid_loss 0.60989302 test_loss 0.58386469 \n",
      "文學與創新 | 630 train_loss 0.60172564 valid_loss 0.61036736 test_loss 0.58540267 \n",
      "文學與創新 | 631 train_loss 0.60364068 valid_loss 0.61370313 test_loss 0.58291489 \n",
      "文學與創新 | 632 train_loss 0.60054153 valid_loss 0.61027682 test_loss 0.58261186 \n",
      "文學與創新 | 633 train_loss 0.60021377 valid_loss 0.60962880 test_loss 0.58211714 \n",
      "文學與創新 | 634 train_loss 0.60061854 valid_loss 0.61047137 test_loss 0.58215642 \n",
      "文學與創新 | 635 train_loss 0.60164183 valid_loss 0.61166632 test_loss 0.58156216 \n",
      "文學與創新 | 636 train_loss 0.60358113 valid_loss 0.61368865 test_loss 0.58296394 \n",
      "文學與創新 | 637 train_loss 0.60020757 valid_loss 0.60946226 test_loss 0.58204252 \n",
      "文學與創新 | 638 train_loss 0.60021508 valid_loss 0.60975194 test_loss 0.58177215 \n",
      "文學與創新 | 639 train_loss 0.60122246 valid_loss 0.60983145 test_loss 0.58470935 \n",
      "文學與創新 | 640 train_loss 0.60020161 valid_loss 0.60958850 test_loss 0.58191395 \n",
      "文學與創新 | 641 train_loss 0.60034645 valid_loss 0.61001527 test_loss 0.58242548 \n",
      "文學與創新 | 642 train_loss 0.60060924 valid_loss 0.61008662 test_loss 0.58313888 \n",
      "文學與創新 | 643 train_loss 0.60126805 valid_loss 0.61069036 test_loss 0.58250654 \n",
      "文學與創新 | 644 train_loss 0.60044152 valid_loss 0.60922074 test_loss 0.58362478 < save\n",
      "文學與創新 | 645 train_loss 0.60388094 valid_loss 0.61380732 test_loss 0.58320493 \n",
      "文學與創新 | 646 train_loss 0.60083485 valid_loss 0.61009872 test_loss 0.58417422 \n",
      "文學與創新 | 647 train_loss 0.60325617 valid_loss 0.61221766 test_loss 0.58741528 \n",
      "文學與創新 | 648 train_loss 0.60052043 valid_loss 0.61008167 test_loss 0.58178514 \n",
      "文學與創新 | 649 train_loss 0.60075849 valid_loss 0.61036777 test_loss 0.58194858 \n",
      "文學與創新 | 650 train_loss 0.60120571 valid_loss 0.61050582 test_loss 0.58411109 \n",
      "文學與創新 | 651 train_loss 0.60017163 valid_loss 0.60962355 test_loss 0.58235526 \n",
      "文學與創新 | 652 train_loss 0.60015440 valid_loss 0.60978824 test_loss 0.58161855 \n",
      "文學與創新 | 653 train_loss 0.60176748 valid_loss 0.61115271 test_loss 0.58506107 \n",
      "文學與創新 | 654 train_loss 0.60016251 valid_loss 0.60936379 test_loss 0.58216250 \n",
      "文學與創新 | 655 train_loss 0.60223454 valid_loss 0.61218065 test_loss 0.58225352 \n",
      "文學與創新 | 656 train_loss 0.60018116 valid_loss 0.60947704 test_loss 0.58249402 \n",
      "文學與創新 | 657 train_loss 0.60005760 valid_loss 0.60933638 test_loss 0.58200026 \n",
      "文學與創新 | 658 train_loss 0.60012877 valid_loss 0.60964453 test_loss 0.58159906 \n",
      "文學與創新 | 659 train_loss 0.60221428 valid_loss 0.61163282 test_loss 0.58258474 \n",
      "文學與創新 | 660 train_loss 0.60346222 valid_loss 0.61169803 test_loss 0.58821857 \n",
      "文學與創新 | 661 train_loss 0.60032463 valid_loss 0.60920888 test_loss 0.58306646 < save\n",
      "文學與創新 | 662 train_loss 0.60014504 valid_loss 0.60952795 test_loss 0.58164418 \n",
      "文學與創新 | 663 train_loss 0.60036844 valid_loss 0.60946018 test_loss 0.58326876 \n",
      "文學與創新 | 664 train_loss 0.60042864 valid_loss 0.60977346 test_loss 0.58232939 \n",
      "文學與創新 | 665 train_loss 0.60846835 valid_loss 0.61904132 test_loss 0.58620948 \n",
      "文學與創新 | 666 train_loss 0.60236728 valid_loss 0.61219031 test_loss 0.58238351 \n",
      "文學與創新 | 667 train_loss 0.60015702 valid_loss 0.60944277 test_loss 0.58260441 \n",
      "文學與創新 | 668 train_loss 0.60364538 valid_loss 0.61346418 test_loss 0.58304918 \n",
      "文學與創新 | 669 train_loss 0.60067761 valid_loss 0.60935217 test_loss 0.58435798 \n",
      "文學與創新 | 670 train_loss 0.60027319 valid_loss 0.60951489 test_loss 0.58174050 \n",
      "文學與創新 | 671 train_loss 0.60038656 valid_loss 0.60956621 test_loss 0.58283442 \n",
      "文學與創新 | 672 train_loss 0.60041535 valid_loss 0.60955578 test_loss 0.58293909 \n",
      "文學與創新 | 673 train_loss 0.60043526 valid_loss 0.60930967 test_loss 0.58365470 \n",
      "文學與創新 | 674 train_loss 0.60023135 valid_loss 0.60889423 test_loss 0.58307499 < save\n",
      "文學與創新 | 675 train_loss 0.60012037 valid_loss 0.60881388 test_loss 0.58281648 < save\n",
      "文學與創新 | 676 train_loss 0.60174710 valid_loss 0.61063153 test_loss 0.58547294 \n",
      "文學與創新 | 677 train_loss 0.60026389 valid_loss 0.60989773 test_loss 0.58125877 \n",
      "文學與創新 | 678 train_loss 0.60016334 valid_loss 0.60931760 test_loss 0.58277822 \n",
      "文學與創新 | 679 train_loss 0.60073692 valid_loss 0.61002380 test_loss 0.58344573 \n",
      "文學與創新 | 680 train_loss 0.60055739 valid_loss 0.61036336 test_loss 0.58146143 \n",
      "文學與創新 | 681 train_loss 0.60043699 valid_loss 0.60992950 test_loss 0.58143330 \n",
      "文學與創新 | 682 train_loss 0.60053551 valid_loss 0.60956478 test_loss 0.58240891 \n",
      "文學與創新 | 683 train_loss 0.60185552 valid_loss 0.61178881 test_loss 0.58172309 \n",
      "文學與創新 | 684 train_loss 0.60097146 valid_loss 0.61008334 test_loss 0.58408064 \n",
      "文學與創新 | 685 train_loss 0.60333443 valid_loss 0.61376983 test_loss 0.58257776 \n",
      "文學與創新 | 686 train_loss 0.60324430 valid_loss 0.61169493 test_loss 0.58774686 \n",
      "文學與創新 | 687 train_loss 0.60003442 valid_loss 0.60969895 test_loss 0.58166909 \n",
      "文學與創新 | 688 train_loss 0.60061592 valid_loss 0.61041754 test_loss 0.58113682 \n",
      "文學與創新 | 689 train_loss 0.60825568 valid_loss 0.61691344 test_loss 0.59317267 \n",
      "文學與創新 | 690 train_loss 0.60022783 valid_loss 0.60955900 test_loss 0.58261341 \n",
      "文學與創新 | 691 train_loss 0.60011357 valid_loss 0.60928011 test_loss 0.58279651 \n",
      "文學與創新 | 692 train_loss 0.59993654 valid_loss 0.60920149 test_loss 0.58194327 \n",
      "文學與創新 | 693 train_loss 0.60273302 valid_loss 0.61281216 test_loss 0.58225071 \n",
      "文學與創新 | 694 train_loss 0.60202891 valid_loss 0.61040115 test_loss 0.58643800 \n",
      "文學與創新 | 695 train_loss 0.60042804 valid_loss 0.60963702 test_loss 0.58134317 \n",
      "文學與創新 | 696 train_loss 0.60004860 valid_loss 0.60969889 test_loss 0.58129692 \n",
      "文學與創新 | 697 train_loss 0.60269475 valid_loss 0.61139953 test_loss 0.58666044 \n",
      "文學與創新 | 698 train_loss 0.60188395 valid_loss 0.61124164 test_loss 0.58495128 \n",
      "文學與創新 | 699 train_loss 0.60014313 valid_loss 0.60968244 test_loss 0.58158016 \n",
      "文學與創新 | 700 train_loss 0.60335261 valid_loss 0.61377215 test_loss 0.58238739 \n",
      "文學與創新 | 701 train_loss 0.60024691 valid_loss 0.60889196 test_loss 0.58333445 \n",
      "文學與創新 | 702 train_loss 0.60205948 valid_loss 0.61112165 test_loss 0.58518416 \n",
      "文學與創新 | 703 train_loss 0.60042799 valid_loss 0.60995066 test_loss 0.58141983 \n",
      "文學與創新 | 704 train_loss 0.60350841 valid_loss 0.61220711 test_loss 0.58782464 \n",
      "文學與創新 | 705 train_loss 0.60026658 valid_loss 0.61000270 test_loss 0.58149433 \n",
      "文學與創新 | 706 train_loss 0.60090894 valid_loss 0.61097193 test_loss 0.58134288 \n",
      "文學與創新 | 707 train_loss 0.60006636 valid_loss 0.60879284 test_loss 0.58261257 < save\n",
      "文學與創新 | 708 train_loss 0.59997380 valid_loss 0.60947615 test_loss 0.58208078 \n",
      "文學與創新 | 709 train_loss 0.60189539 valid_loss 0.61100084 test_loss 0.58509147 \n",
      "文學與創新 | 710 train_loss 0.60105050 valid_loss 0.61004430 test_loss 0.58446050 \n",
      "文學與創新 | 711 train_loss 0.60145807 valid_loss 0.61058229 test_loss 0.58451724 \n",
      "文學與創新 | 712 train_loss 0.60101408 valid_loss 0.60981554 test_loss 0.58445543 \n",
      "文學與創新 | 713 train_loss 0.60015589 valid_loss 0.60992825 test_loss 0.58119571 \n",
      "文學與創新 | 714 train_loss 0.60019416 valid_loss 0.60973650 test_loss 0.58134413 \n",
      "文學與創新 | 715 train_loss 0.60063517 valid_loss 0.60939831 test_loss 0.58394456 \n",
      "文學與創新 | 716 train_loss 0.60449898 valid_loss 0.61334711 test_loss 0.58872652 \n",
      "文學與創新 | 717 train_loss 0.60224748 valid_loss 0.61093187 test_loss 0.58607352 \n",
      "文學與創新 | 718 train_loss 0.60186994 valid_loss 0.61160111 test_loss 0.58199298 \n",
      "文學與創新 | 719 train_loss 0.59987897 valid_loss 0.60910654 test_loss 0.58146650 \n",
      "文學與創新 | 720 train_loss 0.60095918 valid_loss 0.61068958 test_loss 0.58165038 \n",
      "文學與創新 | 721 train_loss 0.60289699 valid_loss 0.61273146 test_loss 0.58287066 \n",
      "文學與創新 | 722 train_loss 0.60002649 valid_loss 0.60923183 test_loss 0.58297431 \n",
      "文學與創新 | 723 train_loss 0.60031599 valid_loss 0.60990971 test_loss 0.58221692 \n",
      "文學與創新 | 724 train_loss 0.59997946 valid_loss 0.60942239 test_loss 0.58156437 \n",
      "文學與創新 | 725 train_loss 0.59981000 valid_loss 0.60893208 test_loss 0.58208734 \n",
      "文學與創新 | 726 train_loss 0.60090274 valid_loss 0.61026329 test_loss 0.58195031 \n",
      "文學與創新 | 727 train_loss 0.59986722 valid_loss 0.60923713 test_loss 0.58149350 \n",
      "文學與創新 | 728 train_loss 0.59986776 valid_loss 0.60874110 test_loss 0.58255911 < save\n",
      "文學與創新 | 729 train_loss 0.60027122 valid_loss 0.61006576 test_loss 0.58160472 \n",
      "文學與創新 | 730 train_loss 0.60019034 valid_loss 0.60885412 test_loss 0.58324933 \n",
      "文學與創新 | 731 train_loss 0.60039097 valid_loss 0.60995811 test_loss 0.58270127 \n",
      "文學與創新 | 732 train_loss 0.60215020 valid_loss 0.61137313 test_loss 0.58535385 \n",
      "文學與創新 | 733 train_loss 0.60084474 valid_loss 0.60931998 test_loss 0.58473212 \n",
      "文學與創新 | 734 train_loss 0.60105634 valid_loss 0.60968488 test_loss 0.58461803 \n",
      "文學與創新 | 735 train_loss 0.59993112 valid_loss 0.60901195 test_loss 0.58269620 \n",
      "文學與創新 | 736 train_loss 0.60096884 valid_loss 0.61009878 test_loss 0.58374631 \n",
      "文學與創新 | 737 train_loss 0.60193068 valid_loss 0.61169809 test_loss 0.58191276 \n",
      "文學與創新 | 738 train_loss 0.59986794 valid_loss 0.60904914 test_loss 0.58218986 \n",
      "文學與創新 | 739 train_loss 0.59997946 valid_loss 0.60926366 test_loss 0.58168167 \n",
      "文學與創新 | 740 train_loss 0.59991264 valid_loss 0.60878009 test_loss 0.58287472 \n",
      "文學與創新 | 741 train_loss 0.60169417 valid_loss 0.61069846 test_loss 0.58497661 \n",
      "文學與創新 | 742 train_loss 0.60023934 valid_loss 0.60880566 test_loss 0.58357888 \n",
      "文學與創新 | 743 train_loss 0.60508257 valid_loss 0.61338496 test_loss 0.59012139 \n",
      "文學與創新 | 744 train_loss 0.60019684 valid_loss 0.60947073 test_loss 0.58250183 \n",
      "文學與創新 | 745 train_loss 0.60010368 valid_loss 0.60944414 test_loss 0.58245498 \n",
      "文學與創新 | 746 train_loss 0.59981036 valid_loss 0.60913962 test_loss 0.58192712 \n",
      "文學與創新 | 747 train_loss 0.60037720 valid_loss 0.60922951 test_loss 0.58193415 \n",
      "文學與創新 | 748 train_loss 0.60049981 valid_loss 0.60970467 test_loss 0.58143181 \n",
      "文學與創新 | 749 train_loss 0.60064691 valid_loss 0.60920864 test_loss 0.58290225 \n",
      "文學與創新 | 750 train_loss 0.59986466 valid_loss 0.60899985 test_loss 0.58155566 \n",
      "文學與創新 | 751 train_loss 0.60171169 valid_loss 0.61135572 test_loss 0.58171374 \n",
      "文學與創新 | 752 train_loss 0.60313100 valid_loss 0.61321509 test_loss 0.58271950 \n",
      "文學與創新 | 753 train_loss 0.60042131 valid_loss 0.60898590 test_loss 0.58382052 \n",
      "文學與創新 | 754 train_loss 0.60000789 valid_loss 0.60851407 test_loss 0.58309561 < save\n",
      "文學與創新 | 755 train_loss 0.59995782 valid_loss 0.60875368 test_loss 0.58279997 \n",
      "文學與創新 | 756 train_loss 0.60056728 valid_loss 0.60976911 test_loss 0.58182400 \n",
      "文學與創新 | 757 train_loss 0.59994900 valid_loss 0.60950214 test_loss 0.58196038 \n",
      "文學與創新 | 758 train_loss 0.60105097 valid_loss 0.60979044 test_loss 0.58430994 \n",
      "文學與創新 | 759 train_loss 0.59981734 valid_loss 0.60914272 test_loss 0.58136100 \n",
      "文學與創新 | 760 train_loss 0.59999764 valid_loss 0.60885811 test_loss 0.58198208 \n",
      "文學與創新 | 761 train_loss 0.60832685 valid_loss 0.61650765 test_loss 0.59381109 \n",
      "文學與創新 | 762 train_loss 0.60114741 valid_loss 0.61061352 test_loss 0.58366162 \n",
      "文學與創新 | 763 train_loss 0.60273921 valid_loss 0.61252654 test_loss 0.58283305 \n",
      "文學與創新 | 764 train_loss 0.60215557 valid_loss 0.61210555 test_loss 0.58249497 \n",
      "文學與創新 | 765 train_loss 0.59981620 valid_loss 0.60845667 test_loss 0.58219314 < save\n",
      "文學與創新 | 766 train_loss 0.59979188 valid_loss 0.60846680 test_loss 0.58268791 \n",
      "文學與創新 | 767 train_loss 0.60149103 valid_loss 0.61127287 test_loss 0.58172840 \n",
      "文學與創新 | 768 train_loss 0.60092300 valid_loss 0.60934740 test_loss 0.58499169 \n",
      "文學與創新 | 769 train_loss 0.60056567 valid_loss 0.60920680 test_loss 0.58401936 \n",
      "文學與創新 | 770 train_loss 0.59990543 valid_loss 0.60905385 test_loss 0.58263457 \n",
      "文學與創新 | 771 train_loss 0.59977239 valid_loss 0.60943240 test_loss 0.58154696 \n",
      "文學與創新 | 772 train_loss 0.59969085 valid_loss 0.60851985 test_loss 0.58203006 \n",
      "文學與創新 | 773 train_loss 0.59971058 valid_loss 0.60883743 test_loss 0.58167475 \n",
      "文學與創新 | 774 train_loss 0.60195363 valid_loss 0.61224300 test_loss 0.58171576 \n",
      "文學與創新 | 775 train_loss 0.60043305 valid_loss 0.60997373 test_loss 0.58127397 \n",
      "文學與創新 | 776 train_loss 0.60051918 valid_loss 0.61029691 test_loss 0.58110237 \n",
      "文學與創新 | 777 train_loss 0.59977484 valid_loss 0.60856247 test_loss 0.58214873 \n",
      "文學與創新 | 778 train_loss 0.60041982 valid_loss 0.60993916 test_loss 0.58140314 \n",
      "文學與創新 | 779 train_loss 0.59994030 valid_loss 0.60902941 test_loss 0.58169252 \n",
      "文學與創新 | 780 train_loss 0.60164809 valid_loss 0.60971856 test_loss 0.58622056 \n",
      "文學與創新 | 781 train_loss 0.59968901 valid_loss 0.60869575 test_loss 0.58181620 \n",
      "文學與創新 | 782 train_loss 0.60076976 valid_loss 0.60904270 test_loss 0.58307701 \n",
      "文學與創新 | 783 train_loss 0.60147190 valid_loss 0.61133885 test_loss 0.58143753 \n",
      "文學與創新 | 784 train_loss 0.60034567 valid_loss 0.60924971 test_loss 0.58176619 \n",
      "文學與創新 | 785 train_loss 0.60211670 valid_loss 0.61020738 test_loss 0.58662122 \n",
      "文學與創新 | 786 train_loss 0.60016048 valid_loss 0.60849267 test_loss 0.58345979 \n",
      "文學與創新 | 787 train_loss 0.60591424 valid_loss 0.61594146 test_loss 0.58446717 \n",
      "文學與創新 | 788 train_loss 0.59978372 valid_loss 0.60894710 test_loss 0.58162439 \n",
      "文學與創新 | 789 train_loss 0.60044295 valid_loss 0.60999423 test_loss 0.58125579 \n",
      "文學與創新 | 790 train_loss 0.59989595 valid_loss 0.60866696 test_loss 0.58296275 \n",
      "文學與創新 | 791 train_loss 0.60051924 valid_loss 0.60931474 test_loss 0.58396500 \n",
      "文學與創新 | 792 train_loss 0.60077697 valid_loss 0.61028695 test_loss 0.58155346 \n",
      "文學與創新 | 793 train_loss 0.60758573 valid_loss 0.61764699 test_loss 0.58543634 \n",
      "文學與創新 | 794 train_loss 0.60191047 valid_loss 0.61076248 test_loss 0.58556330 \n",
      "文學與創新 | 795 train_loss 0.59981579 valid_loss 0.60924858 test_loss 0.58152825 \n",
      "文學與創新 | 796 train_loss 0.60011208 valid_loss 0.60874945 test_loss 0.58325505 \n",
      "文學與創新 | 797 train_loss 0.59967506 valid_loss 0.60857481 test_loss 0.58204752 \n",
      "文學與創新 | 798 train_loss 0.59991056 valid_loss 0.60904062 test_loss 0.58244383 \n",
      "文學與創新 | 799 train_loss 0.60001290 valid_loss 0.60896373 test_loss 0.58228606 \n",
      "文學與創新 | 800 train_loss 0.60034478 valid_loss 0.60983974 test_loss 0.58136916 \n",
      "文學與創新 | 801 train_loss 0.59973037 valid_loss 0.60888392 test_loss 0.58177871 \n",
      "文學與創新 | 802 train_loss 0.60036361 valid_loss 0.60900867 test_loss 0.58359492 \n",
      "文學與創新 | 803 train_loss 0.59964770 valid_loss 0.60871142 test_loss 0.58196682 \n",
      "文學與創新 | 804 train_loss 0.60267973 valid_loss 0.61269546 test_loss 0.58246869 \n",
      "文學與創新 | 805 train_loss 0.59985846 valid_loss 0.60863394 test_loss 0.58241874 \n",
      "文學與創新 | 806 train_loss 0.60006064 valid_loss 0.60889232 test_loss 0.58184135 \n",
      "文學與創新 | 807 train_loss 0.60140896 valid_loss 0.61118919 test_loss 0.58154601 \n",
      "文學與創新 | 808 train_loss 0.59961057 valid_loss 0.60868406 test_loss 0.58203280 \n",
      "文學與創新 | 809 train_loss 0.59984601 valid_loss 0.60881644 test_loss 0.58136952 \n",
      "文學與創新 | 810 train_loss 0.59986913 valid_loss 0.60833466 test_loss 0.58271831 < save\n",
      "文學與創新 | 811 train_loss 0.60022104 valid_loss 0.60872275 test_loss 0.58397967 \n",
      "文學與創新 | 812 train_loss 0.59999973 valid_loss 0.60921788 test_loss 0.58220112 \n",
      "文學與創新 | 813 train_loss 0.59990495 valid_loss 0.60892206 test_loss 0.58170581 \n",
      "文學與創新 | 814 train_loss 0.60055208 valid_loss 0.61000985 test_loss 0.58145946 \n",
      "文學與創新 | 815 train_loss 0.59966046 valid_loss 0.60884398 test_loss 0.58148426 \n",
      "文學與創新 | 816 train_loss 0.59991854 valid_loss 0.60864908 test_loss 0.58292264 \n",
      "文學與創新 | 817 train_loss 0.60213161 valid_loss 0.61201292 test_loss 0.58217251 \n",
      "文學與創新 | 818 train_loss 0.60020369 valid_loss 0.60904670 test_loss 0.58234406 \n",
      "文學與創新 | 819 train_loss 0.60034984 valid_loss 0.60980552 test_loss 0.58163565 \n",
      "文學與創新 | 820 train_loss 0.60071552 valid_loss 0.60979259 test_loss 0.58187288 \n",
      "文學與創新 | 821 train_loss 0.59965706 valid_loss 0.60896242 test_loss 0.58128500 \n",
      "文學與創新 | 822 train_loss 0.59979808 valid_loss 0.60884094 test_loss 0.58170146 \n",
      "文學與創新 | 823 train_loss 0.60249758 valid_loss 0.61185372 test_loss 0.58258373 \n",
      "文學與創新 | 824 train_loss 0.59980971 valid_loss 0.60897768 test_loss 0.58178669 \n",
      "文學與創新 | 825 train_loss 0.59977341 valid_loss 0.60915035 test_loss 0.58096039 \n",
      "文學與創新 | 826 train_loss 0.60135692 valid_loss 0.60977906 test_loss 0.58536148 \n",
      "文學與創新 | 827 train_loss 0.60247630 valid_loss 0.61254394 test_loss 0.58175302 \n",
      "文學與創新 | 828 train_loss 0.60052657 valid_loss 0.60969609 test_loss 0.58323205 \n",
      "文學與創新 | 829 train_loss 0.60027242 valid_loss 0.60876787 test_loss 0.58362204 \n",
      "文學與創新 | 830 train_loss 0.59960228 valid_loss 0.60850042 test_loss 0.58194894 \n",
      "文學與創新 | 831 train_loss 0.59978384 valid_loss 0.60880876 test_loss 0.58225328 \n",
      "文學與創新 | 832 train_loss 0.59966195 valid_loss 0.60909158 test_loss 0.58132958 \n",
      "文學與創新 | 833 train_loss 0.60004634 valid_loss 0.60883933 test_loss 0.58322847 \n",
      "文學與創新 | 834 train_loss 0.59955287 valid_loss 0.60853374 test_loss 0.58168834 \n",
      "文學與創新 | 835 train_loss 0.60031098 valid_loss 0.61022365 test_loss 0.58137238 \n",
      "文學與創新 | 836 train_loss 0.60242218 valid_loss 0.61203212 test_loss 0.58192915 \n",
      "文學與創新 | 837 train_loss 0.59953934 valid_loss 0.60860020 test_loss 0.58162135 \n",
      "文學與創新 | 838 train_loss 0.60043931 valid_loss 0.60969955 test_loss 0.58292228 \n",
      "文學與創新 | 839 train_loss 0.59998822 valid_loss 0.60921061 test_loss 0.58253241 \n",
      "文學與創新 | 840 train_loss 0.59984195 valid_loss 0.60840231 test_loss 0.58271772 \n",
      "文學與創新 | 841 train_loss 0.59970474 valid_loss 0.60885119 test_loss 0.58213365 \n",
      "文學與創新 | 842 train_loss 0.60111624 valid_loss 0.60937136 test_loss 0.58532852 \n",
      "文學與創新 | 843 train_loss 0.59962744 valid_loss 0.60814834 test_loss 0.58219492 < save\n",
      "文學與創新 | 844 train_loss 0.60146332 valid_loss 0.60950565 test_loss 0.58592993 \n",
      "文學與創新 | 845 train_loss 0.60216576 valid_loss 0.61061180 test_loss 0.58596921 \n",
      "文學與創新 | 846 train_loss 0.59997678 valid_loss 0.60864085 test_loss 0.58235931 \n",
      "文學與創新 | 847 train_loss 0.60171962 valid_loss 0.61080682 test_loss 0.58509225 \n",
      "文學與創新 | 848 train_loss 0.60170954 valid_loss 0.61140406 test_loss 0.58232015 \n",
      "文學與創新 | 849 train_loss 0.59955072 valid_loss 0.60865790 test_loss 0.58174694 \n",
      "文學與創新 | 850 train_loss 0.59972751 valid_loss 0.60857511 test_loss 0.58226854 \n",
      "文學與創新 | 851 train_loss 0.59973967 valid_loss 0.60841066 test_loss 0.58280689 \n",
      "文學與創新 | 852 train_loss 0.59962624 valid_loss 0.60895145 test_loss 0.58165020 \n",
      "文學與創新 | 853 train_loss 0.60072267 valid_loss 0.60899872 test_loss 0.58494711 \n",
      "文學與創新 | 854 train_loss 0.59992206 valid_loss 0.60950869 test_loss 0.58127451 \n",
      "文學與創新 | 855 train_loss 0.60039634 valid_loss 0.60890782 test_loss 0.58395189 \n",
      "文學與創新 | 856 train_loss 0.60228378 valid_loss 0.61202753 test_loss 0.58239138 \n",
      "文學與創新 | 857 train_loss 0.60119522 valid_loss 0.61051136 test_loss 0.58189470 \n",
      "文學與創新 | 858 train_loss 0.59985000 valid_loss 0.60860300 test_loss 0.58279419 \n",
      "文學與創新 | 859 train_loss 0.60091960 valid_loss 0.61055350 test_loss 0.58139002 \n",
      "文學與創新 | 860 train_loss 0.59953851 valid_loss 0.60852849 test_loss 0.58214378 \n",
      "文學與創新 | 861 train_loss 0.60034823 valid_loss 0.60961044 test_loss 0.58133703 \n",
      "文學與創新 | 862 train_loss 0.59975421 valid_loss 0.60879600 test_loss 0.58226359 \n",
      "文學與創新 | 863 train_loss 0.59961200 valid_loss 0.60858262 test_loss 0.58161211 \n",
      "文學與創新 | 864 train_loss 0.60264796 valid_loss 0.61112934 test_loss 0.58707374 \n",
      "文學與創新 | 865 train_loss 0.59972942 valid_loss 0.60826218 test_loss 0.58283848 \n",
      "文學與創新 | 866 train_loss 0.60269791 valid_loss 0.61053836 test_loss 0.58759767 \n",
      "文學與創新 | 867 train_loss 0.60034496 valid_loss 0.60964805 test_loss 0.58142143 \n",
      "文學與創新 | 868 train_loss 0.59953183 valid_loss 0.60829949 test_loss 0.58216625 \n",
      "文學與創新 | 869 train_loss 0.59963250 valid_loss 0.60886425 test_loss 0.58217287 \n",
      "文學與創新 | 870 train_loss 0.59969020 valid_loss 0.60838521 test_loss 0.58196706 \n",
      "文學與創新 | 871 train_loss 0.59970963 valid_loss 0.60830653 test_loss 0.58293730 \n",
      "文學與創新 | 872 train_loss 0.59998268 valid_loss 0.60974276 test_loss 0.58089423 \n",
      "文學與創新 | 873 train_loss 0.59952354 valid_loss 0.60872054 test_loss 0.58130991 \n",
      "文學與創新 | 874 train_loss 0.60092527 valid_loss 0.61051667 test_loss 0.58160669 \n",
      "文學與創新 | 875 train_loss 0.59949952 valid_loss 0.60868716 test_loss 0.58203059 \n",
      "文學與創新 | 876 train_loss 0.60274726 valid_loss 0.61135817 test_loss 0.58673728 \n",
      "文學與創新 | 877 train_loss 0.60025895 valid_loss 0.60868520 test_loss 0.58384466 \n",
      "文學與創新 | 878 train_loss 0.60001910 valid_loss 0.60942531 test_loss 0.58247334 \n",
      "文學與創新 | 879 train_loss 0.60262442 valid_loss 0.61278248 test_loss 0.58192670 \n",
      "文學與創新 | 880 train_loss 0.59985083 valid_loss 0.60825360 test_loss 0.58300012 \n",
      "文學與創新 | 881 train_loss 0.60428858 valid_loss 0.61460650 test_loss 0.58291262 \n",
      "文學與創新 | 882 train_loss 0.59984422 valid_loss 0.60876030 test_loss 0.58258629 \n",
      "文學與創新 | 883 train_loss 0.60210168 valid_loss 0.61172873 test_loss 0.58208126 \n",
      "文學與創新 | 884 train_loss 0.60164589 valid_loss 0.61086661 test_loss 0.58225721 \n",
      "文學與創新 | 885 train_loss 0.60123372 valid_loss 0.61109954 test_loss 0.58167601 \n",
      "文學與創新 | 886 train_loss 0.59961033 valid_loss 0.60844499 test_loss 0.58157754 \n",
      "文學與創新 | 887 train_loss 0.60009563 valid_loss 0.60889590 test_loss 0.58306485 \n",
      "文學與創新 | 888 train_loss 0.60607713 valid_loss 0.61638278 test_loss 0.58418077 \n",
      "文學與創新 | 889 train_loss 0.59965181 valid_loss 0.60841256 test_loss 0.58262521 \n",
      "文學與創新 | 890 train_loss 0.60098743 valid_loss 0.61053813 test_loss 0.58147943 \n",
      "文學與創新 | 891 train_loss 0.59965885 valid_loss 0.60835463 test_loss 0.58204406 \n",
      "文學與創新 | 892 train_loss 0.59957212 valid_loss 0.60878766 test_loss 0.58125788 \n",
      "文學與創新 | 893 train_loss 0.60413682 valid_loss 0.61242074 test_loss 0.58890551 \n",
      "文學與創新 | 894 train_loss 0.60049081 valid_loss 0.60938030 test_loss 0.58357924 \n",
      "文學與創新 | 895 train_loss 0.60219330 valid_loss 0.61091995 test_loss 0.58601719 \n",
      "文學與創新 | 896 train_loss 0.59975624 valid_loss 0.60908061 test_loss 0.58090883 \n",
      "文學與創新 | 897 train_loss 0.59983921 valid_loss 0.60901845 test_loss 0.58118552 \n",
      "文學與創新 | 898 train_loss 0.60091543 valid_loss 0.61014616 test_loss 0.58182979 \n",
      "文學與創新 | 899 train_loss 0.60038573 valid_loss 0.61021465 test_loss 0.58083558 \n",
      "文學與創新 | 900 train_loss 0.59970748 valid_loss 0.60879338 test_loss 0.58181971 \n",
      "文學與創新 | 901 train_loss 0.59942520 valid_loss 0.60835546 test_loss 0.58155394 \n",
      "文學與創新 | 902 train_loss 0.60137010 valid_loss 0.61042899 test_loss 0.58388221 \n",
      "文學與創新 | 903 train_loss 0.59942669 valid_loss 0.60820508 test_loss 0.58155853 \n",
      "文學與創新 | 904 train_loss 0.59964448 valid_loss 0.60894626 test_loss 0.58113962 \n",
      "文學與創新 | 905 train_loss 0.60030842 valid_loss 0.60925859 test_loss 0.58368731 \n",
      "文學與創新 | 906 train_loss 0.59949738 valid_loss 0.60826343 test_loss 0.58205181 \n",
      "文學與創新 | 907 train_loss 0.59985709 valid_loss 0.60908484 test_loss 0.58118087 \n",
      "文學與創新 | 908 train_loss 0.60073030 valid_loss 0.61011291 test_loss 0.58362466 \n",
      "文學與創新 | 909 train_loss 0.59966868 valid_loss 0.60896963 test_loss 0.58114839 \n",
      "文學與創新 | 910 train_loss 0.59990650 valid_loss 0.60904264 test_loss 0.58170551 \n",
      "文學與創新 | 911 train_loss 0.60059404 valid_loss 0.60857308 test_loss 0.58403730 \n",
      "文學與創新 | 912 train_loss 0.59955692 valid_loss 0.60818839 test_loss 0.58179528 \n",
      "文學與創新 | 913 train_loss 0.60200238 valid_loss 0.61020094 test_loss 0.58636302 \n",
      "文學與創新 | 914 train_loss 0.60083956 valid_loss 0.61024487 test_loss 0.58152306 \n",
      "文學與創新 | 915 train_loss 0.60024387 valid_loss 0.60908008 test_loss 0.58169109 \n",
      "文學與創新 | 916 train_loss 0.59946209 valid_loss 0.60855049 test_loss 0.58126390 \n",
      "文學與創新 | 917 train_loss 0.59972703 valid_loss 0.60887194 test_loss 0.58142757 \n",
      "文學與創新 | 918 train_loss 0.60100961 valid_loss 0.60914141 test_loss 0.58521742 \n",
      "文學與創新 | 919 train_loss 0.60076272 valid_loss 0.60982311 test_loss 0.58340436 \n",
      "文學與創新 | 920 train_loss 0.59945762 valid_loss 0.60818928 test_loss 0.58200479 \n",
      "文學與創新 | 921 train_loss 0.60087788 valid_loss 0.60945368 test_loss 0.58449066 \n",
      "文學與創新 | 922 train_loss 0.59943408 valid_loss 0.60836464 test_loss 0.58159363 \n",
      "文學與創新 | 923 train_loss 0.59950739 valid_loss 0.60876286 test_loss 0.58125716 \n",
      "文學與創新 | 924 train_loss 0.60531574 valid_loss 0.61380190 test_loss 0.59056181 \n",
      "文學與創新 | 925 train_loss 0.60504919 valid_loss 0.61293942 test_loss 0.59048730 \n",
      "文學與創新 | 926 train_loss 0.60022622 valid_loss 0.60907614 test_loss 0.58311224 \n",
      "文學與創新 | 927 train_loss 0.60164547 valid_loss 0.61053264 test_loss 0.58541709 \n",
      "文學與創新 | 928 train_loss 0.59995955 valid_loss 0.60885024 test_loss 0.58294117 \n",
      "文學與創新 | 929 train_loss 0.60035360 valid_loss 0.60984129 test_loss 0.58128077 \n",
      "文學與創新 | 930 train_loss 0.60020936 valid_loss 0.60940534 test_loss 0.58295757 \n",
      "文學與創新 | 931 train_loss 0.60008812 valid_loss 0.60891032 test_loss 0.58366287 \n",
      "文學與創新 | 932 train_loss 0.60055804 valid_loss 0.60968369 test_loss 0.58147138 \n",
      "文學與創新 | 933 train_loss 0.60227579 valid_loss 0.61210257 test_loss 0.58240557 \n",
      "文學與創新 | 934 train_loss 0.59944606 valid_loss 0.60840642 test_loss 0.58204544 \n",
      "文學與創新 | 935 train_loss 0.59977430 valid_loss 0.60864979 test_loss 0.58143467 \n",
      "文學與創新 | 936 train_loss 0.59987682 valid_loss 0.60855609 test_loss 0.58300084 \n",
      "文學與創新 | 937 train_loss 0.60490745 valid_loss 0.61497724 test_loss 0.58376658 \n",
      "文學與創新 | 938 train_loss 0.59938651 valid_loss 0.60836327 test_loss 0.58146596 \n",
      "文學與創新 | 939 train_loss 0.59964424 valid_loss 0.60908741 test_loss 0.58150285 \n",
      "文學與創新 | 940 train_loss 0.59951204 valid_loss 0.60890275 test_loss 0.58122879 \n",
      "文學與創新 | 941 train_loss 0.59971189 valid_loss 0.60831946 test_loss 0.58301133 \n",
      "文學與創新 | 942 train_loss 0.60169464 valid_loss 0.61141080 test_loss 0.58192801 \n",
      "文學與創新 | 943 train_loss 0.59936213 valid_loss 0.60810649 test_loss 0.58197677 < save\n",
      "文學與創新 | 944 train_loss 0.60034329 valid_loss 0.60984945 test_loss 0.58218312 \n",
      "文學與創新 | 945 train_loss 0.59965199 valid_loss 0.60858005 test_loss 0.58187819 \n",
      "文學與創新 | 946 train_loss 0.59936965 valid_loss 0.60825950 test_loss 0.58181500 \n",
      "文學與創新 | 947 train_loss 0.59973842 valid_loss 0.60861164 test_loss 0.58248675 \n",
      "文學與創新 | 948 train_loss 0.59943622 valid_loss 0.60870290 test_loss 0.58139008 \n",
      "文學與創新 | 949 train_loss 0.60116822 valid_loss 0.61097199 test_loss 0.58141911 \n",
      "文學與創新 | 950 train_loss 0.59935892 valid_loss 0.60852242 test_loss 0.58159560 \n",
      "文學與創新 | 951 train_loss 0.59954512 valid_loss 0.60814446 test_loss 0.58283138 \n",
      "文學與創新 | 952 train_loss 0.59948635 valid_loss 0.60844648 test_loss 0.58141565 \n",
      "文學與創新 | 953 train_loss 0.59994459 valid_loss 0.60856640 test_loss 0.58304900 \n",
      "文學與創新 | 954 train_loss 0.60030049 valid_loss 0.60992956 test_loss 0.58114642 \n",
      "文學與創新 | 955 train_loss 0.59960675 valid_loss 0.60820371 test_loss 0.58193582 \n",
      "文學與創新 | 956 train_loss 0.59936565 valid_loss 0.60837883 test_loss 0.58140481 \n",
      "文學與創新 | 957 train_loss 0.59962064 valid_loss 0.60905439 test_loss 0.58150357 \n",
      "文學與創新 | 958 train_loss 0.59945172 valid_loss 0.60893631 test_loss 0.58112669 \n",
      "文學與創新 | 959 train_loss 0.59937990 valid_loss 0.60824078 test_loss 0.58189052 \n",
      "文學與創新 | 960 train_loss 0.60062766 valid_loss 0.60973406 test_loss 0.58194053 \n",
      "文學與創新 | 961 train_loss 0.59942484 valid_loss 0.60814750 test_loss 0.58239329 \n",
      "文學與創新 | 962 train_loss 0.60536206 valid_loss 0.61343998 test_loss 0.59084082 \n",
      "文學與創新 | 963 train_loss 0.60119683 valid_loss 0.60977912 test_loss 0.58483559 \n",
      "文學與創新 | 964 train_loss 0.59948528 valid_loss 0.60895360 test_loss 0.58086824 \n",
      "文學與創新 | 965 train_loss 0.59949142 valid_loss 0.60820872 test_loss 0.58233929 \n",
      "文學與創新 | 966 train_loss 0.60206074 valid_loss 0.61194825 test_loss 0.58211333 \n",
      "文學與創新 | 967 train_loss 0.59939224 valid_loss 0.60795557 test_loss 0.58196324 < save\n",
      "文學與創新 | 968 train_loss 0.59967059 valid_loss 0.60865515 test_loss 0.58190858 \n",
      "文學與創新 | 969 train_loss 0.60271871 valid_loss 0.61259788 test_loss 0.58257103 \n",
      "文學與創新 | 970 train_loss 0.59946907 valid_loss 0.60862130 test_loss 0.58117634 \n",
      "文學與創新 | 971 train_loss 0.59948468 valid_loss 0.60802716 test_loss 0.58241081 \n",
      "文學與創新 | 972 train_loss 0.60698712 valid_loss 0.61530012 test_loss 0.59228563 \n",
      "文學與創新 | 973 train_loss 0.60006833 valid_loss 0.60906994 test_loss 0.58173066 \n",
      "文學與創新 | 974 train_loss 0.60096496 valid_loss 0.60914654 test_loss 0.58517951 \n",
      "文學與創新 | 975 train_loss 0.59953606 valid_loss 0.60838658 test_loss 0.58209264 \n",
      "文學與創新 | 976 train_loss 0.59947181 valid_loss 0.60892463 test_loss 0.58119237 \n",
      "文學與創新 | 977 train_loss 0.59967971 valid_loss 0.60933268 test_loss 0.58091831 \n",
      "文學與創新 | 978 train_loss 0.59961027 valid_loss 0.60897428 test_loss 0.58077908 \n",
      "文學與創新 | 979 train_loss 0.59995162 valid_loss 0.60862690 test_loss 0.58319002 \n",
      "文學與創新 | 980 train_loss 0.59971464 valid_loss 0.60861373 test_loss 0.58266336 \n",
      "文學與創新 | 981 train_loss 0.60267866 valid_loss 0.61233652 test_loss 0.58260679 \n",
      "文學與創新 | 982 train_loss 0.59952384 valid_loss 0.60883522 test_loss 0.58075893 \n",
      "文學與創新 | 983 train_loss 0.59952909 valid_loss 0.60826582 test_loss 0.58240664 \n",
      "文學與創新 | 984 train_loss 0.60134566 valid_loss 0.60979384 test_loss 0.58554888 \n",
      "文學與創新 | 985 train_loss 0.59934616 valid_loss 0.60799414 test_loss 0.58170229 \n",
      "文學與創新 | 986 train_loss 0.60161370 valid_loss 0.61185235 test_loss 0.58133632 \n",
      "文學與創新 | 987 train_loss 0.60754526 valid_loss 0.61544013 test_loss 0.59361655 \n",
      "文學與創新 | 988 train_loss 0.59939069 valid_loss 0.60808825 test_loss 0.58174682 \n",
      "文學與創新 | 989 train_loss 0.59963298 valid_loss 0.60834020 test_loss 0.58253235 \n",
      "文學與創新 | 990 train_loss 0.60325819 valid_loss 0.61173093 test_loss 0.58761621 \n",
      "文學與創新 | 991 train_loss 0.60029042 valid_loss 0.60860378 test_loss 0.58416855 \n",
      "文學與創新 | 992 train_loss 0.59932429 valid_loss 0.60840917 test_loss 0.58147514 \n",
      "文學與創新 | 993 train_loss 0.60091567 valid_loss 0.60948944 test_loss 0.58463109 \n",
      "文學與創新 | 994 train_loss 0.60204262 valid_loss 0.61199117 test_loss 0.58189315 \n",
      "文學與創新 | 995 train_loss 0.59942687 valid_loss 0.60840148 test_loss 0.58128071 \n",
      "文學與創新 | 996 train_loss 0.60114753 valid_loss 0.61049104 test_loss 0.58162344 \n",
      "文學與創新 | 997 train_loss 0.59988952 valid_loss 0.60919821 test_loss 0.58111423 \n",
      "文學與創新 | 998 train_loss 0.60258180 valid_loss 0.61209488 test_loss 0.58238667 \n",
      "文學與創新 | 999 train_loss 0.59952724 valid_loss 0.60873264 test_loss 0.58111686 \n",
      "文學與創新 | 1000 train_loss 0.60358649 valid_loss 0.61378741 test_loss 0.58264142 \n",
      "ReTraining done, RMSE: 0.8166195154190063 => 0.6079555749893188\n",
      "decrease: 0.2086639404296875\n",
      "\n",
      "Now processing model: 當機器人來上班\n",
      "original RMSE: 1.370413064956665\n",
      "當機器人來上班 | 1 train_loss 1.45261645 valid_loss 1.82502806 test_loss 1.75400436 \n",
      "當機器人來上班 | 2 train_loss 1.43395054 valid_loss 1.80317402 test_loss 1.73344696 \n",
      "當機器人來上班 | 3 train_loss 1.42398334 valid_loss 1.79041898 test_loss 1.72328317 \n",
      "當機器人來上班 | 4 train_loss 1.41796947 valid_loss 1.78113508 test_loss 1.71633446 \n",
      "當機器人來上班 | 5 train_loss 1.41412199 valid_loss 1.77122831 test_loss 1.71027863 \n",
      "當機器人來上班 | 6 train_loss 1.41035533 valid_loss 1.77000439 test_loss 1.70858228 \n",
      "當機器人來上班 | 7 train_loss 1.40801144 valid_loss 1.76581633 test_loss 1.70601964 \n",
      "當機器人來上班 | 8 train_loss 1.40960598 valid_loss 1.75928104 test_loss 1.70380223 \n",
      "當機器人來上班 | 9 train_loss 1.41604114 valid_loss 1.76126027 test_loss 1.70867670 \n",
      "當機器人來上班 | 10 train_loss 1.40627158 valid_loss 1.75560725 test_loss 1.70080793 \n",
      "當機器人來上班 | 11 train_loss 1.40401328 valid_loss 1.75419486 test_loss 1.69903648 \n",
      "當機器人來上班 | 12 train_loss 1.40304303 valid_loss 1.75381804 test_loss 1.69836330 \n",
      "當機器人來上班 | 13 train_loss 1.40157831 valid_loss 1.75461972 test_loss 1.69839525 \n",
      "當機器人來上班 | 14 train_loss 1.40291679 valid_loss 1.76078308 test_loss 1.70222735 \n",
      "當機器人來上班 | 15 train_loss 1.40173471 valid_loss 1.75861156 test_loss 1.70039046 \n",
      "當機器人來上班 | 16 train_loss 1.40022421 valid_loss 1.75096679 test_loss 1.69563544 \n",
      "當機器人來上班 | 17 train_loss 1.39984190 valid_loss 1.74899411 test_loss 1.69446599 \n",
      "當機器人來上班 | 18 train_loss 1.40162945 valid_loss 1.74786592 test_loss 1.69491982 \n",
      "當機器人來上班 | 19 train_loss 1.39846241 valid_loss 1.75195479 test_loss 1.69563913 \n",
      "當機器人來上班 | 20 train_loss 1.39834487 valid_loss 1.75239134 test_loss 1.69552875 \n",
      "當機器人來上班 | 21 train_loss 1.39881611 valid_loss 1.74603426 test_loss 1.69265234 \n",
      "當機器人來上班 | 22 train_loss 1.39830434 valid_loss 1.74619269 test_loss 1.69187307 \n",
      "當機器人來上班 | 23 train_loss 1.39646387 valid_loss 1.74703515 test_loss 1.69193137 \n",
      "當機器人來上班 | 24 train_loss 1.39647889 valid_loss 1.74495745 test_loss 1.69067645 \n",
      "當機器人來上班 | 25 train_loss 1.39818656 valid_loss 1.74396408 test_loss 1.69125164 \n",
      "當機器人來上班 | 26 train_loss 1.39767349 valid_loss 1.75341702 test_loss 1.69549608 \n",
      "當機器人來上班 | 27 train_loss 1.39505124 valid_loss 1.74377382 test_loss 1.68914557 \n",
      "當機器人來上班 | 28 train_loss 1.39525938 valid_loss 1.74247324 test_loss 1.68877363 \n",
      "當機器人來上班 | 29 train_loss 1.40006268 valid_loss 1.75951672 test_loss 1.69986522 \n",
      "當機器人來上班 | 30 train_loss 1.39383924 valid_loss 1.74281180 test_loss 1.68821216 \n",
      "當機器人來上班 | 31 train_loss 1.39442158 valid_loss 1.74038410 test_loss 1.68713737 \n",
      "當機器人來上班 | 32 train_loss 1.39456737 valid_loss 1.74887967 test_loss 1.69169652 \n",
      "當機器人來上班 | 33 train_loss 1.39695656 valid_loss 1.75435448 test_loss 1.69555724 \n",
      "當機器人來上班 | 34 train_loss 1.39208126 valid_loss 1.74167562 test_loss 1.68671119 \n",
      "當機器人來上班 | 35 train_loss 1.39214814 valid_loss 1.73950291 test_loss 1.68557858 \n",
      "當機器人來上班 | 36 train_loss 1.39246821 valid_loss 1.74548459 test_loss 1.68834054 \n",
      "當機器人來上班 | 37 train_loss 1.39103353 valid_loss 1.74048924 test_loss 1.68554544 \n",
      "當機器人來上班 | 38 train_loss 1.39547348 valid_loss 1.75239837 test_loss 1.69337428 \n",
      "當機器人來上班 | 39 train_loss 1.39715719 valid_loss 1.75590098 test_loss 1.69620967 \n",
      "當機器人來上班 | 40 train_loss 1.39271772 valid_loss 1.73706603 test_loss 1.68489695 \n",
      "當機器人來上班 | 41 train_loss 1.39236867 valid_loss 1.73553455 test_loss 1.68365860 \n",
      "當機器人來上班 | 42 train_loss 1.39437926 valid_loss 1.73589492 test_loss 1.68511999 \n",
      "當機器人來上班 | 43 train_loss 1.40020394 valid_loss 1.73785603 test_loss 1.68900597 \n",
      "當機器人來上班 | 44 train_loss 1.39023650 valid_loss 1.73435605 test_loss 1.68208826 \n",
      "當機器人來上班 | 45 train_loss 1.38908756 valid_loss 1.73447180 test_loss 1.68147027 \n",
      "當機器人來上班 | 46 train_loss 1.38849378 valid_loss 1.73469317 test_loss 1.68140554 \n",
      "當機器人來上班 | 47 train_loss 1.38796759 valid_loss 1.73396027 test_loss 1.68028843 \n",
      "當機器人來上班 | 48 train_loss 1.38841784 valid_loss 1.73341072 test_loss 1.68058419 \n",
      "當機器人來上班 | 49 train_loss 1.38704491 valid_loss 1.73455822 test_loss 1.68050158 \n",
      "當機器人來上班 | 50 train_loss 1.39040542 valid_loss 1.73208261 test_loss 1.68117380 \n",
      "當機器人來上班 | 51 train_loss 1.38751054 valid_loss 1.73166311 test_loss 1.67931199 \n",
      "當機器人來上班 | 52 train_loss 1.38618970 valid_loss 1.73301291 test_loss 1.67912018 \n",
      "當機器人來上班 | 53 train_loss 1.38726556 valid_loss 1.73977435 test_loss 1.68324399 \n",
      "當機器人來上班 | 54 train_loss 1.38626814 valid_loss 1.73747325 test_loss 1.68114614 \n",
      "當機器人來上班 | 55 train_loss 1.38571298 valid_loss 1.73129714 test_loss 1.67779255 \n",
      "當機器人來上班 | 56 train_loss 1.38647175 valid_loss 1.72978878 test_loss 1.67783296 \n",
      "當機器人來上班 | 57 train_loss 1.38602090 valid_loss 1.72934079 test_loss 1.67754543 \n",
      "當機器人來上班 | 58 train_loss 1.38438022 valid_loss 1.73110068 test_loss 1.67728591 \n",
      "當機器人來上班 | 59 train_loss 1.38412631 valid_loss 1.73020697 test_loss 1.67666221 \n",
      "當機器人來上班 | 60 train_loss 1.38412702 valid_loss 1.73368359 test_loss 1.67835724 \n",
      "當機器人來上班 | 61 train_loss 1.38407826 valid_loss 1.72887373 test_loss 1.67628145 \n",
      "當機器人來上班 | 62 train_loss 1.38326216 valid_loss 1.73003459 test_loss 1.67627668 \n",
      "當機器人來上班 | 63 train_loss 1.38834655 valid_loss 1.74371433 test_loss 1.68483019 \n",
      "當機器人來上班 | 64 train_loss 1.38280952 valid_loss 1.73074973 test_loss 1.67639875 \n",
      "當機器人來上班 | 65 train_loss 1.38257265 valid_loss 1.73061073 test_loss 1.67616308 \n",
      "當機器人來上班 | 66 train_loss 1.38275158 valid_loss 1.72796726 test_loss 1.67487442 \n",
      "當機器人來上班 | 67 train_loss 1.38231289 valid_loss 1.72700596 test_loss 1.67400360 \n",
      "當機器人來上班 | 68 train_loss 1.38176513 valid_loss 1.72897029 test_loss 1.67476737 \n",
      "當機器人來上班 | 69 train_loss 1.38406551 valid_loss 1.73679650 test_loss 1.67944193 \n",
      "當機器人來上班 | 70 train_loss 1.38331199 valid_loss 1.73562813 test_loss 1.67867100 \n",
      "當機器人來上班 | 71 train_loss 1.38414967 valid_loss 1.72437346 test_loss 1.67400348 \n",
      "當機器人來上班 | 72 train_loss 1.38093889 valid_loss 1.72611761 test_loss 1.67298174 \n",
      "當機器人來上班 | 73 train_loss 1.38185179 valid_loss 1.72433400 test_loss 1.67280436 \n",
      "當機器人來上班 | 74 train_loss 1.38234901 valid_loss 1.72420919 test_loss 1.67289305 \n",
      "當機器人來上班 | 75 train_loss 1.38028264 valid_loss 1.72825217 test_loss 1.67358923 \n",
      "當機器人來上班 | 76 train_loss 1.38116395 valid_loss 1.72371721 test_loss 1.67189431 \n",
      "當機器人來上班 | 77 train_loss 1.38154233 valid_loss 1.72265375 test_loss 1.67191124 \n",
      "當機器人來上班 | 78 train_loss 1.38014865 valid_loss 1.72305870 test_loss 1.67094398 \n",
      "當機器人來上班 | 79 train_loss 1.37923276 valid_loss 1.72501969 test_loss 1.67141271 \n",
      "當機器人來上班 | 80 train_loss 1.37913454 valid_loss 1.72430921 test_loss 1.67126453 \n",
      "當機器人來上班 | 81 train_loss 1.37883437 valid_loss 1.72396171 test_loss 1.67056799 \n",
      "當機器人來上班 | 82 train_loss 1.38166857 valid_loss 1.73436737 test_loss 1.67719126 \n",
      "當機器人來上班 | 83 train_loss 1.37843072 valid_loss 1.72493505 test_loss 1.67071390 \n",
      "當機器人來上班 | 84 train_loss 1.37858403 valid_loss 1.72707450 test_loss 1.67198110 \n",
      "當機器人來上班 | 85 train_loss 1.38277519 valid_loss 1.73680389 test_loss 1.67876387 \n",
      "當機器人來上班 | 86 train_loss 1.37804508 valid_loss 1.72575057 test_loss 1.67068923 \n",
      "當機器人來上班 | 87 train_loss 1.37801921 valid_loss 1.72635782 test_loss 1.67160618 \n",
      "當機器人來上班 | 88 train_loss 1.38310969 valid_loss 1.72050822 test_loss 1.67156911 \n",
      "當機器人來上班 | 89 train_loss 1.38469839 valid_loss 1.72071230 test_loss 1.67220712 \n",
      "當機器人來上班 | 90 train_loss 1.38274920 valid_loss 1.73750949 test_loss 1.67875659 \n",
      "當機器人來上班 | 91 train_loss 1.37748432 valid_loss 1.72624075 test_loss 1.67086983 \n",
      "當機器人來上班 | 92 train_loss 1.37826610 valid_loss 1.71956015 test_loss 1.66826534 \n",
      "當機器人來上班 | 93 train_loss 1.37789154 valid_loss 1.71890700 test_loss 1.66728365 \n",
      "當機器人來上班 | 94 train_loss 1.37629414 valid_loss 1.72241914 test_loss 1.66866612 \n",
      "當機器人來上班 | 95 train_loss 1.37767982 valid_loss 1.71836472 test_loss 1.66729367 \n",
      "當機器人來上班 | 96 train_loss 1.37649238 valid_loss 1.72481573 test_loss 1.66924822 \n",
      "當機器人來上班 | 97 train_loss 1.37611008 valid_loss 1.72404552 test_loss 1.66906250 \n",
      "當機器人來上班 | 98 train_loss 1.37556612 valid_loss 1.72044063 test_loss 1.66672742 \n",
      "當機器人來上班 | 99 train_loss 1.37539399 valid_loss 1.72017038 test_loss 1.66652131 \n",
      "當機器人來上班 | 100 train_loss 1.37522674 valid_loss 1.71974885 test_loss 1.66622734 \n",
      "當機器人來上班 | 101 train_loss 1.37649679 valid_loss 1.71754777 test_loss 1.66596901 \n",
      "當機器人來上班 | 102 train_loss 1.37499821 valid_loss 1.72163427 test_loss 1.66690350 \n",
      "當機器人來上班 | 103 train_loss 1.37464130 valid_loss 1.71998107 test_loss 1.66580904 \n",
      "當機器人來上班 | 104 train_loss 1.37461507 valid_loss 1.71878672 test_loss 1.66549218 \n",
      "當機器人來上班 | 105 train_loss 1.37474227 valid_loss 1.72212005 test_loss 1.66674793 \n",
      "當機器人來上班 | 106 train_loss 1.37603486 valid_loss 1.71636355 test_loss 1.66531301 \n",
      "當機器人來上班 | 107 train_loss 1.37576473 valid_loss 1.71576214 test_loss 1.66437912 \n",
      "當機器人來上班 | 108 train_loss 1.37383032 valid_loss 1.71974230 test_loss 1.66546822 \n",
      "當機器人來上班 | 109 train_loss 1.37375247 valid_loss 1.71730995 test_loss 1.66390824 \n",
      "當機器人來上班 | 110 train_loss 1.37451124 valid_loss 1.72357774 test_loss 1.66721344 \n",
      "當機器人來上班 | 111 train_loss 1.37618446 valid_loss 1.71500206 test_loss 1.66458631 \n",
      "當機器人來上班 | 112 train_loss 1.37331629 valid_loss 1.71967375 test_loss 1.66490829 \n",
      "當機器人來上班 | 113 train_loss 1.37625659 valid_loss 1.72859251 test_loss 1.67066658 \n",
      "當機器人來上班 | 114 train_loss 1.37363636 valid_loss 1.71526957 test_loss 1.66311872 \n",
      "當機器人來上班 | 115 train_loss 1.37290454 valid_loss 1.71592450 test_loss 1.66258395 \n",
      "當機器人來上班 | 116 train_loss 1.37634301 valid_loss 1.71419561 test_loss 1.66446233 \n",
      "當機器人來上班 | 117 train_loss 1.37269318 valid_loss 1.71518528 test_loss 1.66236830 \n",
      "當機器人來上班 | 118 train_loss 1.37221718 valid_loss 1.71623719 test_loss 1.66248381 \n",
      "當機器人來上班 | 119 train_loss 1.37205732 valid_loss 1.71763635 test_loss 1.66304314 \n",
      "當機器人來上班 | 120 train_loss 1.37199235 valid_loss 1.71810794 test_loss 1.66327572 \n",
      "當機器人來上班 | 121 train_loss 1.37225437 valid_loss 1.71413302 test_loss 1.66147304 \n",
      "當機器人來上班 | 122 train_loss 1.37170148 valid_loss 1.71520352 test_loss 1.66173410 \n",
      "當機器人來上班 | 123 train_loss 1.37908256 valid_loss 1.73455453 test_loss 1.67457807 \n",
      "當機器人來上班 | 124 train_loss 1.37300158 valid_loss 1.71312129 test_loss 1.66138935 \n",
      "當機器人來上班 | 125 train_loss 1.37193179 valid_loss 1.71366799 test_loss 1.66108227 \n",
      "當機器人來上班 | 126 train_loss 1.37105966 valid_loss 1.71627545 test_loss 1.66168916 \n",
      "當機器人來上班 | 127 train_loss 1.37352538 valid_loss 1.72431111 test_loss 1.66654372 \n",
      "當機器人來上班 | 128 train_loss 1.37099850 valid_loss 1.71761012 test_loss 1.66191924 \n",
      "當機器人來上班 | 129 train_loss 1.37197411 valid_loss 1.72084641 test_loss 1.66427374 \n",
      "當機器人來上班 | 130 train_loss 1.37070656 valid_loss 1.71715820 test_loss 1.66168284 \n",
      "當機器人來上班 | 131 train_loss 1.37158501 valid_loss 1.72037077 test_loss 1.66370749 \n",
      "當機器人來上班 | 132 train_loss 1.37556040 valid_loss 1.72922981 test_loss 1.66996181 \n",
      "當機器人來上班 | 133 train_loss 1.37283659 valid_loss 1.72381151 test_loss 1.66569793 \n",
      "當機器人來上班 | 134 train_loss 1.37006307 valid_loss 1.71547723 test_loss 1.66052139 \n",
      "當機器人來上班 | 135 train_loss 1.37095034 valid_loss 1.71179044 test_loss 1.65943789 \n",
      "當機器人來上班 | 136 train_loss 1.37094831 valid_loss 1.71121073 test_loss 1.65921652 \n",
      "當機器人來上班 | 137 train_loss 1.36966479 valid_loss 1.71413922 test_loss 1.65954447 \n",
      "當機器人來上班 | 138 train_loss 1.36957741 valid_loss 1.71327090 test_loss 1.65915811 \n",
      "當機器人來上班 | 139 train_loss 1.37334895 valid_loss 1.71088290 test_loss 1.66054606 \n",
      "當機器人來上班 | 140 train_loss 1.37116301 valid_loss 1.72018671 test_loss 1.66292632 \n",
      "當機器人來上班 | 141 train_loss 1.36924231 valid_loss 1.71417689 test_loss 1.65943027 \n",
      "當機器人來上班 | 142 train_loss 1.37494445 valid_loss 1.72852921 test_loss 1.66856432 \n",
      "當機器人來上班 | 143 train_loss 1.37228334 valid_loss 1.72343075 test_loss 1.66490710 \n",
      "當機器人來上班 | 144 train_loss 1.37405622 valid_loss 1.72729242 test_loss 1.66744506 \n",
      "當機器人來上班 | 145 train_loss 1.36900020 valid_loss 1.71220934 test_loss 1.65816450 \n",
      "當機器人來上班 | 146 train_loss 1.37019765 valid_loss 1.71007311 test_loss 1.65808511 \n",
      "當機器人來上班 | 147 train_loss 1.36867464 valid_loss 1.71363258 test_loss 1.65837932 \n",
      "當機器人來上班 | 148 train_loss 1.37410676 valid_loss 1.72758162 test_loss 1.66771233 \n",
      "當機器人來上班 | 149 train_loss 1.37015939 valid_loss 1.71941888 test_loss 1.66216075 \n",
      "當機器人來上班 | 150 train_loss 1.36856747 valid_loss 1.71074593 test_loss 1.65706539 \n",
      "當機器人來上班 | 151 train_loss 1.36840045 valid_loss 1.71130729 test_loss 1.65727437 \n",
      "當機器人來上班 | 152 train_loss 1.36859441 valid_loss 1.71052587 test_loss 1.65696764 \n",
      "當機器人來上班 | 153 train_loss 1.36865175 valid_loss 1.71562874 test_loss 1.65905488 \n",
      "當機器人來上班 | 154 train_loss 1.37200665 valid_loss 1.72382009 test_loss 1.66469324 \n",
      "當機器人來上班 | 155 train_loss 1.37028468 valid_loss 1.72027206 test_loss 1.66198456 \n",
      "當機器人來上班 | 156 train_loss 1.36784744 valid_loss 1.71237087 test_loss 1.65714896 \n",
      "當機器人來上班 | 157 train_loss 1.36869454 valid_loss 1.71645272 test_loss 1.65904677 \n",
      "當機器人來上班 | 158 train_loss 1.36779094 valid_loss 1.71291125 test_loss 1.65737784 \n",
      "當機器人來上班 | 159 train_loss 1.38189971 valid_loss 1.74127030 test_loss 1.67832828 \n",
      "當機器人來上班 | 160 train_loss 1.37025201 valid_loss 1.72075391 test_loss 1.66220367 \n",
      "當機器人來上班 | 161 train_loss 1.36868906 valid_loss 1.71712589 test_loss 1.65977883 \n",
      "當機器人來上班 | 162 train_loss 1.36746407 valid_loss 1.71201885 test_loss 1.65646136 \n",
      "當機器人來上班 | 163 train_loss 1.37026978 valid_loss 1.70800030 test_loss 1.65697539 \n",
      "當機器人來上班 | 164 train_loss 1.36735225 valid_loss 1.71220803 test_loss 1.65684509 \n",
      "當機器人來上班 | 165 train_loss 1.36846554 valid_loss 1.71656621 test_loss 1.65891671 \n",
      "當機器人來上班 | 166 train_loss 1.36827481 valid_loss 1.71631551 test_loss 1.65888286 \n",
      "當機器人來上班 | 167 train_loss 1.36778545 valid_loss 1.70889366 test_loss 1.65539718 \n",
      "當機器人來上班 | 168 train_loss 1.36766684 valid_loss 1.70877528 test_loss 1.65567374 \n",
      "當機器人來上班 | 169 train_loss 1.36809456 valid_loss 1.70816231 test_loss 1.65504193 \n",
      "當機器人來上班 | 170 train_loss 1.36923862 valid_loss 1.71853435 test_loss 1.66027772 \n",
      "當機器人來上班 | 171 train_loss 1.36686087 valid_loss 1.71146739 test_loss 1.65585172 \n",
      "當機器人來上班 | 172 train_loss 1.36697197 valid_loss 1.71252441 test_loss 1.65649450 \n",
      "當機器人來上班 | 173 train_loss 1.36672294 valid_loss 1.71026731 test_loss 1.65514064 \n",
      "當機器人來上班 | 174 train_loss 1.37015176 valid_loss 1.72098041 test_loss 1.66194057 \n",
      "當機器人來上班 | 175 train_loss 1.36662352 valid_loss 1.71027088 test_loss 1.65538549 \n",
      "當機器人來上班 | 176 train_loss 1.36756516 valid_loss 1.70729637 test_loss 1.65481687 \n",
      "當機器人來上班 | 177 train_loss 1.37021708 valid_loss 1.72090936 test_loss 1.66163981 \n",
      "當機器人來上班 | 178 train_loss 1.36785603 valid_loss 1.71570849 test_loss 1.65844011 \n",
      "當機器人來上班 | 179 train_loss 1.36647534 valid_loss 1.70913625 test_loss 1.65462554 \n",
      "當機器人來上班 | 180 train_loss 1.36781061 valid_loss 1.71557522 test_loss 1.65778613 \n",
      "當機器人來上班 | 181 train_loss 1.36813843 valid_loss 1.70698929 test_loss 1.65453088 \n",
      "當機器人來上班 | 182 train_loss 1.36670077 valid_loss 1.71270823 test_loss 1.65609407 \n",
      "當機器人來上班 | 183 train_loss 1.36714292 valid_loss 1.71430862 test_loss 1.65706611 \n",
      "當機器人來上班 | 184 train_loss 1.36973381 valid_loss 1.72056687 test_loss 1.66144609 \n",
      "當機器人來上班 | 185 train_loss 1.36716843 valid_loss 1.71436942 test_loss 1.65693212 \n",
      "當機器人來上班 | 186 train_loss 1.36788273 valid_loss 1.71658301 test_loss 1.65846276 \n",
      "當機器人來上班 | 187 train_loss 1.36672640 valid_loss 1.71316016 test_loss 1.65624547 \n",
      "當機器人來上班 | 188 train_loss 1.36629307 valid_loss 1.71150470 test_loss 1.65523875 \n",
      "當機器人來上班 | 189 train_loss 1.37020934 valid_loss 1.70613372 test_loss 1.65552890 \n",
      "當機器人來上班 | 190 train_loss 1.36802793 valid_loss 1.71644831 test_loss 1.65838718 \n",
      "當機器人來上班 | 191 train_loss 1.36774671 valid_loss 1.71638131 test_loss 1.65828156 \n",
      "當機器人來上班 | 192 train_loss 1.36584353 valid_loss 1.70904064 test_loss 1.65410626 \n",
      "當機器人來上班 | 193 train_loss 1.36880624 valid_loss 1.71837151 test_loss 1.65929890 \n",
      "當機器人來上班 | 194 train_loss 1.36848307 valid_loss 1.71803606 test_loss 1.65936649 \n",
      "當機器人來上班 | 195 train_loss 1.36895585 valid_loss 1.71887803 test_loss 1.66029441 \n",
      "當機器人來上班 | 196 train_loss 1.36627316 valid_loss 1.70637774 test_loss 1.65303469 \n",
      "當機器人來上班 | 197 train_loss 1.36626673 valid_loss 1.71226311 test_loss 1.65571010 \n",
      "當機器人來上班 | 198 train_loss 1.36560488 valid_loss 1.70878458 test_loss 1.65361524 \n",
      "當機器人來上班 | 199 train_loss 1.36560297 valid_loss 1.70819342 test_loss 1.65292990 \n",
      "當機器人來上班 | 200 train_loss 1.36701453 valid_loss 1.70545948 test_loss 1.65338302 \n",
      "Accuracy of validation is CRASH !!\n",
      "ReTraining done, RMSE: 1.370413064956665 => 1.370413064956665\n",
      "decrease: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "course = pd.read_excel('data/other/course_list.xlsx', 'course')\n",
    "course = course['course_name'].drop_duplicates()\n",
    "model = torch.load('model_pytorch/Advice.pt')\n",
    "for c in course:\n",
    "    \n",
    "    c_model = torch.load('model_pytorch/Advice.pt')\n",
    "\n",
    "    torch.save(c_model, 'model_pytorch/Advice_{}.pt'.format(c))\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(c_model.parameters())\n",
    "    \n",
    "    x_train, x_valid, x_test, y_train, y_valid, y_test = getData_Advice(c)\n",
    "    x_train = x_train.to(device)\n",
    "    x_valid = x_valid.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    y_valid = y_valid.to(device)\n",
    "    x_test = x_test.to(device)\n",
    "    y_test = y_test.to(device)  \n",
    "    train_dl = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), shuffle=True, batch_size=64) \n",
    "    minVL = float(criterion(model(x_valid), y_valid) ** 0.5)\n",
    "    minVL_0 = minVL\n",
    "    minVL_ep = 0\n",
    "\n",
    "    print('Now processing model:', c)\n",
    "    print('original RMSE:', minVL)\n",
    "    \n",
    "    for e in range(1000):\n",
    "        ep = e + 1\n",
    "        for xb, yb in train_dl:\n",
    "            pred = c_model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss = float(criterion(c_model(x_train), y_train))\n",
    "        valid_loss = float(criterion(c_model(x_valid), y_valid))\n",
    "        test_loss = float(criterion(c_model(x_test), y_test))\n",
    "\n",
    "        save = ''\n",
    "        if minVL > valid_loss:\n",
    "            minVL = valid_loss\n",
    "            minVL_ep = ep\n",
    "            save = '< save'\n",
    "            torch.save(c_model, 'model_pytorch/Advice_{}.pt'.format(c))\n",
    "        print(c, '|', ep, \n",
    "            'train_loss', '{:.8f}'.format(train_loss), \n",
    "            'valid_loss', '{:.8f}'.format(valid_loss), \n",
    "            'test_loss', '{:.8f}'.format(test_loss), \n",
    "            save)\n",
    "        ## early drop\n",
    "        if ep < 200: ## 至少執行200次\n",
    "            pass\n",
    "        elif minVL_ep < ep/2: ## 連續10次小於maxValAcc的一半\n",
    "            print('Accuracy of validation is CRASH !!')\n",
    "            break\n",
    "    print('ReTraining done, RMSE:', minVL_0, '=>', minVL)\n",
    "    print('decrease:',minVL_0 - minVL)\n",
    "    print()\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0aa66b553f575c766ebf8dae9865f08740f7a672498c10dd9d6d0f09d940d9df"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
